{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Name: Guanzhi Deng\n",
    "### USC ID: 8763-5651-24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "asK6iTZ5o9FE",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guanz\\anaconda3\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "#import nltk_data\n",
    "#from nltk_data.corpora import wordnet, stopwords\n",
    "#from nltk_data.tokenizers import punkt\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import gzip\n",
    "from gensim import models\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WyhzVdPeykHU",
    "outputId": "92763337-1569-40bb-9541-421a9249f4be"
   },
   "outputs": [],
   "source": [
    "#pip install -U gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NLkvOyUEfj8z",
    "outputId": "46885b0a-ae5b-4471-d5b9-eea1b889e84b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d45lfLhto9FI"
   },
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LIK9l3sko9FK",
    "outputId": "424d0020-08a1-459e-a223-8fc13bd9fb85",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 16148: expected 15 fields, saw 22\\nSkipping line 20100: expected 15 fields, saw 22\\nSkipping line 45178: expected 15 fields, saw 22\\nSkipping line 48700: expected 15 fields, saw 22\\nSkipping line 63331: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 86053: expected 15 fields, saw 22\\nSkipping line 88858: expected 15 fields, saw 22\\nSkipping line 115017: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 137366: expected 15 fields, saw 22\\nSkipping line 139110: expected 15 fields, saw 22\\nSkipping line 165540: expected 15 fields, saw 22\\nSkipping line 171813: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 203723: expected 15 fields, saw 22\\nSkipping line 209366: expected 15 fields, saw 22\\nSkipping line 211310: expected 15 fields, saw 22\\nSkipping line 246351: expected 15 fields, saw 22\\nSkipping line 252364: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 267003: expected 15 fields, saw 22\\nSkipping line 268957: expected 15 fields, saw 22\\nSkipping line 303336: expected 15 fields, saw 22\\nSkipping line 306021: expected 15 fields, saw 22\\nSkipping line 311569: expected 15 fields, saw 22\\nSkipping line 316767: expected 15 fields, saw 22\\nSkipping line 324009: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 359107: expected 15 fields, saw 22\\nSkipping line 368367: expected 15 fields, saw 22\\nSkipping line 381180: expected 15 fields, saw 22\\nSkipping line 390453: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 412243: expected 15 fields, saw 22\\nSkipping line 419342: expected 15 fields, saw 22\\nSkipping line 457388: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 459935: expected 15 fields, saw 22\\nSkipping line 460167: expected 15 fields, saw 22\\nSkipping line 466460: expected 15 fields, saw 22\\nSkipping line 500314: expected 15 fields, saw 22\\nSkipping line 500339: expected 15 fields, saw 22\\nSkipping line 505396: expected 15 fields, saw 22\\nSkipping line 507760: expected 15 fields, saw 22\\nSkipping line 513626: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 527638: expected 15 fields, saw 22\\nSkipping line 534209: expected 15 fields, saw 22\\nSkipping line 535687: expected 15 fields, saw 22\\nSkipping line 547671: expected 15 fields, saw 22\\nSkipping line 549054: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 599929: expected 15 fields, saw 22\\nSkipping line 604776: expected 15 fields, saw 22\\nSkipping line 609937: expected 15 fields, saw 22\\nSkipping line 632059: expected 15 fields, saw 22\\nSkipping line 638546: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 665017: expected 15 fields, saw 22\\nSkipping line 677680: expected 15 fields, saw 22\\nSkipping line 684370: expected 15 fields, saw 22\\nSkipping line 720217: expected 15 fields, saw 29\\n'\n",
      "b'Skipping line 723240: expected 15 fields, saw 22\\nSkipping line 723433: expected 15 fields, saw 22\\nSkipping line 763891: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 800288: expected 15 fields, saw 22\\nSkipping line 802942: expected 15 fields, saw 22\\nSkipping line 803379: expected 15 fields, saw 22\\nSkipping line 805122: expected 15 fields, saw 22\\nSkipping line 821899: expected 15 fields, saw 22\\nSkipping line 831707: expected 15 fields, saw 22\\nSkipping line 842829: expected 15 fields, saw 22\\nSkipping line 843604: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 863904: expected 15 fields, saw 22\\nSkipping line 875655: expected 15 fields, saw 22\\nSkipping line 886796: expected 15 fields, saw 22\\nSkipping line 892299: expected 15 fields, saw 22\\nSkipping line 902518: expected 15 fields, saw 22\\nSkipping line 903079: expected 15 fields, saw 22\\nSkipping line 912678: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 932953: expected 15 fields, saw 22\\nSkipping line 936838: expected 15 fields, saw 22\\nSkipping line 937177: expected 15 fields, saw 22\\nSkipping line 947695: expected 15 fields, saw 22\\nSkipping line 960713: expected 15 fields, saw 22\\nSkipping line 965225: expected 15 fields, saw 22\\nSkipping line 980776: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 999318: expected 15 fields, saw 22\\nSkipping line 1007247: expected 15 fields, saw 22\\nSkipping line 1015987: expected 15 fields, saw 22\\nSkipping line 1018984: expected 15 fields, saw 22\\nSkipping line 1028671: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1063360: expected 15 fields, saw 22\\nSkipping line 1066195: expected 15 fields, saw 22\\nSkipping line 1066578: expected 15 fields, saw 22\\nSkipping line 1066869: expected 15 fields, saw 22\\nSkipping line 1068809: expected 15 fields, saw 22\\nSkipping line 1069505: expected 15 fields, saw 22\\nSkipping line 1087983: expected 15 fields, saw 22\\nSkipping line 1108184: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1118137: expected 15 fields, saw 22\\nSkipping line 1142723: expected 15 fields, saw 22\\nSkipping line 1152492: expected 15 fields, saw 22\\nSkipping line 1156947: expected 15 fields, saw 22\\nSkipping line 1172563: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1209254: expected 15 fields, saw 22\\nSkipping line 1212966: expected 15 fields, saw 22\\nSkipping line 1236533: expected 15 fields, saw 22\\nSkipping line 1237598: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1273825: expected 15 fields, saw 22\\nSkipping line 1277898: expected 15 fields, saw 22\\nSkipping line 1283654: expected 15 fields, saw 22\\nSkipping line 1286023: expected 15 fields, saw 22\\nSkipping line 1302038: expected 15 fields, saw 22\\nSkipping line 1305179: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1326022: expected 15 fields, saw 22\\nSkipping line 1338120: expected 15 fields, saw 22\\nSkipping line 1338503: expected 15 fields, saw 22\\nSkipping line 1338849: expected 15 fields, saw 22\\nSkipping line 1341513: expected 15 fields, saw 22\\nSkipping line 1346493: expected 15 fields, saw 22\\nSkipping line 1373127: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1389508: expected 15 fields, saw 22\\nSkipping line 1413951: expected 15 fields, saw 22\\nSkipping line 1433626: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1442698: expected 15 fields, saw 22\\nSkipping line 1472982: expected 15 fields, saw 22\\nSkipping line 1482282: expected 15 fields, saw 22\\nSkipping line 1487808: expected 15 fields, saw 22\\nSkipping line 1500636: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1511479: expected 15 fields, saw 22\\nSkipping line 1532302: expected 15 fields, saw 22\\nSkipping line 1537952: expected 15 fields, saw 22\\nSkipping line 1539951: expected 15 fields, saw 22\\nSkipping line 1541020: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1594217: expected 15 fields, saw 22\\nSkipping line 1612264: expected 15 fields, saw 22\\nSkipping line 1615907: expected 15 fields, saw 22\\nSkipping line 1621859: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1653542: expected 15 fields, saw 22\\nSkipping line 1671537: expected 15 fields, saw 22\\nSkipping line 1672879: expected 15 fields, saw 22\\nSkipping line 1674523: expected 15 fields, saw 22\\nSkipping line 1677355: expected 15 fields, saw 22\\nSkipping line 1703907: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1713046: expected 15 fields, saw 22\\nSkipping line 1722982: expected 15 fields, saw 22\\nSkipping line 1727290: expected 15 fields, saw 22\\nSkipping line 1744482: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1803858: expected 15 fields, saw 22\\nSkipping line 1810069: expected 15 fields, saw 22\\nSkipping line 1829751: expected 15 fields, saw 22\\nSkipping line 1831699: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1863131: expected 15 fields, saw 22\\nSkipping line 1867917: expected 15 fields, saw 22\\nSkipping line 1874790: expected 15 fields, saw 22\\nSkipping line 1879952: expected 15 fields, saw 22\\nSkipping line 1880501: expected 15 fields, saw 22\\nSkipping line 1886655: expected 15 fields, saw 22\\nSkipping line 1887888: expected 15 fields, saw 22\\nSkipping line 1894286: expected 15 fields, saw 22\\nSkipping line 1895400: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1904040: expected 15 fields, saw 22\\nSkipping line 1907604: expected 15 fields, saw 22\\nSkipping line 1915739: expected 15 fields, saw 22\\nSkipping line 1921514: expected 15 fields, saw 22\\nSkipping line 1939428: expected 15 fields, saw 22\\nSkipping line 1944342: expected 15 fields, saw 22\\nSkipping line 1949699: expected 15 fields, saw 22\\nSkipping line 1961872: expected 15 fields, saw 22\\n'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 1968846: expected 15 fields, saw 22\\nSkipping line 1999941: expected 15 fields, saw 22\\nSkipping line 2001492: expected 15 fields, saw 22\\nSkipping line 2011204: expected 15 fields, saw 22\\nSkipping line 2025295: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2041266: expected 15 fields, saw 22\\nSkipping line 2073314: expected 15 fields, saw 22\\nSkipping line 2080133: expected 15 fields, saw 22\\nSkipping line 2088521: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2103490: expected 15 fields, saw 22\\nSkipping line 2115278: expected 15 fields, saw 22\\nSkipping line 2153174: expected 15 fields, saw 22\\nSkipping line 2161731: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2165250: expected 15 fields, saw 22\\nSkipping line 2175132: expected 15 fields, saw 22\\nSkipping line 2206817: expected 15 fields, saw 22\\nSkipping line 2215848: expected 15 fields, saw 22\\nSkipping line 2223811: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2257265: expected 15 fields, saw 22\\nSkipping line 2259163: expected 15 fields, saw 22\\nSkipping line 2263291: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2301943: expected 15 fields, saw 22\\nSkipping line 2304371: expected 15 fields, saw 22\\nSkipping line 2306015: expected 15 fields, saw 22\\nSkipping line 2312186: expected 15 fields, saw 22\\nSkipping line 2314740: expected 15 fields, saw 22\\nSkipping line 2317754: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2383514: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2449763: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2589323: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2775036: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2935174: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 3078830: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 3123091: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 3185533: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 4150395: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 4748401: expected 15 fields, saw 22\\n'\n"
     ]
    }
   ],
   "source": [
    "#df = pd.read_csv(\"https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Kitchen_v1_00.tsv.gz\", sep = '\\t', compression = 'gzip', error_bad_lines = False)\n",
    "df = pd.read_csv(\"amazon_reviews_us_Kitchen_v1_00.tsv.gz\", sep = '\\t', compression = 'gzip', error_bad_lines = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XRxXZ8heo9FK"
   },
   "source": [
    "# Dataset Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "SSr8Ga4cHPt9",
    "outputId": "7eef3df4-bbc7-40ae-a221-42d2ccb7d58c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reviews</th>\n",
       "      <th>Ratings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Beautiful. Looks great on counter Beautiful.  ...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Awesome &amp; Self-ness I personally have 5 days s...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fabulous and worth every penny Fabulous and wo...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Five Stars A must if you love garlic on tomato...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Better than sex Worth every penny! Buy one now...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Reviews  Ratings\n",
       "0  Beautiful. Looks great on counter Beautiful.  ...      5.0\n",
       "1  Awesome & Self-ness I personally have 5 days s...      5.0\n",
       "2  Fabulous and worth every penny Fabulous and wo...      5.0\n",
       "3  Five Stars A must if you love garlic on tomato...      5.0\n",
       "4  Better than sex Worth every penny! Buy one now...      5.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Reviews'] = df['review_headline'] + ' ' + df['review_body']\n",
    "df['Ratings'] = df['star_rating']\n",
    "df1 = df[['Reviews', 'Ratings']].copy()\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "XO4UWsjHqa3Y"
   },
   "outputs": [],
   "source": [
    "df_1 = df1[df1['Ratings'] == 1].sample(n =50000, random_state = 1)\n",
    "df_2 = df1[df1['Ratings'] == 2].sample(n = 50000, random_state = 1)\n",
    "df_3 = df1[df1['Ratings'] == 3].sample(n = 50000, random_state = 1)\n",
    "df_4 = df1[df1['Ratings'] == 4].sample(n = 50000, random_state = 1)\n",
    "df_5 = df1[df1['Ratings'] == 5].sample(n = 50000, random_state = 1)\n",
    "df2 = pd.concat([df_1, df_2, df_3, df_4, df_5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dtfEOWxF9iII",
    "outputId": "12be72cd-5ebb-4686-8b61-06594d11f894"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   Reviews  Ratings\n",
      "3501986  Must Have Tingle!! Love, Love, Love! This loti...      5.0\n",
      "2615490  Nordic Ware Microwave Micro-Go-Round Not happy...      1.0\n",
      "2255353  Five Stars thought they were smaller.. but I w...      5.0\n"
     ]
    }
   ],
   "source": [
    "print(df2.sample(n=3, random_state=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8oiOSkgJRwWz",
    "outputId": "ed4b54eb-a8e0-413a-8361-64ea94e19410"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Reviews\n",
      "Ratings         \n",
      "1.0        49995\n",
      "2.0        49998\n",
      "3.0        49997\n",
      "4.0        49998\n",
      "5.0        49997\n"
     ]
    }
   ],
   "source": [
    "print(df2.groupby(\"Ratings\").count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "1sWjLXhso9FN"
   },
   "outputs": [],
   "source": [
    " def sentiment(n):\n",
    "   if n > 3:\n",
    "     return 1\n",
    "   elif n < 3:\n",
    "     return 2\n",
    "   else:\n",
    "     return 3    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "jJoK8z9ETnRL"
   },
   "outputs": [],
   "source": [
    "df3 = df2.copy()\n",
    "df3['Ratings'] = df3['Ratings'].apply(sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "abgW8TqGn_ct",
    "outputId": "e0b3dba5-3e2d-4f56-a723-a82bfc66a087"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive reviews: 99995\n",
      "\n",
      "Number of negative reviews: 99993\n",
      "\n",
      "Number of neutral reviews: 49997\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of positive reviews: {df3[df3['Ratings'] == 1]['Reviews'].count()}\\n\")\n",
    "print(f\"Number of negative reviews: {df3[df3['Ratings'] == 2]['Reviews'].count()}\\n\")\n",
    "print(f\"Number of neutral reviews: {df3[df3['Ratings'] == 3]['Reviews'].count()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4stRvTaB0XGr",
    "outputId": "acc5536a-2a2a-4877-8237-9d26c6da0c73"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3227995    It does not work The concept is great but the ...\n",
       "583124     0 Zero Stars!!! Didn't work. Didn't heat. Didn...\n",
       "3015847    Egg Cutter Doesn't work for me. The opening is...\n",
       "1498212                           One Star Broke right away.\n",
       "4235151    stuck to glass cooktop Very disappointed after...\n",
       "Name: Reviews, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3['Reviews'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "eZt-76JG4YAk"
   },
   "outputs": [],
   "source": [
    "# Drop data that has NaNs\n",
    "df3 = df3.dropna(subset=['Reviews'])\n",
    "# Convert characters to lower case\n",
    "df3['Reviews'] = df3['Reviews'].str.lower()\n",
    "# Remove non-alphabetic characters in reviews\n",
    "#df3['Reviews'] = df3['Reviews'].apply(lambda x: re.sub('[^a-zA-Z ]', '', x))\n",
    "# Remove extra spaces\n",
    "#df3['Reviews'] = df3['Reviews'].apply(lambda x: re.sub(r'\\s\\s+', ' ', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "ZB616S5VhO4Y"
   },
   "outputs": [],
   "source": [
    "del df, df1, df2, df_1, df_2, df_3, df_4, df_5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V-znpHGeo9FS"
   },
   "source": [
    "# Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NIynUivVrEG-"
   },
   "source": [
    "## pretrained word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "ygkUYrrdo9FS"
   },
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "wv_google = api.load('word2vec-google-news-300')\n",
    "\"\"\"wv_google = models.KeyedVectors.load_word2vec_format(\n",
    "    'GoogleNews-vectors-negative300.bin', binary=True)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JoBXHzNlr6gj",
    "outputId": "ed709365-17ab-4d17-9e21-3bfbb6003e68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check semantic similarities using the pretrained model:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Check semantic similarities using the pretrained model:\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "PKgbHCRrNnAb",
    "outputId": "c60bfc9e-197a-494a-fccd-7bcfe2c363e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('mother', 0.8462507128715515)]\n"
     ]
    }
   ],
   "source": [
    "print(wv_google.most_similar(positive=['father', 'woman'], negative=['man'], topn=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "9Dz2DK_LDEwM",
    "outputId": "7c0e69d9-527f-4716-db41-f308a5ffc252"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8317594\n"
     ]
    }
   ],
   "source": [
    "print(wv_google.similarity('fantastic', 'terrific'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IFZ65UPWrLmZ"
   },
   "source": [
    "## my own word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "Dk_sd0qEo-Ms"
   },
   "outputs": [],
   "source": [
    "from gensim import utils\n",
    "\n",
    "class MyCorpus:\n",
    "    \"\"\"An iterator that yields sentences (lists of str).\"\"\"\n",
    "\n",
    "    def __iter__(self):\n",
    "        for line in df3['Reviews']:      \n",
    "            # assume there's one document per line, tokens separated by whitespace\n",
    "            yield utils.simple_preprocess(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "wzZp67noO8oP",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = models.Word2Vec(sentences=MyCorpus(), vector_size=300, window=11, min_count=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "ZEz30W79O8rQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check semantic similarities using my own model:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Check semantic similarities using my own model:\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bF0bbe2krxZ-",
    "outputId": "ee2b74f7-8741-4fef-b747-461429fb3c54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('mother', 0.7102171778678894)]\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.most_similar(positive=['father', 'woman'], negative=['man'], topn=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gPGeVvrEO8uw",
    "outputId": "7d7446af-150d-45cc-d61b-a00d042261aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.77164054\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.similarity('fantastic', 'terrific'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dvnWpflqW22T"
   },
   "source": [
    "The pretrained model was trained by a much larger corpus (in terms of both quantities and varieties) than my own training dataset. Therefore, it is not surprising to see the pretrained model encodes better semantic similarities between words than my own model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Zl8p47fg69l"
   },
   "source": [
    "## word vector functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ch812sGDzPDa"
   },
   "source": [
    "Define a function to compute the average word vectors of a string using a word vector model(wv_google or model.wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "WqCd-G9wP44o"
   },
   "outputs": [],
   "source": [
    "def word2v_avg(s, wv_model):\n",
    "  if type(s) == str:\n",
    "    ls = s.split(' ')\n",
    "  if type(s) == list:\n",
    "    ls = s\n",
    "  eb_sum = np.zeros(shape=(300,))\n",
    "  count = 0\n",
    "  for word in ls:\n",
    "    if word in wv_model:\n",
    "      eb_word = wv_model[word]\n",
    "      eb_sum += eb_word\n",
    "      count += 1\n",
    "  eb_avg = eb_sum / count\n",
    "  return eb_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MTdypjNgzZ6z"
   },
   "source": [
    "Define a function to compute the first 10 word vectors of a string using a word vector model(wv_google or model.wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "0GGIMXErqgmD"
   },
   "outputs": [],
   "source": [
    "def word2v_concat(s, wv_model):\n",
    "  if type(s) == list:\n",
    "    ls = s\n",
    "  if type(s) == str:\n",
    "    ls = s.split(' ')\n",
    "  i = 0 # position in the list of words\n",
    "  j = 0 # number of word vectors\n",
    "  while (i < len(ls)) & (j < 10):\n",
    "    if ls[i] in wv_model:\n",
    "      wv = wv_model[ls[i]]\n",
    "      if j == 0:\n",
    "        a = wv\n",
    "      else:\n",
    "        a = np.concatenate((a, wv))\n",
    "      i += 1\n",
    "      j += 1\n",
    "    else:\n",
    "      i += 1\n",
    "  if j < 10:\n",
    "    n = 10 - j\n",
    "    zeros = np.zeros(shape=(300*n, ))\n",
    "    if n == 10:\n",
    "      a = zeros\n",
    "    else:\n",
    "      a = np.concatenate((a, zeros))\n",
    "  return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cFJeVDwvykHz"
   },
   "source": [
    "Define a function to compute the word embeddings of a word sequence of 20 words (padded with zero vectors if there are less than 20 words in the sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "NnIgUysbykHz"
   },
   "outputs": [],
   "source": [
    "def word2v_seq(s, model):\n",
    "    if type(s) == str:\n",
    "        ls = s.split(' ')\n",
    "    if type(s) == list:\n",
    "        ls = s\n",
    "    i, j = 0, 0\n",
    "    while (i < len(ls)) & (j < 20):\n",
    "        if ls[i] in model:\n",
    "            if j == 0:\n",
    "                wv_mtx = model[ls[i]]\n",
    "            else:\n",
    "                wv_mtx_add = model[ls[i]] \n",
    "                wv_mtx = np.vstack((wv_mtx, wv_mtx_add))\n",
    "            i += 1\n",
    "            j += 1\n",
    "        else:\n",
    "            i += 1\n",
    "    if j == 0:\n",
    "        wv_mtx = np.zeros(shape=(20, 300))\n",
    "    elif j < 20:\n",
    "        wv_mtx_zeros = np.zeros(shape=(20-j, 300))\n",
    "        wv_mtx = np.vstack((wv_mtx, wv_mtx_zeros))\n",
    "    return wv_mtx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2q9QGM-LzgDL"
   },
   "source": [
    "Define a function to find out the row indices for all the NaN's in a matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "_9QIHtMZP44p"
   },
   "outputs": [],
   "source": [
    "def idx_nan(matrix):\n",
    "  if np.any(np.isnan(matrix)):\n",
    "    arr_nan = np.argwhere(np.isnan(matrix))\n",
    "    num_nan = arr_nan.shape[0]\n",
    "    arr = np.arange(0, num_nan, 300)\n",
    "    idx = []\n",
    "    for i in arr:\n",
    "      idx.append(arr_nan[i][0])\n",
    "    return idx\n",
    "  else:\n",
    "    return None\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d_dhNHJkwVR-"
   },
   "source": [
    "# Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "j5npGj0_wkUI"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "RCGJtMwqvkgp"
   },
   "outputs": [],
   "source": [
    "df_binary = df3[(df3['Ratings']==1)|(df3['Ratings']==2)]\n",
    "df_ternary = df3\n",
    "labels_binary = df_binary['Ratings'].to_numpy()\n",
    "labels_ternary = df_ternary['Ratings'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "_x1syluGt6tl"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_binary['Reviews'], labels_binary, test_size=0.2, random_state=1)\n",
    "X_train_te, X_test_te, y_train_te, y_test_te = train_test_split(df_ternary['Reviews'], labels_ternary, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "oQdRM19d0iT3"
   },
   "outputs": [],
   "source": [
    "del df_binary, df_ternary, labels_binary, labels_ternary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Fy5EMklnA6X"
   },
   "source": [
    "# Simple Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GG7uTtUno9FP"
   },
   "source": [
    "## data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fOQb3I0foJ0u"
   },
   "source": [
    "### convert the all reviews into the lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "G-Bq_JpqSSWp"
   },
   "outputs": [],
   "source": [
    "X_train_simple = X_train.fillna('')\n",
    "X_test_simple = X_test.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "DSOzsfCxo9FP"
   },
   "outputs": [],
   "source": [
    "X_train_simple = X_train_simple.str.lower()\n",
    "X_test_simple = X_test_simple.str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BDIK-j9mo9FQ"
   },
   "source": [
    "### remove the HTML and URLs from the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "3nbVgrPvBkzx"
   },
   "outputs": [],
   "source": [
    "X_train_simple = X_train_simple.apply(lambda x: re.sub('<.*>', '', x))\n",
    "X_test_simple = X_test_simple.apply(lambda x: re.sub('<.*>', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "hKDdpXQsW7XD"
   },
   "outputs": [],
   "source": [
    "X_train_simple = X_train_simple.apply(lambda x: re.sub(r'https?://\\S+', '', x))\n",
    "X_test_simple = X_test_simple.apply(lambda x: re.sub(r'https?://\\S+', '', x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gytGCZQoo9FR"
   },
   "source": [
    "### perform contractions on the reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "jj2E5pjnKJUJ"
   },
   "outputs": [],
   "source": [
    "contractions = { \n",
    "\"ain't\": \"am not / are not / is not / has not / have not\",\n",
    "\"aren't\": \"are not / am not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he had / he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he shall / he will\",\n",
    "\"he'll've\": \"he shall have / he will have\",\n",
    "\"he's\": \"he has / he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how has / how is / how does\",\n",
    "\"I'd\": \"I had / I would\",\n",
    "\"I'd've\": \"I would have\",\n",
    "\"I'll\": \"I shall / I will\",\n",
    "\"I'll've\": \"I shall have / I will have\",\n",
    "\"I'm\": \"I am\",\n",
    "\"I've\": \"I have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it had / it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it shall / it will\",\n",
    "\"it'll've\": \"it shall have / it will have\",\n",
    "\"it's\": \"it has / it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she had / she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she shall / she will\",\n",
    "\"she'll've\": \"she shall have / she will have\",\n",
    "\"she's\": \"she has / she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as / so is\",\n",
    "\"that'd\": \"that would / that had\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that has / that is\",\n",
    "\"there'd\": \"there had / there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there has / there is\",\n",
    "\"they'd\": \"they had / they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they shall / they will\",\n",
    "\"they'll've\": \"they shall have / they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we had / we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what shall / what will\",\n",
    "\"what'll've\": \"what shall have / what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what has / what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when has / when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where has / where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who shall / who will\",\n",
    "\"who'll've\": \"who shall have / who will have\",\n",
    "\"who's\": \"who has / who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why has / why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you had / you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you shall / you will\",\n",
    "\"you'll've\": \"you shall have / you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "sDeVarL8KjSS"
   },
   "outputs": [],
   "source": [
    "contraction_re = re.compile('|'.join(contractions.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "7bieR31Lo9FR"
   },
   "outputs": [],
   "source": [
    "def contractionfunction(s):\n",
    "    def replace(match):\n",
    "      return contractions[match.group(0)]\n",
    "    return contraction_re.sub(replace, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "Zr2F9JirMS74"
   },
   "outputs": [],
   "source": [
    "X_train_simple = X_train_simple.apply(contractionfunction)\n",
    "X_test_simple = X_test_simple.apply(contractionfunction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yuPZxv7uo9FQ"
   },
   "source": [
    "### remove non-alphabetical characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "97nSY3zEo9FR"
   },
   "outputs": [],
   "source": [
    "X_train_simple = X_train_simple.apply(lambda x: re.sub('[^a-zA-Z ]', '', x))\n",
    "X_test_simple = X_test_simple.apply(lambda x: re.sub('[^a-zA-Z ]', '', x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YMB2yBIRo9FR"
   },
   "source": [
    "### remove extra spaces between words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "r8vgO2AAo9FR"
   },
   "outputs": [],
   "source": [
    "X_train_simple = X_train_simple.apply(lambda x: re.sub(r'\\s\\s+', ' ', x))\n",
    "X_test_simple = X_test_simple.apply(lambda x: re.sub(r'\\s\\s+', ' ', x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KJ_OOzJ9o9FS"
   },
   "source": [
    "## pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iqFTNNXwo9FS"
   },
   "source": [
    "### remove the stop words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "6o_B9Nbfo9FS"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "ImjovKeMiJgp"
   },
   "outputs": [],
   "source": [
    "stop_words =set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "uPGB3mUSkcuY"
   },
   "outputs": [],
   "source": [
    "X_train_simple = X_train_simple.apply(word_tokenize)\n",
    "X_test_simple = X_test_simple.apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "zBecH-vDkgnt"
   },
   "outputs": [],
   "source": [
    "def rmstopwords(wordlist):\n",
    "  newlist = []\n",
    "  for w in wordlist:\n",
    "    if not w in stop_words:\n",
    "      newlist.append(w)\n",
    "  return newlist    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "qereHUJOlbfU"
   },
   "outputs": [],
   "source": [
    "X_train_simple = X_train_simple.apply(rmstopwords)\n",
    "X_test_simple = X_test_simple.apply(rmstopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y3HPTa3to9FS"
   },
   "source": [
    "### perform lemmatization  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "_KRoqZkQo9FS"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "RX-2Q0I3yZU7"
   },
   "outputs": [],
   "source": [
    "def lemmafunc(wordlist):\n",
    "  newlist = []\n",
    "  for w in wordlist:\n",
    "    newlist.append(lemmatizer.lemmatize(w))\n",
    "  return newlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "dU5EEbzPybAu"
   },
   "outputs": [],
   "source": [
    "X_train_simple = X_train_simple.apply(lemmafunc)\n",
    "X_test_simple = X_test_simple.apply(lemmafunc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vpmFTWcypdBz"
   },
   "source": [
    "## perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "tOGvFvR4o9FT"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yFHgJuJ2MMLd"
   },
   "source": [
    "### feature extraction using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "zBvfW7X8ODBt"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse.csr import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "3dhJRDYDOL0A"
   },
   "outputs": [],
   "source": [
    "tf = TfidfVectorizer()\n",
    "X_train_tfidf, X_test_tfidf = X_train_simple.apply(', '.join), X_test_simple.apply(', '.join)\n",
    "X_train_tfidf, X_test_tfidf = tf.fit_transform(X_train_tfidf), tf.transform(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ym85xkZFOTed"
   },
   "source": [
    "### feature extraction using my own Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "x-mEds8RQQJC"
   },
   "outputs": [],
   "source": [
    "# Apply the word2v function to compute the average embeddings for the training\n",
    "# and test datasets\n",
    "X_train_own_series = X_train_simple.apply(lambda x: word2v_avg(x, model.wv))\n",
    "X_train_own_mtx = np.array(X_train_own_series.values.tolist())\n",
    "\n",
    "X_test_own_series = X_test_simple.apply(lambda x: word2v_avg(x, model.wv))\n",
    "X_test_own_mtx = np.array(X_test_own_series.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "_41nWc2MSsSQ"
   },
   "outputs": [],
   "source": [
    "# Remove the NaN's (if any) and their labels from the training and test datasets \n",
    "idx_nan_train = idx_nan(X_train_own_mtx)\n",
    "if idx_nan_train == None:\n",
    "  y_train_own = y_train\n",
    "else:\n",
    "  X_train_own_mtx = np.delete(X_train_own_mtx, idx_nan_train, 0)\n",
    "  y_train_own = np.delete(y_train, idx_nan_train)\n",
    "\n",
    "idx_nan_test = idx_nan(X_test_own_mtx)\n",
    "if idx_nan_test == None:\n",
    "  y_test_own = y_test\n",
    "else:\n",
    "  X_test_own_mtx = np.delete(X_test_own_mtx, idx_nan_test, 0)\n",
    "  y_test_own = np.delete(y_test, idx_nan_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0sMF4YyvOtFR"
   },
   "source": [
    "### feature extraction using the pretrained Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "W6HrRCOPUjce"
   },
   "outputs": [],
   "source": [
    "# Apply the word2v function to compute the average embeddings for the training\n",
    "# and test datasets\n",
    "X_train_pretrained_series = X_train_simple.apply(lambda x: word2v_avg(x, wv_google))\n",
    "X_train_pretrained_mtx = np.array(X_train_pretrained_series.values.tolist())\n",
    "\n",
    "X_test_pretrained_series = X_test_simple.apply(lambda x: word2v_avg(x, wv_google))\n",
    "X_test_pretrained_mtx = np.array(X_test_pretrained_series.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "5VF2kcH1Ujce"
   },
   "outputs": [],
   "source": [
    "# Remove the NaN's (if any) and their labels from the training and test datasets\n",
    "idx_nan_train = idx_nan(X_train_pretrained_mtx)\n",
    "if idx_nan_train == None:\n",
    "  y_train_pretrained = y_train\n",
    "else:\n",
    "  X_train_pretrained_mtx = np.delete(X_train_pretrained_mtx, idx_nan_train, 0)\n",
    "  y_train_pretrained = np.delete(y_train, idx_nan_train)\n",
    "\n",
    "idx_nan_test = idx_nan(X_test_pretrained_mtx)\n",
    "if idx_nan_test == None:\n",
    "  y_test_pretrained = y_test\n",
    "else:\n",
    "  X_test_pretrained_mtx = np.delete(X_test_pretrained_mtx, idx_nan_test, 0)\n",
    "  y_test_pretrained = np.delete(y_test, idx_nan_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jF9eeQtxPoJH"
   },
   "source": [
    "### perceptron training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "PuRwvZcLyOWK"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Perceptron()"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_percep_tfidf = Perceptron(random_state=0)\n",
    "clf_percep_own = Perceptron(random_state=0)\n",
    "clf_percep_pretrained = Perceptron(random_state=0)\n",
    "\n",
    "clf_percep_tfidf.fit(X_train_tfidf, y_train)\n",
    "clf_percep_own.fit(X_train_own_mtx, y_train_own)\n",
    "clf_percep_pretrained.fit(X_train_pretrained_mtx, y_train_pretrained)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wDPvIh6eUYFV"
   },
   "source": [
    "### perceptron testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "N-PqI0JNygSB"
   },
   "outputs": [],
   "source": [
    "pred_percep_test_tfidf = clf_percep_tfidf.predict(X_test_tfidf)\n",
    "pred_percep_test_own = clf_percep_own.predict(X_test_own_mtx)\n",
    "pred_percep_test_pretrained = clf_percep_pretrained.predict(X_test_pretrained_mtx)\n",
    "\n",
    "accuracy_perceptron_tfidf = accuracy_score(y_test, pred_percep_test_tfidf)\n",
    "accuracy_perceptron_own = accuracy_score(y_test_own, pred_percep_test_own)\n",
    "accuracy_perceptron_pretrained = accuracy_score(y_test_pretrained, pred_percep_test_pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "vLHyXe53vNq7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron accuracy using the TFIDF feature: 0.8653682684134206\n",
      "Perceptron accuracy using my own Word2Vec model: 0.7985399269963498\n",
      "Perceptron accuracy using the pretrained Word2Vec model: 0.8268913445672283\n"
     ]
    }
   ],
   "source": [
    "print('Perceptron accuracy using the TFIDF feature:', accuracy_perceptron_tfidf)\n",
    "print('Perceptron accuracy using my own Word2Vec model:', accuracy_perceptron_own)\n",
    "print('Perceptron accuracy using the pretrained Word2Vec model:', accuracy_perceptron_pretrained)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dzUbaCdyo9FT"
   },
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "Rg0alluLo9FT"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "45cJTD5Xxuqh"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guanz\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearSVC(random_state=0)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_svm_tfidf = LinearSVC(max_iter=1000, random_state=0)\n",
    "clf_svm_own = LinearSVC(max_iter=1000, random_state=0)\n",
    "clf_svm_pretrained = LinearSVC(max_iter=1000, random_state=0)\n",
    "\n",
    "clf_svm_tfidf.fit(X_train_tfidf, y_train)\n",
    "clf_svm_own.fit(X_train_own_mtx, y_train_own)\n",
    "clf_svm_pretrained.fit(X_train_pretrained_mtx, y_train_pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "g4ObV3MWpTiQ"
   },
   "outputs": [],
   "source": [
    "pred_svm_tfidf = clf_svm_tfidf.predict(X_test_tfidf)\n",
    "pred_svm_own = clf_svm_own.predict(X_test_own_mtx)\n",
    "pred_svm_pretrained = clf_svm_pretrained.predict(X_test_pretrained_mtx)\n",
    "\n",
    "accuracy_svm_tfidf = accuracy_score(y_test, pred_svm_tfidf)\n",
    "accuracy_svm_own = accuracy_score(y_test_own, pred_svm_own)\n",
    "accuracy_svm_pretrained = accuracy_score(y_test_pretrained, pred_svm_pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "VNc8mbS_6OP_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM accuracy using the TFIDF feature: 0.9000450022501125\n",
      "SVM accuracy using my own Word2Vec model: 0.8744687234361718\n",
      "SVM accuracy using the pretrained Word2Vec model: 0.8403170158507925\n"
     ]
    }
   ],
   "source": [
    "print('SVM accuracy using the TFIDF feature:', accuracy_svm_tfidf)\n",
    "print('SVM accuracy using my own Word2Vec model:', accuracy_svm_own)\n",
    "print('SVM accuracy using the pretrained Word2Vec model:', accuracy_svm_pretrained)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J6D0sJTSo9FT"
   },
   "source": [
    "# 4. Feedforward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "IjOF6FPVr9gw"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "ZPj7TddTKGtL"
   },
   "outputs": [],
   "source": [
    "X_train_bi,  X_test_bi, y_train_bi, y_test_bi = X_train, X_test, y_train, y_test\n",
    "X_train_te,  X_test_te, y_train_te, y_test_te = X_train_te, X_test_te, y_train_te, y_test_te"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qXzmGHPpkSII"
   },
   "source": [
    "## a. Use Average Word Embeddings as Input Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9RfayGZc4F9D"
   },
   "source": [
    "## a.1 Use the Pretrained Word2Vec Model to Compute Input Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kCtXlSZ2AqAK"
   },
   "source": [
    "### a.1.1 Compute Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uywCqlI9A4Mw"
   },
   "source": [
    "Input features are average word embeddings of each review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jveMvJQDlKU_"
   },
   "source": [
    "#### Use **the pretrained model** to compute the average word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "3e3QJ6l_79FC"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'X_train_te_series = X_train_te.apply(lambda x: word2v_avg(x, wv_google))\\nX_train_te_avg = np.array(X_train_te_series.values.tolist())\\n\\nX_test_te_series = X_test_te.apply(lambda x: word2v_avg(x, wv_google))\\nX_test_te_avg = np.array(X_test_te_series.values.tolist())'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the word2v_avg function to compute the average embeddings\n",
    "X_train_bi_series = X_train_bi.apply(lambda x: word2v_avg(x, wv_google))\n",
    "X_train_bi_avg = np.array(X_train_bi_series.values.tolist())\n",
    "\n",
    "X_test_bi_series = X_test_bi.apply(lambda x: word2v_avg(x, wv_google))\n",
    "X_test_bi_avg = np.array(X_test_bi_series.values.tolist())\n",
    "\n",
    "X_train_te_series = X_train_te.apply(lambda x: word2v_avg(x, wv_google))\n",
    "X_train_te_avg = np.array(X_train_te_series.values.tolist())\n",
    "\n",
    "X_test_te_series = X_test_te.apply(lambda x: word2v_avg(x, wv_google))\n",
    "X_test_te_avg = np.array(X_test_te_series.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "0k0k3zqZnGVi"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'idx_nan_train_te = idx_nan(X_train_te_avg)\\nif idx_nan_train_te != None:\\n  X_train_te_avg = np.delete(X_train_te_avg, idx_nan_train_te, 0)\\n  y_train_te_avg = np.delete(y_train_te, idx_nan_train_te)\\n\\nidx_nan_test_te = idx_nan(X_test_te_avg)\\nif idx_nan_test_te != None:\\n  X_test_te_avg = np.delete(X_test_te_avg, idx_nan_test_te, 0)\\n  y_test_te_avg = np.delete(y_test_te, idx_nan_test_te)'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the idx_nan function to remove the NaN's and corresponding\n",
    "# labels (if any)\n",
    "idx_nan_train_bi = idx_nan(X_train_bi_avg)\n",
    "if idx_nan_train_bi != None:\n",
    "  X_train_bi_avg = np.delete(X_train_bi_avg, idx_nan_train_bi, 0)\n",
    "  y_train_bi_avg = np.delete(y_train_bi, idx_nan_train_bi)\n",
    "\n",
    "idx_nan_test_bi = idx_nan(X_test_bi_avg)\n",
    "if idx_nan_test_bi != None:\n",
    "  X_test_bi_avg = np.delete(X_test_bi_avg, idx_nan_test_bi, 0)\n",
    "  y_test_bi_avg = np.delete(y_test_bi, idx_nan_test_bi)\n",
    "\n",
    "idx_nan_train_te = idx_nan(X_train_te_avg)\n",
    "if idx_nan_train_te != None:\n",
    "  X_train_te_avg = np.delete(X_train_te_avg, idx_nan_train_te, 0)\n",
    "  y_train_te_avg = np.delete(y_train_te, idx_nan_train_te)\n",
    "\n",
    "idx_nan_test_te = idx_nan(X_test_te_avg)\n",
    "if idx_nan_test_te != None:\n",
    "  X_test_te_avg = np.delete(X_test_te_avg, idx_nan_test_te, 0)\n",
    "  y_test_te_avg = np.delete(y_test_te, idx_nan_test_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cv37YJWexMPK"
   },
   "source": [
    "### a.1.2 Define Dataset Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "vD3xnPy_sZFG"
   },
   "outputs": [],
   "source": [
    "class Train(Dataset):\n",
    "  def __init__(self, xtrain, ytrain):\n",
    "    'Initialization'\n",
    "    self.data = xtrain\n",
    "    self.labels = ytrain\n",
    "\n",
    "  def __len__(self):\n",
    "    'Denotes the total number of samples'\n",
    "    return len(self.data)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    'Generates one sample of data'\n",
    "    X = self.data[index]\n",
    "    y = self.labels[index]\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "i4lUjtKqvpSc"
   },
   "outputs": [],
   "source": [
    "class Test(Dataset):\n",
    "  def __init__(self, xtest, ytest):\n",
    "    'Initialization'\n",
    "    self.data = xtest\n",
    "    self.labels = ytest\n",
    "\n",
    "  def __len__(self):\n",
    "    'Denotes the total number of samples'\n",
    "    return len(self.data)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    'Generates one sample of data'\n",
    "    X = self.data[index]\n",
    "    y = self.labels[index]\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SdqPThFEAvTx"
   },
   "source": [
    "### a.1.3 Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "GBphsKqMLE_J"
   },
   "outputs": [],
   "source": [
    "# use average Word2Vec vectors as input features\n",
    "train_data_bi, test_data_bi = Train(X_train_bi_avg, y_train_bi_avg-1), Test(X_test_bi_avg, y_test_bi_avg-1)\n",
    "train_data_te, test_data_te = Train(X_train_te_avg, y_train_te_avg-1), Test(X_test_te_avg, y_test_te_avg-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "29-rtgftJWvt"
   },
   "source": [
    "#### Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "IRTYWb5VQdAn"
   },
   "outputs": [],
   "source": [
    "# number of subprocesses to use for data loading\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 100\n",
    "# percentage of training set to use as validation\n",
    "valid_size = 0.2\n",
    "\n",
    "# obtain training indices that will be used for validation\n",
    "num_train = len(train_data_bi)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "# define samples for obtaining training and validation batches\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "# prepare data loaders\n",
    "train_loader_bi = torch.utils.data.DataLoader(train_data_bi, batch_size=batch_size,\n",
    "                                           sampler=train_sampler, num_workers\n",
    "                                           =num_workers)\n",
    "valid_loader_bi = torch.utils.data.DataLoader(train_data_bi, batch_size=batch_size,\n",
    "                                           sampler=valid_sampler, num_workers\n",
    "                                           =num_workers)\n",
    "test_loader_bi = torch.utils.data.DataLoader(test_data_bi, batch_size=batch_size,\n",
    "                                           num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8eVGtO59KqTK"
   },
   "source": [
    "#### Ternary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "IVys38UUKqTK"
   },
   "outputs": [],
   "source": [
    "# number of subprocesses to use for data loading\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 100\n",
    "# percentage of training set to use as validation\n",
    "valid_size = 0.2\n",
    "\n",
    "# obtain training indices that will be used for validation\n",
    "num_train = len(train_data_te)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "# define samples for obtaining training and validation batches\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "# prepare data loaders\n",
    "train_loader_te = torch.utils.data.DataLoader(train_data_te, batch_size=batch_size,\n",
    "                                           sampler=train_sampler, num_workers\n",
    "                                           =num_workers)\n",
    "valid_loader_te = torch.utils.data.DataLoader(train_data_te, batch_size=batch_size,\n",
    "                                           sampler=valid_sampler, num_workers\n",
    "                                           =num_workers)\n",
    "test_loader_te = torch.utils.data.DataLoader(test_data_te, batch_size=batch_size,\n",
    "                                           num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z_3ER6s2xaQ6"
   },
   "source": [
    "### a.1.4 Define the MLP architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "293rPsP9wAOC"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "nm3huCpNxnb8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ThreeLayerMLP(\n",
      "  (linear1): Linear(in_features=300, out_features=50, bias=True)\n",
      "  (linear2): Linear(in_features=50, out_features=10, bias=True)\n",
      "  (linear3): Linear(in_features=10, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "ThreeLayerMLP(\n",
      "  (linear1): Linear(in_features=300, out_features=50, bias=True)\n",
      "  (linear2): Linear(in_features=50, out_features=10, bias=True)\n",
      "  (linear3): Linear(in_features=10, out_features=3, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# define the MLP architecture\n",
    "class ThreeLayerMLP(nn.Module):\n",
    "  def __init__(self, D_in, H1, H2, D_out):\n",
    "    super().__init__()\n",
    "    self.linear1 = nn.Linear(D_in, H1)\n",
    "    self.linear2 = nn.Linear(H1, H2)\n",
    "    self.linear3 = nn.Linear(H2, D_out)\n",
    "    # dropout layer (p=0.2)\n",
    "    self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "  def forward(self, x):\n",
    "    # add hidden layer, with relu activation function\n",
    "    h1_relu = F.relu(self.linear1(x))\n",
    "    # add dropout layer\n",
    "    h1_drop = self.dropout(h1_relu)\n",
    "    # add another hidden layer, with relu activation function\n",
    "    h2_relu = F.relu(self.linear2(h1_drop))\n",
    "    # add another dropout layer\n",
    "    h2_drop = self.dropout(h2_relu)\n",
    "    # add output layer\n",
    "    h2_output = self.linear3(h2_drop)\n",
    "\n",
    "    return h2_output\n",
    "\n",
    "# initialize MLP\n",
    "model_bi = ThreeLayerMLP(300, 50, 10, 2)\n",
    "model_te = ThreeLayerMLP(300, 50, 10, 3)\n",
    "model_bi.cuda()\n",
    "model_te.cuda()\n",
    "print(model_bi)\n",
    "print(model_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "wL4zzdUk7_-X"
   },
   "outputs": [],
   "source": [
    "# specify loss function (categorical cross-entropy)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# specify optimizer (stochastic gradient descent) and learning rate = 0.01\n",
    "optimizer_bi = torch.optim.SGD(model_bi.parameters(), lr=0.0075)\n",
    "optimizer_te = torch.optim.SGD(model_te.parameters(), lr=0.0075)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U2M-U1t-9Qox"
   },
   "source": [
    "### a.1.5 Train the Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_cjNvAGiM0aA"
   },
   "source": [
    "#### Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "PE_85oqq9P5S"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.554523 \tValidation Loss: 0.138488\n",
      "Validation loss decreased (inf --> 0.138488).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 0.553574 \tValidation Loss: 0.138265\n",
      "Validation loss decreased (0.138488 --> 0.138265).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 0.552288 \tValidation Loss: 0.137838\n",
      "Validation loss decreased (0.138265 --> 0.137838).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 0.549726 \tValidation Loss: 0.136853\n",
      "Validation loss decreased (0.137838 --> 0.136853).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 0.543424 \tValidation Loss: 0.134285\n",
      "Validation loss decreased (0.136853 --> 0.134285).  Saving model ...\n",
      "Epoch: 6 \tTraining Loss: 0.526983 \tValidation Loss: 0.127493\n",
      "Validation loss decreased (0.134285 --> 0.127493).  Saving model ...\n",
      "Epoch: 7 \tTraining Loss: 0.492716 \tValidation Loss: 0.115267\n",
      "Validation loss decreased (0.127493 --> 0.115267).  Saving model ...\n",
      "Epoch: 8 \tTraining Loss: 0.448807 \tValidation Loss: 0.102999\n",
      "Validation loss decreased (0.115267 --> 0.102999).  Saving model ...\n",
      "Epoch: 9 \tTraining Loss: 0.414688 \tValidation Loss: 0.094742\n",
      "Validation loss decreased (0.102999 --> 0.094742).  Saving model ...\n",
      "Epoch: 10 \tTraining Loss: 0.391574 \tValidation Loss: 0.089179\n",
      "Validation loss decreased (0.094742 --> 0.089179).  Saving model ...\n",
      "Epoch: 11 \tTraining Loss: 0.377378 \tValidation Loss: 0.085570\n",
      "Validation loss decreased (0.089179 --> 0.085570).  Saving model ...\n",
      "Epoch: 12 \tTraining Loss: 0.366474 \tValidation Loss: 0.083533\n",
      "Validation loss decreased (0.085570 --> 0.083533).  Saving model ...\n",
      "Epoch: 13 \tTraining Loss: 0.357640 \tValidation Loss: 0.080959\n",
      "Validation loss decreased (0.083533 --> 0.080959).  Saving model ...\n",
      "Epoch: 14 \tTraining Loss: 0.351848 \tValidation Loss: 0.079515\n",
      "Validation loss decreased (0.080959 --> 0.079515).  Saving model ...\n",
      "Epoch: 15 \tTraining Loss: 0.346607 \tValidation Loss: 0.078486\n",
      "Validation loss decreased (0.079515 --> 0.078486).  Saving model ...\n",
      "Epoch: 16 \tTraining Loss: 0.340419 \tValidation Loss: 0.077137\n",
      "Validation loss decreased (0.078486 --> 0.077137).  Saving model ...\n",
      "Epoch: 17 \tTraining Loss: 0.336842 \tValidation Loss: 0.076919\n",
      "Validation loss decreased (0.077137 --> 0.076919).  Saving model ...\n",
      "Epoch: 18 \tTraining Loss: 0.332683 \tValidation Loss: 0.075317\n",
      "Validation loss decreased (0.076919 --> 0.075317).  Saving model ...\n",
      "Epoch: 19 \tTraining Loss: 0.330160 \tValidation Loss: 0.075301\n",
      "Validation loss decreased (0.075317 --> 0.075301).  Saving model ...\n",
      "Epoch: 20 \tTraining Loss: 0.327416 \tValidation Loss: 0.074538\n",
      "Validation loss decreased (0.075301 --> 0.074538).  Saving model ...\n",
      "Epoch: 21 \tTraining Loss: 0.324512 \tValidation Loss: 0.073605\n",
      "Validation loss decreased (0.074538 --> 0.073605).  Saving model ...\n",
      "Epoch: 22 \tTraining Loss: 0.320611 \tValidation Loss: 0.074345\n",
      "Epoch: 23 \tTraining Loss: 0.318507 \tValidation Loss: 0.072721\n",
      "Validation loss decreased (0.073605 --> 0.072721).  Saving model ...\n",
      "Epoch: 24 \tTraining Loss: 0.314792 \tValidation Loss: 0.072974\n",
      "Epoch: 25 \tTraining Loss: 0.314289 \tValidation Loss: 0.071927\n",
      "Validation loss decreased (0.072721 --> 0.071927).  Saving model ...\n",
      "Epoch: 26 \tTraining Loss: 0.313526 \tValidation Loss: 0.072421\n",
      "Epoch: 27 \tTraining Loss: 0.310973 \tValidation Loss: 0.071195\n",
      "Validation loss decreased (0.071927 --> 0.071195).  Saving model ...\n",
      "Epoch: 28 \tTraining Loss: 0.309482 \tValidation Loss: 0.071050\n",
      "Validation loss decreased (0.071195 --> 0.071050).  Saving model ...\n",
      "Epoch: 29 \tTraining Loss: 0.307668 \tValidation Loss: 0.070791\n",
      "Validation loss decreased (0.071050 --> 0.070791).  Saving model ...\n",
      "Epoch: 30 \tTraining Loss: 0.305857 \tValidation Loss: 0.070527\n",
      "Validation loss decreased (0.070791 --> 0.070527).  Saving model ...\n",
      "Epoch: 31 \tTraining Loss: 0.305520 \tValidation Loss: 0.070865\n",
      "Epoch: 32 \tTraining Loss: 0.303837 \tValidation Loss: 0.069864\n",
      "Validation loss decreased (0.070527 --> 0.069864).  Saving model ...\n",
      "Epoch: 33 \tTraining Loss: 0.303516 \tValidation Loss: 0.069868\n",
      "Epoch: 34 \tTraining Loss: 0.302561 \tValidation Loss: 0.069385\n",
      "Validation loss decreased (0.069864 --> 0.069385).  Saving model ...\n",
      "Epoch: 35 \tTraining Loss: 0.300789 \tValidation Loss: 0.069277\n",
      "Validation loss decreased (0.069385 --> 0.069277).  Saving model ...\n",
      "Epoch: 36 \tTraining Loss: 0.300868 \tValidation Loss: 0.069696\n",
      "Epoch: 37 \tTraining Loss: 0.299130 \tValidation Loss: 0.069249\n",
      "Validation loss decreased (0.069277 --> 0.069249).  Saving model ...\n",
      "Epoch: 38 \tTraining Loss: 0.298782 \tValidation Loss: 0.068997\n",
      "Validation loss decreased (0.069249 --> 0.068997).  Saving model ...\n",
      "Epoch: 39 \tTraining Loss: 0.297465 \tValidation Loss: 0.068617\n",
      "Validation loss decreased (0.068997 --> 0.068617).  Saving model ...\n",
      "Epoch: 40 \tTraining Loss: 0.296462 \tValidation Loss: 0.068837\n",
      "Epoch: 41 \tTraining Loss: 0.295899 \tValidation Loss: 0.068283\n",
      "Validation loss decreased (0.068617 --> 0.068283).  Saving model ...\n",
      "Epoch: 42 \tTraining Loss: 0.294659 \tValidation Loss: 0.069138\n",
      "Epoch: 43 \tTraining Loss: 0.295124 \tValidation Loss: 0.067980\n",
      "Validation loss decreased (0.068283 --> 0.067980).  Saving model ...\n",
      "Epoch: 44 \tTraining Loss: 0.294739 \tValidation Loss: 0.069084\n",
      "Epoch: 45 \tTraining Loss: 0.292606 \tValidation Loss: 0.067774\n",
      "Validation loss decreased (0.067980 --> 0.067774).  Saving model ...\n",
      "Epoch: 46 \tTraining Loss: 0.292314 \tValidation Loss: 0.067386\n",
      "Validation loss decreased (0.067774 --> 0.067386).  Saving model ...\n",
      "Epoch: 47 \tTraining Loss: 0.291171 \tValidation Loss: 0.067207\n",
      "Validation loss decreased (0.067386 --> 0.067207).  Saving model ...\n",
      "Epoch: 48 \tTraining Loss: 0.290906 \tValidation Loss: 0.067295\n",
      "Epoch: 49 \tTraining Loss: 0.290317 \tValidation Loss: 0.067051\n",
      "Validation loss decreased (0.067207 --> 0.067051).  Saving model ...\n",
      "Epoch: 50 \tTraining Loss: 0.288777 \tValidation Loss: 0.066683\n",
      "Validation loss decreased (0.067051 --> 0.066683).  Saving model ...\n",
      "Epoch: 51 \tTraining Loss: 0.288250 \tValidation Loss: 0.066783\n",
      "Epoch: 52 \tTraining Loss: 0.287377 \tValidation Loss: 0.067253\n",
      "Epoch: 53 \tTraining Loss: 0.287690 \tValidation Loss: 0.066435\n",
      "Validation loss decreased (0.066683 --> 0.066435).  Saving model ...\n",
      "Epoch: 54 \tTraining Loss: 0.287178 \tValidation Loss: 0.066247\n",
      "Validation loss decreased (0.066435 --> 0.066247).  Saving model ...\n",
      "Epoch: 55 \tTraining Loss: 0.286089 \tValidation Loss: 0.066351\n",
      "Epoch: 56 \tTraining Loss: 0.285790 \tValidation Loss: 0.066205\n",
      "Validation loss decreased (0.066247 --> 0.066205).  Saving model ...\n",
      "Epoch: 57 \tTraining Loss: 0.285366 \tValidation Loss: 0.066321\n",
      "Epoch: 58 \tTraining Loss: 0.284902 \tValidation Loss: 0.065875\n",
      "Validation loss decreased (0.066205 --> 0.065875).  Saving model ...\n",
      "Epoch: 59 \tTraining Loss: 0.284828 \tValidation Loss: 0.065681\n",
      "Validation loss decreased (0.065875 --> 0.065681).  Saving model ...\n",
      "Epoch: 60 \tTraining Loss: 0.283484 \tValidation Loss: 0.065854\n",
      "Epoch: 61 \tTraining Loss: 0.282779 \tValidation Loss: 0.065658\n",
      "Validation loss decreased (0.065681 --> 0.065658).  Saving model ...\n",
      "Epoch: 62 \tTraining Loss: 0.281754 \tValidation Loss: 0.066942\n",
      "Epoch: 63 \tTraining Loss: 0.282296 \tValidation Loss: 0.065230\n",
      "Validation loss decreased (0.065658 --> 0.065230).  Saving model ...\n",
      "Epoch: 64 \tTraining Loss: 0.280626 \tValidation Loss: 0.065126\n",
      "Validation loss decreased (0.065230 --> 0.065126).  Saving model ...\n",
      "Epoch: 65 \tTraining Loss: 0.280943 \tValidation Loss: 0.064853\n",
      "Validation loss decreased (0.065126 --> 0.064853).  Saving model ...\n",
      "Epoch: 66 \tTraining Loss: 0.279567 \tValidation Loss: 0.065008\n",
      "Epoch: 67 \tTraining Loss: 0.279931 \tValidation Loss: 0.064830\n",
      "Validation loss decreased (0.064853 --> 0.064830).  Saving model ...\n",
      "Epoch: 68 \tTraining Loss: 0.278006 \tValidation Loss: 0.064803\n",
      "Validation loss decreased (0.064830 --> 0.064803).  Saving model ...\n",
      "Epoch: 69 \tTraining Loss: 0.278476 \tValidation Loss: 0.064255\n",
      "Validation loss decreased (0.064803 --> 0.064255).  Saving model ...\n",
      "Epoch: 70 \tTraining Loss: 0.277991 \tValidation Loss: 0.064229\n",
      "Validation loss decreased (0.064255 --> 0.064229).  Saving model ...\n",
      "Epoch: 71 \tTraining Loss: 0.278633 \tValidation Loss: 0.064607\n",
      "Epoch: 72 \tTraining Loss: 0.277512 \tValidation Loss: 0.063972\n",
      "Validation loss decreased (0.064229 --> 0.063972).  Saving model ...\n",
      "Epoch: 73 \tTraining Loss: 0.275675 \tValidation Loss: 0.063903\n",
      "Validation loss decreased (0.063972 --> 0.063903).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 74 \tTraining Loss: 0.276965 \tValidation Loss: 0.065050\n",
      "Epoch: 75 \tTraining Loss: 0.275714 \tValidation Loss: 0.063691\n",
      "Validation loss decreased (0.063903 --> 0.063691).  Saving model ...\n",
      "Epoch: 76 \tTraining Loss: 0.275364 \tValidation Loss: 0.063578\n",
      "Validation loss decreased (0.063691 --> 0.063578).  Saving model ...\n",
      "Epoch: 77 \tTraining Loss: 0.274936 \tValidation Loss: 0.064026\n",
      "Epoch: 78 \tTraining Loss: 0.274115 \tValidation Loss: 0.063527\n",
      "Validation loss decreased (0.063578 --> 0.063527).  Saving model ...\n",
      "Epoch: 79 \tTraining Loss: 0.273823 \tValidation Loss: 0.063227\n",
      "Validation loss decreased (0.063527 --> 0.063227).  Saving model ...\n",
      "Epoch: 80 \tTraining Loss: 0.273607 \tValidation Loss: 0.063390\n",
      "Epoch: 81 \tTraining Loss: 0.273021 \tValidation Loss: 0.063180\n",
      "Validation loss decreased (0.063227 --> 0.063180).  Saving model ...\n",
      "Epoch: 82 \tTraining Loss: 0.271512 \tValidation Loss: 0.062850\n",
      "Validation loss decreased (0.063180 --> 0.062850).  Saving model ...\n",
      "Epoch: 83 \tTraining Loss: 0.272120 \tValidation Loss: 0.063140\n",
      "Epoch: 84 \tTraining Loss: 0.271445 \tValidation Loss: 0.062823\n",
      "Validation loss decreased (0.062850 --> 0.062823).  Saving model ...\n",
      "Epoch: 85 \tTraining Loss: 0.270936 \tValidation Loss: 0.063277\n",
      "Epoch: 86 \tTraining Loss: 0.271687 \tValidation Loss: 0.062968\n",
      "Epoch: 87 \tTraining Loss: 0.271424 \tValidation Loss: 0.062716\n",
      "Validation loss decreased (0.062823 --> 0.062716).  Saving model ...\n",
      "Epoch: 88 \tTraining Loss: 0.270706 \tValidation Loss: 0.062652\n",
      "Validation loss decreased (0.062716 --> 0.062652).  Saving model ...\n",
      "Epoch: 89 \tTraining Loss: 0.270684 \tValidation Loss: 0.063275\n",
      "Epoch: 90 \tTraining Loss: 0.269512 \tValidation Loss: 0.063387\n",
      "Epoch: 91 \tTraining Loss: 0.269223 \tValidation Loss: 0.062765\n",
      "Epoch: 92 \tTraining Loss: 0.268863 \tValidation Loss: 0.062500\n",
      "Validation loss decreased (0.062652 --> 0.062500).  Saving model ...\n",
      "Epoch: 93 \tTraining Loss: 0.267679 \tValidation Loss: 0.062462\n",
      "Validation loss decreased (0.062500 --> 0.062462).  Saving model ...\n",
      "Epoch: 94 \tTraining Loss: 0.268214 \tValidation Loss: 0.062063\n",
      "Validation loss decreased (0.062462 --> 0.062063).  Saving model ...\n",
      "Epoch: 95 \tTraining Loss: 0.268331 \tValidation Loss: 0.062341\n",
      "Epoch: 96 \tTraining Loss: 0.268139 \tValidation Loss: 0.062030\n",
      "Validation loss decreased (0.062063 --> 0.062030).  Saving model ...\n",
      "Epoch: 97 \tTraining Loss: 0.267864 \tValidation Loss: 0.062217\n",
      "Epoch: 98 \tTraining Loss: 0.266872 \tValidation Loss: 0.061946\n",
      "Validation loss decreased (0.062030 --> 0.061946).  Saving model ...\n",
      "Epoch: 99 \tTraining Loss: 0.266601 \tValidation Loss: 0.061827\n",
      "Validation loss decreased (0.061946 --> 0.061827).  Saving model ...\n",
      "Epoch: 100 \tTraining Loss: 0.266891 \tValidation Loss: 0.061741\n",
      "Validation loss decreased (0.061827 --> 0.061741).  Saving model ...\n",
      "Time elapsed: 338.82 s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "# number of epochs to train the model\n",
    "n_epochs = 100\n",
    "\n",
    "# initialize tracker for minimum validation loss\n",
    "valid_loss_min = np.Inf # set initial \"min\" to infinity\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "  # monitor training loss\n",
    "  train_loss = 0.0\n",
    "  valid_loss = 0.0\n",
    "\n",
    "  ###################\n",
    "  # train the model #\n",
    "  ###################\n",
    "  model_bi.train() # prep model for training\n",
    "  for data, target in train_loader_bi:\n",
    "    # transfer data and target to GPU\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    # clear the gradients of all optimized variables\n",
    "    optimizer_bi.zero_grad()\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output = model_bi(data.float())\n",
    "    # calculate the loss\n",
    "    loss = criterion(output, target)\n",
    "    # backward pass: compute gradient of the loss with respect to model parameters\n",
    "    loss.backward()\n",
    "    # perform a single optimization step (parameter update)\n",
    "    optimizer_bi.step()\n",
    "    # update running training loss\n",
    "    train_loss += loss.item()*data.size(0)\n",
    "\n",
    "  ######################\n",
    "  # validate the model #\n",
    "  ######################\n",
    "  model_bi.eval() # prep model for evaluation\n",
    "  for data, target in valid_loader_bi:\n",
    "    # transfer data and target to GPU\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output = model_bi(data.float())\n",
    "    # calculate the loss\n",
    "    loss = criterion(output, target)\n",
    "    # update running validation loss\n",
    "    valid_loss += loss.item()*data.size(0)\n",
    "    \n",
    "\n",
    "  # print training/validation statistics\n",
    "  # calculate average loss over an epoch\n",
    "  train_loss = train_loss/len(train_loader_bi.dataset)\n",
    "  valid_loss = valid_loss/len(valid_loader_bi.dataset)\n",
    "\n",
    "  print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch+1, train_loss, valid_loss))\n",
    "    \n",
    "  # save model if validation loss has decreased\n",
    "  if valid_loss <= valid_loss_min:\n",
    "    print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'\n",
    "        .format(valid_loss_min, valid_loss))\n",
    "    torch.save(model_bi.state_dict(), 'model.pt')\n",
    "    valid_loss_min = valid_loss\n",
    "\n",
    "end = time.time()\n",
    "print('Time elapsed: %.2f s' % (end - start))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "ULEHnzgATzLa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model with the lowest validation loss\n",
    "model_bi.load_state_dict(torch.load('model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "Eoh4cvBUUz_V"
   },
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for out outputs\n",
    "with torch.no_grad():\n",
    "  for data in test_loader_bi:\n",
    "    embeddings, labels = data\n",
    "    # calculating outputs by running embeddings through the network\n",
    "    model_bi.to(\"cpu\")\n",
    "    outputs = model_bi(embeddings.float())\n",
    "    # the class with the highest score is what we choose as prediction\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "KDHbF-hhj3JT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy_MLP-model_average-word-vectors_pretrained-word2vec-model_binary-test-set: 86 %\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy_MLP-model_average-word-vectors_pretrained-word2vec-model_binary-test-set: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hh4LDnCGNCFl"
   },
   "source": [
    "#### Ternary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "JBOX2tJoM_oI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.845751 \tValidation Loss: 0.210990\n",
      "Validation loss decreased (inf --> 0.210990).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 0.842704 \tValidation Loss: 0.210840\n",
      "Validation loss decreased (0.210990 --> 0.210840).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 0.842051 \tValidation Loss: 0.210595\n",
      "Validation loss decreased (0.210840 --> 0.210595).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 0.840445 \tValidation Loss: 0.209949\n",
      "Validation loss decreased (0.210595 --> 0.209949).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 0.836692 \tValidation Loss: 0.208559\n",
      "Validation loss decreased (0.209949 --> 0.208559).  Saving model ...\n",
      "Epoch: 6 \tTraining Loss: 0.828696 \tValidation Loss: 0.205343\n",
      "Validation loss decreased (0.208559 --> 0.205343).  Saving model ...\n",
      "Epoch: 7 \tTraining Loss: 0.810447 \tValidation Loss: 0.198367\n",
      "Validation loss decreased (0.205343 --> 0.198367).  Saving model ...\n",
      "Epoch: 8 \tTraining Loss: 0.778538 \tValidation Loss: 0.187681\n",
      "Validation loss decreased (0.198367 --> 0.187681).  Saving model ...\n",
      "Epoch: 9 \tTraining Loss: 0.741890 \tValidation Loss: 0.177414\n",
      "Validation loss decreased (0.187681 --> 0.177414).  Saving model ...\n",
      "Epoch: 10 \tTraining Loss: 0.710531 \tValidation Loss: 0.170585\n",
      "Validation loss decreased (0.177414 --> 0.170585).  Saving model ...\n",
      "Epoch: 11 \tTraining Loss: 0.689844 \tValidation Loss: 0.165579\n",
      "Validation loss decreased (0.170585 --> 0.165579).  Saving model ...\n",
      "Epoch: 12 \tTraining Loss: 0.676824 \tValidation Loss: 0.162817\n",
      "Validation loss decreased (0.165579 --> 0.162817).  Saving model ...\n",
      "Epoch: 13 \tTraining Loss: 0.666484 \tValidation Loss: 0.160548\n",
      "Validation loss decreased (0.162817 --> 0.160548).  Saving model ...\n",
      "Epoch: 14 \tTraining Loss: 0.659236 \tValidation Loss: 0.158848\n",
      "Validation loss decreased (0.160548 --> 0.158848).  Saving model ...\n",
      "Epoch: 15 \tTraining Loss: 0.652791 \tValidation Loss: 0.157568\n",
      "Validation loss decreased (0.158848 --> 0.157568).  Saving model ...\n",
      "Epoch: 16 \tTraining Loss: 0.648027 \tValidation Loss: 0.156262\n",
      "Validation loss decreased (0.157568 --> 0.156262).  Saving model ...\n",
      "Epoch: 17 \tTraining Loss: 0.643614 \tValidation Loss: 0.155374\n",
      "Validation loss decreased (0.156262 --> 0.155374).  Saving model ...\n",
      "Epoch: 18 \tTraining Loss: 0.640032 \tValidation Loss: 0.154460\n",
      "Validation loss decreased (0.155374 --> 0.154460).  Saving model ...\n",
      "Epoch: 19 \tTraining Loss: 0.636563 \tValidation Loss: 0.153585\n",
      "Validation loss decreased (0.154460 --> 0.153585).  Saving model ...\n",
      "Epoch: 20 \tTraining Loss: 0.632138 \tValidation Loss: 0.152921\n",
      "Validation loss decreased (0.153585 --> 0.152921).  Saving model ...\n",
      "Epoch: 21 \tTraining Loss: 0.630746 \tValidation Loss: 0.152611\n",
      "Validation loss decreased (0.152921 --> 0.152611).  Saving model ...\n",
      "Epoch: 22 \tTraining Loss: 0.628016 \tValidation Loss: 0.151720\n",
      "Validation loss decreased (0.152611 --> 0.151720).  Saving model ...\n",
      "Epoch: 23 \tTraining Loss: 0.626634 \tValidation Loss: 0.151551\n",
      "Validation loss decreased (0.151720 --> 0.151551).  Saving model ...\n",
      "Epoch: 24 \tTraining Loss: 0.624170 \tValidation Loss: 0.150963\n",
      "Validation loss decreased (0.151551 --> 0.150963).  Saving model ...\n",
      "Epoch: 25 \tTraining Loss: 0.623741 \tValidation Loss: 0.151897\n",
      "Epoch: 26 \tTraining Loss: 0.621415 \tValidation Loss: 0.150694\n",
      "Validation loss decreased (0.150963 --> 0.150694).  Saving model ...\n",
      "Epoch: 27 \tTraining Loss: 0.619419 \tValidation Loss: 0.150075\n",
      "Validation loss decreased (0.150694 --> 0.150075).  Saving model ...\n",
      "Epoch: 28 \tTraining Loss: 0.617212 \tValidation Loss: 0.149435\n",
      "Validation loss decreased (0.150075 --> 0.149435).  Saving model ...\n",
      "Epoch: 29 \tTraining Loss: 0.616757 \tValidation Loss: 0.149150\n",
      "Validation loss decreased (0.149435 --> 0.149150).  Saving model ...\n",
      "Epoch: 30 \tTraining Loss: 0.615590 \tValidation Loss: 0.149032\n",
      "Validation loss decreased (0.149150 --> 0.149032).  Saving model ...\n",
      "Epoch: 31 \tTraining Loss: 0.614130 \tValidation Loss: 0.148986\n",
      "Validation loss decreased (0.149032 --> 0.148986).  Saving model ...\n",
      "Epoch: 32 \tTraining Loss: 0.613125 \tValidation Loss: 0.148265\n",
      "Validation loss decreased (0.148986 --> 0.148265).  Saving model ...\n",
      "Epoch: 33 \tTraining Loss: 0.611233 \tValidation Loss: 0.148078\n",
      "Validation loss decreased (0.148265 --> 0.148078).  Saving model ...\n",
      "Epoch: 34 \tTraining Loss: 0.611329 \tValidation Loss: 0.147675\n",
      "Validation loss decreased (0.148078 --> 0.147675).  Saving model ...\n",
      "Epoch: 35 \tTraining Loss: 0.610805 \tValidation Loss: 0.148655\n",
      "Epoch: 36 \tTraining Loss: 0.609308 \tValidation Loss: 0.147373\n",
      "Validation loss decreased (0.147675 --> 0.147373).  Saving model ...\n",
      "Epoch: 37 \tTraining Loss: 0.608302 \tValidation Loss: 0.146999\n",
      "Validation loss decreased (0.147373 --> 0.146999).  Saving model ...\n",
      "Epoch: 38 \tTraining Loss: 0.607841 \tValidation Loss: 0.146989\n",
      "Validation loss decreased (0.146999 --> 0.146989).  Saving model ...\n",
      "Epoch: 39 \tTraining Loss: 0.606166 \tValidation Loss: 0.146663\n",
      "Validation loss decreased (0.146989 --> 0.146663).  Saving model ...\n",
      "Epoch: 40 \tTraining Loss: 0.606290 \tValidation Loss: 0.146728\n",
      "Epoch: 41 \tTraining Loss: 0.605088 \tValidation Loss: 0.146407\n",
      "Validation loss decreased (0.146663 --> 0.146407).  Saving model ...\n",
      "Epoch: 42 \tTraining Loss: 0.604359 \tValidation Loss: 0.146576\n",
      "Epoch: 43 \tTraining Loss: 0.603578 \tValidation Loss: 0.145873\n",
      "Validation loss decreased (0.146407 --> 0.145873).  Saving model ...\n",
      "Epoch: 44 \tTraining Loss: 0.602338 \tValidation Loss: 0.145612\n",
      "Validation loss decreased (0.145873 --> 0.145612).  Saving model ...\n",
      "Epoch: 45 \tTraining Loss: 0.602676 \tValidation Loss: 0.145920\n",
      "Epoch: 46 \tTraining Loss: 0.601701 \tValidation Loss: 0.145306\n",
      "Validation loss decreased (0.145612 --> 0.145306).  Saving model ...\n",
      "Epoch: 47 \tTraining Loss: 0.601346 \tValidation Loss: 0.145337\n",
      "Epoch: 48 \tTraining Loss: 0.599676 \tValidation Loss: 0.144808\n",
      "Validation loss decreased (0.145306 --> 0.144808).  Saving model ...\n",
      "Epoch: 49 \tTraining Loss: 0.599778 \tValidation Loss: 0.144612\n",
      "Validation loss decreased (0.144808 --> 0.144612).  Saving model ...\n",
      "Epoch: 50 \tTraining Loss: 0.598525 \tValidation Loss: 0.146898\n",
      "Epoch: 51 \tTraining Loss: 0.598845 \tValidation Loss: 0.144771\n",
      "Epoch: 52 \tTraining Loss: 0.598300 \tValidation Loss: 0.144237\n",
      "Validation loss decreased (0.144612 --> 0.144237).  Saving model ...\n",
      "Epoch: 53 \tTraining Loss: 0.596047 \tValidation Loss: 0.144042\n",
      "Validation loss decreased (0.144237 --> 0.144042).  Saving model ...\n",
      "Epoch: 54 \tTraining Loss: 0.595870 \tValidation Loss: 0.143868\n",
      "Validation loss decreased (0.144042 --> 0.143868).  Saving model ...\n",
      "Epoch: 55 \tTraining Loss: 0.595314 \tValidation Loss: 0.143554\n",
      "Validation loss decreased (0.143868 --> 0.143554).  Saving model ...\n",
      "Epoch: 56 \tTraining Loss: 0.594898 \tValidation Loss: 0.143846\n",
      "Epoch: 57 \tTraining Loss: 0.594960 \tValidation Loss: 0.143227\n",
      "Validation loss decreased (0.143554 --> 0.143227).  Saving model ...\n",
      "Epoch: 58 \tTraining Loss: 0.593526 \tValidation Loss: 0.143203\n",
      "Validation loss decreased (0.143227 --> 0.143203).  Saving model ...\n",
      "Epoch: 59 \tTraining Loss: 0.593403 \tValidation Loss: 0.142999\n",
      "Validation loss decreased (0.143203 --> 0.142999).  Saving model ...\n",
      "Epoch: 60 \tTraining Loss: 0.592348 \tValidation Loss: 0.142730\n",
      "Validation loss decreased (0.142999 --> 0.142730).  Saving model ...\n",
      "Epoch: 61 \tTraining Loss: 0.591952 \tValidation Loss: 0.142508\n",
      "Validation loss decreased (0.142730 --> 0.142508).  Saving model ...\n",
      "Epoch: 62 \tTraining Loss: 0.591475 \tValidation Loss: 0.142354\n",
      "Validation loss decreased (0.142508 --> 0.142354).  Saving model ...\n",
      "Epoch: 63 \tTraining Loss: 0.591341 \tValidation Loss: 0.142744\n",
      "Epoch: 64 \tTraining Loss: 0.589969 \tValidation Loss: 0.142444\n",
      "Epoch: 65 \tTraining Loss: 0.590220 \tValidation Loss: 0.142387\n",
      "Epoch: 66 \tTraining Loss: 0.588907 \tValidation Loss: 0.142352\n",
      "Validation loss decreased (0.142354 --> 0.142352).  Saving model ...\n",
      "Epoch: 67 \tTraining Loss: 0.588222 \tValidation Loss: 0.141635\n",
      "Validation loss decreased (0.142352 --> 0.141635).  Saving model ...\n",
      "Epoch: 68 \tTraining Loss: 0.589264 \tValidation Loss: 0.141466\n",
      "Validation loss decreased (0.141635 --> 0.141466).  Saving model ...\n",
      "Epoch: 69 \tTraining Loss: 0.587167 \tValidation Loss: 0.141182\n",
      "Validation loss decreased (0.141466 --> 0.141182).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 70 \tTraining Loss: 0.587334 \tValidation Loss: 0.141272\n",
      "Epoch: 71 \tTraining Loss: 0.586348 \tValidation Loss: 0.140900\n",
      "Validation loss decreased (0.141182 --> 0.140900).  Saving model ...\n",
      "Epoch: 72 \tTraining Loss: 0.585988 \tValidation Loss: 0.140685\n",
      "Validation loss decreased (0.140900 --> 0.140685).  Saving model ...\n",
      "Epoch: 73 \tTraining Loss: 0.584161 \tValidation Loss: 0.141339\n",
      "Epoch: 74 \tTraining Loss: 0.583918 \tValidation Loss: 0.140928\n",
      "Epoch: 75 \tTraining Loss: 0.583583 \tValidation Loss: 0.140092\n",
      "Validation loss decreased (0.140685 --> 0.140092).  Saving model ...\n",
      "Epoch: 76 \tTraining Loss: 0.582236 \tValidation Loss: 0.140449\n",
      "Epoch: 77 \tTraining Loss: 0.583532 \tValidation Loss: 0.140234\n",
      "Epoch: 78 \tTraining Loss: 0.582220 \tValidation Loss: 0.139757\n",
      "Validation loss decreased (0.140092 --> 0.139757).  Saving model ...\n",
      "Epoch: 79 \tTraining Loss: 0.581845 \tValidation Loss: 0.140388\n",
      "Epoch: 80 \tTraining Loss: 0.581381 \tValidation Loss: 0.139534\n",
      "Validation loss decreased (0.139757 --> 0.139534).  Saving model ...\n",
      "Epoch: 81 \tTraining Loss: 0.581024 \tValidation Loss: 0.139095\n",
      "Validation loss decreased (0.139534 --> 0.139095).  Saving model ...\n",
      "Epoch: 82 \tTraining Loss: 0.579931 \tValidation Loss: 0.139962\n",
      "Epoch: 83 \tTraining Loss: 0.579003 \tValidation Loss: 0.138674\n",
      "Validation loss decreased (0.139095 --> 0.138674).  Saving model ...\n",
      "Epoch: 84 \tTraining Loss: 0.578862 \tValidation Loss: 0.138812\n",
      "Epoch: 85 \tTraining Loss: 0.578606 \tValidation Loss: 0.138201\n",
      "Validation loss decreased (0.138674 --> 0.138201).  Saving model ...\n",
      "Epoch: 86 \tTraining Loss: 0.577510 \tValidation Loss: 0.138345\n",
      "Epoch: 87 \tTraining Loss: 0.576526 \tValidation Loss: 0.138082\n",
      "Validation loss decreased (0.138201 --> 0.138082).  Saving model ...\n",
      "Epoch: 88 \tTraining Loss: 0.576364 \tValidation Loss: 0.138380\n",
      "Epoch: 89 \tTraining Loss: 0.576582 \tValidation Loss: 0.137730\n",
      "Validation loss decreased (0.138082 --> 0.137730).  Saving model ...\n",
      "Epoch: 90 \tTraining Loss: 0.576228 \tValidation Loss: 0.137944\n",
      "Epoch: 91 \tTraining Loss: 0.576143 \tValidation Loss: 0.137470\n",
      "Validation loss decreased (0.137730 --> 0.137470).  Saving model ...\n",
      "Epoch: 92 \tTraining Loss: 0.574234 \tValidation Loss: 0.137646\n",
      "Epoch: 93 \tTraining Loss: 0.574020 \tValidation Loss: 0.138541\n",
      "Epoch: 94 \tTraining Loss: 0.573755 \tValidation Loss: 0.136976\n",
      "Validation loss decreased (0.137470 --> 0.136976).  Saving model ...\n",
      "Epoch: 95 \tTraining Loss: 0.573175 \tValidation Loss: 0.137209\n",
      "Epoch: 96 \tTraining Loss: 0.572543 \tValidation Loss: 0.136895\n",
      "Validation loss decreased (0.136976 --> 0.136895).  Saving model ...\n",
      "Epoch: 97 \tTraining Loss: 0.572274 \tValidation Loss: 0.136513\n",
      "Validation loss decreased (0.136895 --> 0.136513).  Saving model ...\n",
      "Epoch: 98 \tTraining Loss: 0.572665 \tValidation Loss: 0.136529\n",
      "Epoch: 99 \tTraining Loss: 0.571015 \tValidation Loss: 0.136579\n",
      "Epoch: 100 \tTraining Loss: 0.570033 \tValidation Loss: 0.136141\n",
      "Validation loss decreased (0.136513 --> 0.136141).  Saving model ...\n",
      "Time elapsed: 458.52 s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "# number of epochs to train the model\n",
    "n_epochs = 100\n",
    "\n",
    "# initialize tracker for minimum validation loss\n",
    "valid_loss_min = np.Inf # set initial \"min\" to infinity\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "  # monitor training loss\n",
    "  train_loss = 0.0\n",
    "  valid_loss = 0.0\n",
    "\n",
    "  ###################\n",
    "  # train the model #\n",
    "  ###################\n",
    "  model_te.train() # prep model for training\n",
    "  for data, target in train_loader_te:\n",
    "    # transfer data and target to GPU\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    # clear the gradients of all optimized variables\n",
    "    optimizer_te.zero_grad()\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output = model_te(data.float())\n",
    "    # calculate the loss\n",
    "    loss = criterion(output, target)\n",
    "    # backward pass: compute gradient of the loss with respect to model parameters\n",
    "    loss.backward()\n",
    "    # perform a single optimization step (parameter update)\n",
    "    optimizer_te.step()\n",
    "    # update running training loss\n",
    "    train_loss += loss.item()*data.size(0)\n",
    "\n",
    "  ######################\n",
    "  # validate the model #\n",
    "  ######################\n",
    "  model_te.eval() # prep model for evaluation\n",
    "  for data, target in valid_loader_te:\n",
    "    # transfer data and target to GPU\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output = model_te(data.float())\n",
    "    # calculate the loss\n",
    "    loss = criterion(output, target)\n",
    "    # update running validation loss\n",
    "    valid_loss += loss.item()*data.size(0)\n",
    "    \n",
    "\n",
    "  # print training/validation statistics\n",
    "  # calculate average loss over an epoch\n",
    "  train_loss = train_loss/len(train_loader_te.dataset)\n",
    "  valid_loss = valid_loss/len(valid_loader_te.dataset)\n",
    "\n",
    "  print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch+1, train_loss, valid_loss))\n",
    "    \n",
    "  # save model if validation loss has decreased\n",
    "  if valid_loss <= valid_loss_min:\n",
    "    print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'\n",
    "        .format(valid_loss_min, valid_loss))\n",
    "    torch.save(model_te.state_dict(), 'model.pt')\n",
    "    valid_loss_min = valid_loss\n",
    "\n",
    "end = time.time()\n",
    "print('Time elapsed: %.2f s' % (end - start))\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "H5EYNE7oM_oI"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model with the lowest validation loss\n",
    "model_te.load_state_dict(torch.load('model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "jdXVtuamNoOJ"
   },
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for out outputs\n",
    "with torch.no_grad():\n",
    "  for data in test_loader_te:\n",
    "    embeddings, labels = data\n",
    "    # calculating outputs by running embeddings through the network\n",
    "    model_te.to(\"cpu\")\n",
    "    outputs = model_te(embeddings.float())\n",
    "    # the class with the highest score is what we choose as prediction\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "FSqc0KjgNoOJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy_MLP-model_average-word-vectors_pretrained-word2vec-model_ternary-test-set: 71 %\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy_MLP-model_average-word-vectors_pretrained-word2vec-model_ternary-test-set: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z9IH0RMM5cZ2"
   },
   "source": [
    "## a.2 Use My Own Word2Vec Model to Compute Input Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iTtS7Ja73top"
   },
   "source": [
    "### a.2.1 Compute Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tIIpN30y3toq"
   },
   "source": [
    "Input features are average word embeddings of each review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IObVKN8H3tos"
   },
   "source": [
    "#### Use **my own model** to compute the average word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "7zKCNINi3tot"
   },
   "outputs": [],
   "source": [
    "# Apply the word2v_avg function to compute the average embeddings\n",
    "X_train_bi_series = X_train_bi.apply(lambda x: word2v_avg(x, model.wv))\n",
    "X_train_bi_avg = np.array(X_train_bi_series.values.tolist())\n",
    "\n",
    "X_test_bi_series = X_test_bi.apply(lambda x: word2v_avg(x, model.wv))\n",
    "X_test_bi_avg = np.array(X_test_bi_series.values.tolist())\n",
    "\n",
    "X_train_te_series = X_train_te.apply(lambda x: word2v_avg(x, model.wv))\n",
    "X_train_te_avg = np.array(X_train_te_series.values.tolist())\n",
    "\n",
    "X_test_te_series = X_test_te.apply(lambda x: word2v_avg(x, model.wv))\n",
    "X_test_te_avg = np.array(X_test_te_series.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "lbkwas9f3tot"
   },
   "outputs": [],
   "source": [
    "# Apply the idx_nan function to remove the NaN's and corresponding\n",
    "# labels (if any)\n",
    "idx_nan_train_bi = idx_nan(X_train_bi_avg)\n",
    "if idx_nan_train_bi != None:\n",
    "  X_train_bi_avg = np.delete(X_train_bi_avg, idx_nan_train_bi, 0)\n",
    "  y_train_bi_avg = np.delete(y_train_bi, idx_nan_train_bi)\n",
    "\n",
    "idx_nan_test_bi = idx_nan(X_test_bi_avg)\n",
    "if idx_nan_test_bi != None:\n",
    "  X_test_bi_avg = np.delete(X_test_bi_avg, idx_nan_test_bi, 0)\n",
    "  y_test_bi_avg = np.delete(y_test_bi, idx_nan_test_bi)\n",
    "\n",
    "idx_nan_train_te = idx_nan(X_train_te_avg)\n",
    "if idx_nan_train_te != None:\n",
    "  X_train_te_avg = np.delete(X_train_te_avg, idx_nan_train_te, 0)\n",
    "  y_train_te_avg = np.delete(y_train_te, idx_nan_train_te)\n",
    "\n",
    "idx_nan_test_te = idx_nan(X_test_te_avg)\n",
    "if idx_nan_test_te != None:\n",
    "  X_test_te_avg = np.delete(X_test_te_avg, idx_nan_test_te, 0)\n",
    "  y_test_te_avg = np.delete(y_test_te, idx_nan_test_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s8GrmnD63tot"
   },
   "source": [
    "### a.2.2 Define Dataset Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "CRsM9tDa3tou"
   },
   "outputs": [],
   "source": [
    "class Train(Dataset):\n",
    "  def __init__(self, xtrain, ytrain):\n",
    "    'Initialization'\n",
    "    self.data = xtrain\n",
    "    self.labels = ytrain\n",
    "\n",
    "  def __len__(self):\n",
    "    'Denotes the total number of samples'\n",
    "    return len(self.data)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    'Generates one sample of data'\n",
    "    X = self.data[index]\n",
    "    y = self.labels[index]\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "lleVDSnK3tou"
   },
   "outputs": [],
   "source": [
    "class Test(Dataset):\n",
    "  def __init__(self, xtest, ytest):\n",
    "    'Initialization'\n",
    "    self.data = xtest\n",
    "    self.labels = ytest\n",
    "\n",
    "  def __len__(self):\n",
    "    'Denotes the total number of samples'\n",
    "    return len(self.data)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    'Generates one sample of data'\n",
    "    X = self.data[index]\n",
    "    y = self.labels[index]\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "92yqSQS83tou"
   },
   "source": [
    "### a.2.3 Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "DlC3r3Gk3tov"
   },
   "outputs": [],
   "source": [
    "# use average Word2Vec vectors as input features\n",
    "train_data_bi, test_data_bi = Train(X_train_bi_avg, y_train_bi_avg-1), Test(X_test_bi_avg, y_test_bi_avg-1)\n",
    "train_data_te, test_data_te = Train(X_train_te_avg, y_train_te_avg-1), Test(X_test_te_avg, y_test_te_avg-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nux47K7U3tov"
   },
   "source": [
    "#### Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "ebeQ44D83tov"
   },
   "outputs": [],
   "source": [
    "# number of subprocesses to use for data loading\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 100\n",
    "# percentage of training set to use as validation\n",
    "valid_size = 0.2\n",
    "\n",
    "# obtain training indices that will be used for validation\n",
    "num_train = len(train_data_bi)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "# define samples for obtaining training and validation batches\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "# prepare data loaders\n",
    "train_loader_bi = torch.utils.data.DataLoader(train_data_bi, batch_size=batch_size,\n",
    "                                           sampler=train_sampler, num_workers\n",
    "                                           =num_workers)\n",
    "valid_loader_bi = torch.utils.data.DataLoader(train_data_bi, batch_size=batch_size,\n",
    "                                           sampler=valid_sampler, num_workers\n",
    "                                           =num_workers)\n",
    "test_loader_bi = torch.utils.data.DataLoader(test_data_bi, batch_size=batch_size,\n",
    "                                           num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EaaVcZ0H3tow"
   },
   "source": [
    "#### Ternary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "wOJmyoQo3tow"
   },
   "outputs": [],
   "source": [
    "# number of subprocesses to use for data loading\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 100\n",
    "# percentage of training set to use as validation\n",
    "valid_size = 0.2\n",
    "\n",
    "# obtain training indices that will be used for validation\n",
    "num_train = len(train_data_te)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "# define samples for obtaining training and validation batches\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "# prepare data loaders\n",
    "train_loader_te = torch.utils.data.DataLoader(train_data_te, batch_size=batch_size,\n",
    "                                           sampler=train_sampler, num_workers\n",
    "                                           =num_workers)\n",
    "valid_loader_te = torch.utils.data.DataLoader(train_data_te, batch_size=batch_size,\n",
    "                                           sampler=valid_sampler, num_workers\n",
    "                                           =num_workers)\n",
    "test_loader_te = torch.utils.data.DataLoader(test_data_te, batch_size=batch_size,\n",
    "                                           num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AJqyJGTz3tow"
   },
   "source": [
    "### a.2.4 Define the MLP architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "s8szxTZu3tow"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "h_Td24AP3tox"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ThreeLayerMLP(\n",
      "  (linear1): Linear(in_features=300, out_features=50, bias=True)\n",
      "  (linear2): Linear(in_features=50, out_features=10, bias=True)\n",
      "  (linear3): Linear(in_features=10, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "ThreeLayerMLP(\n",
      "  (linear1): Linear(in_features=300, out_features=50, bias=True)\n",
      "  (linear2): Linear(in_features=50, out_features=10, bias=True)\n",
      "  (linear3): Linear(in_features=10, out_features=3, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# define the MLP architecture\n",
    "class ThreeLayerMLP(nn.Module):\n",
    "  def __init__(self, D_in, H1, H2, D_out):\n",
    "    super().__init__()\n",
    "    self.linear1 = nn.Linear(D_in, H1)\n",
    "    self.linear2 = nn.Linear(H1, H2)\n",
    "    self.linear3 = nn.Linear(H2, D_out)\n",
    "    # dropout layer (p=0.2)\n",
    "    self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "  def forward(self, x):\n",
    "    # add hidden layer, with relu activation function\n",
    "    h1_relu = F.relu(self.linear1(x))\n",
    "    # add dropout layer\n",
    "    h1_drop = self.dropout(h1_relu)\n",
    "    # add another hidden layer, with relu activation function\n",
    "    h2_relu = F.relu(self.linear2(h1_drop))\n",
    "    # add another dropout layer\n",
    "    h2_drop = self.dropout(h2_relu)\n",
    "    # add output layer\n",
    "    h2_output = self.linear3(h2_drop)\n",
    "\n",
    "    return h2_output\n",
    "\n",
    "# initialize MLP\n",
    "model_bi = ThreeLayerMLP(300, 50, 10, 2)\n",
    "model_te = ThreeLayerMLP(300, 50, 10, 3)\n",
    "model_bi.cuda()\n",
    "model_te.cuda()\n",
    "print(model_bi)\n",
    "print(model_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "PStCBBdx3tox"
   },
   "outputs": [],
   "source": [
    "# specify loss function (categorical cross-entropy)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# specify optimizer (stochastic gradient descent) and learning rate = 0.01\n",
    "optimizer_bi = torch.optim.SGD(model_bi.parameters(), lr=0.0075)\n",
    "optimizer_te = torch.optim.SGD(model_te.parameters(), lr=0.0075)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fudV38Oz3tox"
   },
   "source": [
    "### a.2.5 Train the Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y0jpuAKs3toy"
   },
   "source": [
    "#### Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "yFHdVLwV3toy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.458027 \tValidation Loss: 0.084087\n",
      "Validation loss decreased (inf --> 0.084087).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 0.321757 \tValidation Loss: 0.068026\n",
      "Validation loss decreased (0.084087 --> 0.068026).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 0.286148 \tValidation Loss: 0.062470\n",
      "Validation loss decreased (0.068026 --> 0.062470).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 0.269708 \tValidation Loss: 0.059469\n",
      "Validation loss decreased (0.062470 --> 0.059469).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 0.260094 \tValidation Loss: 0.057592\n",
      "Validation loss decreased (0.059469 --> 0.057592).  Saving model ...\n",
      "Epoch: 6 \tTraining Loss: 0.253149 \tValidation Loss: 0.056334\n",
      "Validation loss decreased (0.057592 --> 0.056334).  Saving model ...\n",
      "Epoch: 7 \tTraining Loss: 0.247672 \tValidation Loss: 0.055366\n",
      "Validation loss decreased (0.056334 --> 0.055366).  Saving model ...\n",
      "Epoch: 8 \tTraining Loss: 0.243468 \tValidation Loss: 0.054631\n",
      "Validation loss decreased (0.055366 --> 0.054631).  Saving model ...\n",
      "Epoch: 9 \tTraining Loss: 0.240464 \tValidation Loss: 0.054039\n",
      "Validation loss decreased (0.054631 --> 0.054039).  Saving model ...\n",
      "Epoch: 10 \tTraining Loss: 0.237266 \tValidation Loss: 0.053668\n",
      "Validation loss decreased (0.054039 --> 0.053668).  Saving model ...\n",
      "Epoch: 11 \tTraining Loss: 0.235022 \tValidation Loss: 0.053115\n",
      "Validation loss decreased (0.053668 --> 0.053115).  Saving model ...\n",
      "Epoch: 12 \tTraining Loss: 0.233575 \tValidation Loss: 0.052771\n",
      "Validation loss decreased (0.053115 --> 0.052771).  Saving model ...\n",
      "Epoch: 13 \tTraining Loss: 0.230858 \tValidation Loss: 0.052412\n",
      "Validation loss decreased (0.052771 --> 0.052412).  Saving model ...\n",
      "Epoch: 14 \tTraining Loss: 0.229945 \tValidation Loss: 0.052280\n",
      "Validation loss decreased (0.052412 --> 0.052280).  Saving model ...\n",
      "Epoch: 15 \tTraining Loss: 0.227985 \tValidation Loss: 0.051769\n",
      "Validation loss decreased (0.052280 --> 0.051769).  Saving model ...\n",
      "Epoch: 16 \tTraining Loss: 0.227127 \tValidation Loss: 0.051578\n",
      "Validation loss decreased (0.051769 --> 0.051578).  Saving model ...\n",
      "Epoch: 17 \tTraining Loss: 0.224460 \tValidation Loss: 0.051398\n",
      "Validation loss decreased (0.051578 --> 0.051398).  Saving model ...\n",
      "Epoch: 18 \tTraining Loss: 0.224622 \tValidation Loss: 0.051099\n",
      "Validation loss decreased (0.051398 --> 0.051099).  Saving model ...\n",
      "Epoch: 19 \tTraining Loss: 0.222753 \tValidation Loss: 0.050947\n",
      "Validation loss decreased (0.051099 --> 0.050947).  Saving model ...\n",
      "Epoch: 20 \tTraining Loss: 0.220778 \tValidation Loss: 0.050660\n",
      "Validation loss decreased (0.050947 --> 0.050660).  Saving model ...\n",
      "Epoch: 21 \tTraining Loss: 0.220320 \tValidation Loss: 0.050364\n",
      "Validation loss decreased (0.050660 --> 0.050364).  Saving model ...\n",
      "Epoch: 22 \tTraining Loss: 0.219163 \tValidation Loss: 0.050217\n",
      "Validation loss decreased (0.050364 --> 0.050217).  Saving model ...\n",
      "Epoch: 23 \tTraining Loss: 0.218281 \tValidation Loss: 0.050051\n",
      "Validation loss decreased (0.050217 --> 0.050051).  Saving model ...\n",
      "Epoch: 24 \tTraining Loss: 0.217441 \tValidation Loss: 0.049974\n",
      "Validation loss decreased (0.050051 --> 0.049974).  Saving model ...\n",
      "Epoch: 25 \tTraining Loss: 0.216060 \tValidation Loss: 0.049802\n",
      "Validation loss decreased (0.049974 --> 0.049802).  Saving model ...\n",
      "Epoch: 26 \tTraining Loss: 0.215194 \tValidation Loss: 0.049690\n",
      "Validation loss decreased (0.049802 --> 0.049690).  Saving model ...\n",
      "Epoch: 27 \tTraining Loss: 0.214914 \tValidation Loss: 0.049415\n",
      "Validation loss decreased (0.049690 --> 0.049415).  Saving model ...\n",
      "Epoch: 28 \tTraining Loss: 0.213974 \tValidation Loss: 0.049371\n",
      "Validation loss decreased (0.049415 --> 0.049371).  Saving model ...\n",
      "Epoch: 29 \tTraining Loss: 0.212728 \tValidation Loss: 0.049402\n",
      "Epoch: 30 \tTraining Loss: 0.212122 \tValidation Loss: 0.049139\n",
      "Validation loss decreased (0.049371 --> 0.049139).  Saving model ...\n",
      "Epoch: 31 \tTraining Loss: 0.212034 \tValidation Loss: 0.049082\n",
      "Validation loss decreased (0.049139 --> 0.049082).  Saving model ...\n",
      "Epoch: 32 \tTraining Loss: 0.211672 \tValidation Loss: 0.049055\n",
      "Validation loss decreased (0.049082 --> 0.049055).  Saving model ...\n",
      "Epoch: 33 \tTraining Loss: 0.209660 \tValidation Loss: 0.048871\n",
      "Validation loss decreased (0.049055 --> 0.048871).  Saving model ...\n",
      "Epoch: 34 \tTraining Loss: 0.208467 \tValidation Loss: 0.048761\n",
      "Validation loss decreased (0.048871 --> 0.048761).  Saving model ...\n",
      "Epoch: 35 \tTraining Loss: 0.209197 \tValidation Loss: 0.048855\n",
      "Epoch: 36 \tTraining Loss: 0.208525 \tValidation Loss: 0.048961\n",
      "Epoch: 37 \tTraining Loss: 0.208034 \tValidation Loss: 0.048704\n",
      "Validation loss decreased (0.048761 --> 0.048704).  Saving model ...\n",
      "Epoch: 38 \tTraining Loss: 0.207617 \tValidation Loss: 0.048526\n",
      "Validation loss decreased (0.048704 --> 0.048526).  Saving model ...\n",
      "Epoch: 39 \tTraining Loss: 0.206555 \tValidation Loss: 0.048478\n",
      "Validation loss decreased (0.048526 --> 0.048478).  Saving model ...\n",
      "Epoch: 40 \tTraining Loss: 0.205894 \tValidation Loss: 0.048288\n",
      "Validation loss decreased (0.048478 --> 0.048288).  Saving model ...\n",
      "Epoch: 41 \tTraining Loss: 0.205042 \tValidation Loss: 0.048476\n",
      "Epoch: 42 \tTraining Loss: 0.203817 \tValidation Loss: 0.048246\n",
      "Validation loss decreased (0.048288 --> 0.048246).  Saving model ...\n",
      "Epoch: 43 \tTraining Loss: 0.204711 \tValidation Loss: 0.048298\n",
      "Epoch: 44 \tTraining Loss: 0.202978 \tValidation Loss: 0.048300\n",
      "Epoch: 45 \tTraining Loss: 0.201965 \tValidation Loss: 0.048076\n",
      "Validation loss decreased (0.048246 --> 0.048076).  Saving model ...\n",
      "Epoch: 46 \tTraining Loss: 0.202916 \tValidation Loss: 0.047837\n",
      "Validation loss decreased (0.048076 --> 0.047837).  Saving model ...\n",
      "Epoch: 47 \tTraining Loss: 0.202743 \tValidation Loss: 0.047818\n",
      "Validation loss decreased (0.047837 --> 0.047818).  Saving model ...\n",
      "Epoch: 48 \tTraining Loss: 0.201510 \tValidation Loss: 0.047920\n",
      "Epoch: 49 \tTraining Loss: 0.200260 \tValidation Loss: 0.047921\n",
      "Epoch: 50 \tTraining Loss: 0.200264 \tValidation Loss: 0.047840\n",
      "Epoch: 51 \tTraining Loss: 0.199650 \tValidation Loss: 0.047922\n",
      "Epoch: 52 \tTraining Loss: 0.200142 \tValidation Loss: 0.047761\n",
      "Validation loss decreased (0.047818 --> 0.047761).  Saving model ...\n",
      "Epoch: 53 \tTraining Loss: 0.198988 \tValidation Loss: 0.047672\n",
      "Validation loss decreased (0.047761 --> 0.047672).  Saving model ...\n",
      "Epoch: 54 \tTraining Loss: 0.198005 \tValidation Loss: 0.047526\n",
      "Validation loss decreased (0.047672 --> 0.047526).  Saving model ...\n",
      "Epoch: 55 \tTraining Loss: 0.197942 \tValidation Loss: 0.047438\n",
      "Validation loss decreased (0.047526 --> 0.047438).  Saving model ...\n",
      "Epoch: 56 \tTraining Loss: 0.196900 \tValidation Loss: 0.047972\n",
      "Epoch: 57 \tTraining Loss: 0.196175 \tValidation Loss: 0.047375\n",
      "Validation loss decreased (0.047438 --> 0.047375).  Saving model ...\n",
      "Epoch: 58 \tTraining Loss: 0.197423 \tValidation Loss: 0.047450\n",
      "Epoch: 59 \tTraining Loss: 0.196504 \tValidation Loss: 0.047608\n",
      "Epoch: 60 \tTraining Loss: 0.195585 \tValidation Loss: 0.047304\n",
      "Validation loss decreased (0.047375 --> 0.047304).  Saving model ...\n",
      "Epoch: 61 \tTraining Loss: 0.194470 \tValidation Loss: 0.047385\n",
      "Epoch: 62 \tTraining Loss: 0.194908 \tValidation Loss: 0.047275\n",
      "Validation loss decreased (0.047304 --> 0.047275).  Saving model ...\n",
      "Epoch: 63 \tTraining Loss: 0.194213 \tValidation Loss: 0.047158\n",
      "Validation loss decreased (0.047275 --> 0.047158).  Saving model ...\n",
      "Epoch: 64 \tTraining Loss: 0.193880 \tValidation Loss: 0.047146\n",
      "Validation loss decreased (0.047158 --> 0.047146).  Saving model ...\n",
      "Epoch: 65 \tTraining Loss: 0.193896 \tValidation Loss: 0.047396\n",
      "Epoch: 66 \tTraining Loss: 0.192675 \tValidation Loss: 0.047275\n",
      "Epoch: 67 \tTraining Loss: 0.193326 \tValidation Loss: 0.047228\n",
      "Epoch: 68 \tTraining Loss: 0.192127 \tValidation Loss: 0.047312\n",
      "Epoch: 69 \tTraining Loss: 0.192150 \tValidation Loss: 0.047216\n",
      "Epoch: 70 \tTraining Loss: 0.191473 \tValidation Loss: 0.047027\n",
      "Validation loss decreased (0.047146 --> 0.047027).  Saving model ...\n",
      "Epoch: 71 \tTraining Loss: 0.191865 \tValidation Loss: 0.047175\n",
      "Epoch: 72 \tTraining Loss: 0.190833 \tValidation Loss: 0.047013\n",
      "Validation loss decreased (0.047027 --> 0.047013).  Saving model ...\n",
      "Epoch: 73 \tTraining Loss: 0.190193 \tValidation Loss: 0.046979\n",
      "Validation loss decreased (0.047013 --> 0.046979).  Saving model ...\n",
      "Epoch: 74 \tTraining Loss: 0.189844 \tValidation Loss: 0.047686\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 75 \tTraining Loss: 0.190330 \tValidation Loss: 0.047223\n",
      "Epoch: 76 \tTraining Loss: 0.189043 \tValidation Loss: 0.047039\n",
      "Epoch: 77 \tTraining Loss: 0.189209 \tValidation Loss: 0.047112\n",
      "Epoch: 78 \tTraining Loss: 0.188290 \tValidation Loss: 0.047072\n",
      "Epoch: 79 \tTraining Loss: 0.188668 \tValidation Loss: 0.047214\n",
      "Epoch: 80 \tTraining Loss: 0.187864 \tValidation Loss: 0.047273\n",
      "Epoch: 81 \tTraining Loss: 0.186963 \tValidation Loss: 0.047252\n",
      "Epoch: 82 \tTraining Loss: 0.187874 \tValidation Loss: 0.047138\n",
      "Epoch: 83 \tTraining Loss: 0.186635 \tValidation Loss: 0.047098\n",
      "Epoch: 84 \tTraining Loss: 0.186571 \tValidation Loss: 0.046967\n",
      "Validation loss decreased (0.046979 --> 0.046967).  Saving model ...\n",
      "Epoch: 85 \tTraining Loss: 0.186848 \tValidation Loss: 0.046938\n",
      "Validation loss decreased (0.046967 --> 0.046938).  Saving model ...\n",
      "Epoch: 86 \tTraining Loss: 0.185784 \tValidation Loss: 0.047076\n",
      "Epoch: 87 \tTraining Loss: 0.186238 \tValidation Loss: 0.047078\n",
      "Epoch: 88 \tTraining Loss: 0.185397 \tValidation Loss: 0.046884\n",
      "Validation loss decreased (0.046938 --> 0.046884).  Saving model ...\n",
      "Epoch: 89 \tTraining Loss: 0.184517 \tValidation Loss: 0.047151\n",
      "Epoch: 90 \tTraining Loss: 0.185108 \tValidation Loss: 0.046983\n",
      "Epoch: 91 \tTraining Loss: 0.184554 \tValidation Loss: 0.046983\n",
      "Epoch: 92 \tTraining Loss: 0.184483 \tValidation Loss: 0.047027\n",
      "Epoch: 93 \tTraining Loss: 0.183611 \tValidation Loss: 0.047141\n",
      "Epoch: 94 \tTraining Loss: 0.183948 \tValidation Loss: 0.047110\n",
      "Epoch: 95 \tTraining Loss: 0.182595 \tValidation Loss: 0.047092\n",
      "Epoch: 96 \tTraining Loss: 0.183575 \tValidation Loss: 0.047175\n",
      "Epoch: 97 \tTraining Loss: 0.182694 \tValidation Loss: 0.047085\n",
      "Epoch: 98 \tTraining Loss: 0.182643 \tValidation Loss: 0.047172\n",
      "Epoch: 99 \tTraining Loss: 0.182290 \tValidation Loss: 0.047077\n",
      "Epoch: 100 \tTraining Loss: 0.181784 \tValidation Loss: 0.047038\n",
      "Time elapsed: 318.71 s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "# number of epochs to train the model\n",
    "n_epochs = 100\n",
    "\n",
    "# initialize tracker for minimum validation loss\n",
    "valid_loss_min = np.Inf # set initial \"min\" to infinity\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "  # monitor training loss\n",
    "  train_loss = 0.0\n",
    "  valid_loss = 0.0\n",
    "\n",
    "  ###################\n",
    "  # train the model #\n",
    "  ###################\n",
    "  model_bi.train() # prep model for training\n",
    "  for data, target in train_loader_bi:\n",
    "    # transfer data and target to GPU\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    # clear the gradients of all optimized variables\n",
    "    optimizer_bi.zero_grad()\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output = model_bi(data.float())\n",
    "    # calculate the loss\n",
    "    loss = criterion(output, target)\n",
    "    # backward pass: compute gradient of the loss with respect to model parameters\n",
    "    loss.backward()\n",
    "    # perform a single optimization step (parameter update)\n",
    "    optimizer_bi.step()\n",
    "    # update running training loss\n",
    "    train_loss += loss.item()*data.size(0)\n",
    "\n",
    "  ######################\n",
    "  # validate the model #\n",
    "  ######################\n",
    "  model_bi.eval() # prep model for evaluation\n",
    "  for data, target in valid_loader_bi:\n",
    "    # transfer data and target to GPU\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output = model_bi(data.float())\n",
    "    # calculate the loss\n",
    "    loss = criterion(output, target)\n",
    "    # update running validation loss\n",
    "    valid_loss += loss.item()*data.size(0)\n",
    "    \n",
    "\n",
    "  # print training/validation statistics\n",
    "  # calculate average loss over an epoch\n",
    "  train_loss = train_loss/len(train_loader_bi.dataset)\n",
    "  valid_loss = valid_loss/len(valid_loader_bi.dataset)\n",
    "\n",
    "  print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch+1, train_loss, valid_loss))\n",
    "    \n",
    "  # save model if validation loss has decreased\n",
    "  if valid_loss <= valid_loss_min:\n",
    "    print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'\n",
    "        .format(valid_loss_min, valid_loss))\n",
    "    torch.save(model_bi.state_dict(), 'model.pt')\n",
    "    valid_loss_min = valid_loss\n",
    "\n",
    "end = time.time()\n",
    "print('Time elapsed: %.2f s' % (end - start))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "bmDtauXi3toy"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model with the lowest validation loss\n",
    "model_bi.load_state_dict(torch.load('model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "0gIeYXE73to0"
   },
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for out outputs\n",
    "with torch.no_grad():\n",
    "  for data in test_loader_bi:\n",
    "    embeddings, labels = data\n",
    "    # calculating outputs by running embeddings through the network\n",
    "    model_bi.to(\"cpu\")\n",
    "    outputs = model_bi(embeddings.float())\n",
    "    # the class with the highest score is what we choose as prediction\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "VUucxLeP3to0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy_MLP-model_average-word-vectors_my-own-word2vec-model_binary-test-set: 90 %\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy_MLP-model_average-word-vectors_my-own-word2vec-model_binary-test-set: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xNljehwm3to1"
   },
   "source": [
    "#### Ternary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "SjIN8amo3to1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.800322 \tValidation Loss: 0.173278\n",
      "Validation loss decreased (inf --> 0.173278).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 0.666301 \tValidation Loss: 0.149552\n",
      "Validation loss decreased (0.173278 --> 0.149552).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 0.608983 \tValidation Loss: 0.139909\n",
      "Validation loss decreased (0.149552 --> 0.139909).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 0.579390 \tValidation Loss: 0.134230\n",
      "Validation loss decreased (0.139909 --> 0.134230).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 0.560245 \tValidation Loss: 0.130471\n",
      "Validation loss decreased (0.134230 --> 0.130471).  Saving model ...\n",
      "Epoch: 6 \tTraining Loss: 0.548958 \tValidation Loss: 0.128281\n",
      "Validation loss decreased (0.130471 --> 0.128281).  Saving model ...\n",
      "Epoch: 7 \tTraining Loss: 0.540290 \tValidation Loss: 0.126501\n",
      "Validation loss decreased (0.128281 --> 0.126501).  Saving model ...\n",
      "Epoch: 8 \tTraining Loss: 0.532025 \tValidation Loss: 0.125380\n",
      "Validation loss decreased (0.126501 --> 0.125380).  Saving model ...\n",
      "Epoch: 9 \tTraining Loss: 0.525467 \tValidation Loss: 0.123946\n",
      "Validation loss decreased (0.125380 --> 0.123946).  Saving model ...\n",
      "Epoch: 10 \tTraining Loss: 0.521631 \tValidation Loss: 0.123167\n",
      "Validation loss decreased (0.123946 --> 0.123167).  Saving model ...\n",
      "Epoch: 11 \tTraining Loss: 0.516080 \tValidation Loss: 0.122194\n",
      "Validation loss decreased (0.123167 --> 0.122194).  Saving model ...\n",
      "Epoch: 12 \tTraining Loss: 0.513609 \tValidation Loss: 0.121466\n",
      "Validation loss decreased (0.122194 --> 0.121466).  Saving model ...\n",
      "Epoch: 13 \tTraining Loss: 0.511211 \tValidation Loss: 0.120988\n",
      "Validation loss decreased (0.121466 --> 0.120988).  Saving model ...\n",
      "Epoch: 14 \tTraining Loss: 0.506532 \tValidation Loss: 0.120362\n",
      "Validation loss decreased (0.120988 --> 0.120362).  Saving model ...\n",
      "Epoch: 15 \tTraining Loss: 0.505333 \tValidation Loss: 0.119634\n",
      "Validation loss decreased (0.120362 --> 0.119634).  Saving model ...\n",
      "Epoch: 16 \tTraining Loss: 0.502096 \tValidation Loss: 0.119277\n",
      "Validation loss decreased (0.119634 --> 0.119277).  Saving model ...\n",
      "Epoch: 17 \tTraining Loss: 0.499643 \tValidation Loss: 0.118961\n",
      "Validation loss decreased (0.119277 --> 0.118961).  Saving model ...\n",
      "Epoch: 18 \tTraining Loss: 0.498310 \tValidation Loss: 0.118728\n",
      "Validation loss decreased (0.118961 --> 0.118728).  Saving model ...\n",
      "Epoch: 19 \tTraining Loss: 0.497283 \tValidation Loss: 0.118332\n",
      "Validation loss decreased (0.118728 --> 0.118332).  Saving model ...\n",
      "Epoch: 20 \tTraining Loss: 0.495739 \tValidation Loss: 0.117890\n",
      "Validation loss decreased (0.118332 --> 0.117890).  Saving model ...\n",
      "Epoch: 21 \tTraining Loss: 0.492851 \tValidation Loss: 0.117533\n",
      "Validation loss decreased (0.117890 --> 0.117533).  Saving model ...\n",
      "Epoch: 22 \tTraining Loss: 0.493494 \tValidation Loss: 0.117461\n",
      "Validation loss decreased (0.117533 --> 0.117461).  Saving model ...\n",
      "Epoch: 23 \tTraining Loss: 0.491777 \tValidation Loss: 0.117325\n",
      "Validation loss decreased (0.117461 --> 0.117325).  Saving model ...\n",
      "Epoch: 24 \tTraining Loss: 0.490118 \tValidation Loss: 0.116879\n",
      "Validation loss decreased (0.117325 --> 0.116879).  Saving model ...\n",
      "Epoch: 25 \tTraining Loss: 0.488958 \tValidation Loss: 0.116909\n",
      "Epoch: 26 \tTraining Loss: 0.487946 \tValidation Loss: 0.116497\n",
      "Validation loss decreased (0.116879 --> 0.116497).  Saving model ...\n",
      "Epoch: 27 \tTraining Loss: 0.486737 \tValidation Loss: 0.116268\n",
      "Validation loss decreased (0.116497 --> 0.116268).  Saving model ...\n",
      "Epoch: 28 \tTraining Loss: 0.485732 \tValidation Loss: 0.116096\n",
      "Validation loss decreased (0.116268 --> 0.116096).  Saving model ...\n",
      "Epoch: 29 \tTraining Loss: 0.484680 \tValidation Loss: 0.115967\n",
      "Validation loss decreased (0.116096 --> 0.115967).  Saving model ...\n",
      "Epoch: 30 \tTraining Loss: 0.484022 \tValidation Loss: 0.115652\n",
      "Validation loss decreased (0.115967 --> 0.115652).  Saving model ...\n",
      "Epoch: 31 \tTraining Loss: 0.481909 \tValidation Loss: 0.115698\n",
      "Epoch: 32 \tTraining Loss: 0.482475 \tValidation Loss: 0.115592\n",
      "Validation loss decreased (0.115652 --> 0.115592).  Saving model ...\n",
      "Epoch: 33 \tTraining Loss: 0.481565 \tValidation Loss: 0.115387\n",
      "Validation loss decreased (0.115592 --> 0.115387).  Saving model ...\n",
      "Epoch: 34 \tTraining Loss: 0.480236 \tValidation Loss: 0.115322\n",
      "Validation loss decreased (0.115387 --> 0.115322).  Saving model ...\n",
      "Epoch: 35 \tTraining Loss: 0.480187 \tValidation Loss: 0.115141\n",
      "Validation loss decreased (0.115322 --> 0.115141).  Saving model ...\n",
      "Epoch: 36 \tTraining Loss: 0.479252 \tValidation Loss: 0.115247\n",
      "Epoch: 37 \tTraining Loss: 0.478757 \tValidation Loss: 0.114928\n",
      "Validation loss decreased (0.115141 --> 0.114928).  Saving model ...\n",
      "Epoch: 38 \tTraining Loss: 0.478565 \tValidation Loss: 0.115189\n",
      "Epoch: 39 \tTraining Loss: 0.476808 \tValidation Loss: 0.114640\n",
      "Validation loss decreased (0.114928 --> 0.114640).  Saving model ...\n",
      "Epoch: 40 \tTraining Loss: 0.476256 \tValidation Loss: 0.114567\n",
      "Validation loss decreased (0.114640 --> 0.114567).  Saving model ...\n",
      "Epoch: 41 \tTraining Loss: 0.475985 \tValidation Loss: 0.114445\n",
      "Validation loss decreased (0.114567 --> 0.114445).  Saving model ...\n",
      "Epoch: 42 \tTraining Loss: 0.475923 \tValidation Loss: 0.114507\n",
      "Epoch: 43 \tTraining Loss: 0.474048 \tValidation Loss: 0.114262\n",
      "Validation loss decreased (0.114445 --> 0.114262).  Saving model ...\n",
      "Epoch: 44 \tTraining Loss: 0.474743 \tValidation Loss: 0.114379\n",
      "Epoch: 45 \tTraining Loss: 0.473259 \tValidation Loss: 0.114279\n",
      "Epoch: 46 \tTraining Loss: 0.473348 \tValidation Loss: 0.114196\n",
      "Validation loss decreased (0.114262 --> 0.114196).  Saving model ...\n",
      "Epoch: 47 \tTraining Loss: 0.472399 \tValidation Loss: 0.114062\n",
      "Validation loss decreased (0.114196 --> 0.114062).  Saving model ...\n",
      "Epoch: 48 \tTraining Loss: 0.471621 \tValidation Loss: 0.114011\n",
      "Validation loss decreased (0.114062 --> 0.114011).  Saving model ...\n",
      "Epoch: 49 \tTraining Loss: 0.471656 \tValidation Loss: 0.113920\n",
      "Validation loss decreased (0.114011 --> 0.113920).  Saving model ...\n",
      "Epoch: 50 \tTraining Loss: 0.471231 \tValidation Loss: 0.113939\n",
      "Epoch: 51 \tTraining Loss: 0.470043 \tValidation Loss: 0.113916\n",
      "Validation loss decreased (0.113920 --> 0.113916).  Saving model ...\n",
      "Epoch: 52 \tTraining Loss: 0.470382 \tValidation Loss: 0.113692\n",
      "Validation loss decreased (0.113916 --> 0.113692).  Saving model ...\n",
      "Epoch: 53 \tTraining Loss: 0.469611 \tValidation Loss: 0.113569\n",
      "Validation loss decreased (0.113692 --> 0.113569).  Saving model ...\n",
      "Epoch: 54 \tTraining Loss: 0.468113 \tValidation Loss: 0.113576\n",
      "Epoch: 55 \tTraining Loss: 0.468920 \tValidation Loss: 0.113783\n",
      "Epoch: 56 \tTraining Loss: 0.467331 \tValidation Loss: 0.113443\n",
      "Validation loss decreased (0.113569 --> 0.113443).  Saving model ...\n",
      "Epoch: 57 \tTraining Loss: 0.467670 \tValidation Loss: 0.113353\n",
      "Validation loss decreased (0.113443 --> 0.113353).  Saving model ...\n",
      "Epoch: 58 \tTraining Loss: 0.465912 \tValidation Loss: 0.113380\n",
      "Epoch: 59 \tTraining Loss: 0.466563 \tValidation Loss: 0.113365\n",
      "Epoch: 60 \tTraining Loss: 0.466219 \tValidation Loss: 0.113480\n",
      "Epoch: 61 \tTraining Loss: 0.466136 \tValidation Loss: 0.113380\n",
      "Epoch: 62 \tTraining Loss: 0.464992 \tValidation Loss: 0.113229\n",
      "Validation loss decreased (0.113353 --> 0.113229).  Saving model ...\n",
      "Epoch: 63 \tTraining Loss: 0.464558 \tValidation Loss: 0.113241\n",
      "Epoch: 64 \tTraining Loss: 0.463020 \tValidation Loss: 0.113257\n",
      "Epoch: 65 \tTraining Loss: 0.464242 \tValidation Loss: 0.113257\n",
      "Epoch: 66 \tTraining Loss: 0.462165 \tValidation Loss: 0.113292\n",
      "Epoch: 67 \tTraining Loss: 0.463221 \tValidation Loss: 0.113269\n",
      "Epoch: 68 \tTraining Loss: 0.462721 \tValidation Loss: 0.113080\n",
      "Validation loss decreased (0.113229 --> 0.113080).  Saving model ...\n",
      "Epoch: 69 \tTraining Loss: 0.462322 \tValidation Loss: 0.112951\n",
      "Validation loss decreased (0.113080 --> 0.112951).  Saving model ...\n",
      "Epoch: 70 \tTraining Loss: 0.461166 \tValidation Loss: 0.112965\n",
      "Epoch: 71 \tTraining Loss: 0.461474 \tValidation Loss: 0.113160\n",
      "Epoch: 72 \tTraining Loss: 0.460047 \tValidation Loss: 0.113016\n",
      "Epoch: 73 \tTraining Loss: 0.459371 \tValidation Loss: 0.112792\n",
      "Validation loss decreased (0.112951 --> 0.112792).  Saving model ...\n",
      "Epoch: 74 \tTraining Loss: 0.459717 \tValidation Loss: 0.113264\n",
      "Epoch: 75 \tTraining Loss: 0.460098 \tValidation Loss: 0.112810\n",
      "Epoch: 76 \tTraining Loss: 0.460452 \tValidation Loss: 0.112946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 77 \tTraining Loss: 0.458581 \tValidation Loss: 0.113223\n",
      "Epoch: 78 \tTraining Loss: 0.458694 \tValidation Loss: 0.112845\n",
      "Epoch: 79 \tTraining Loss: 0.458165 \tValidation Loss: 0.112526\n",
      "Validation loss decreased (0.112792 --> 0.112526).  Saving model ...\n",
      "Epoch: 80 \tTraining Loss: 0.457439 \tValidation Loss: 0.112820\n",
      "Epoch: 81 \tTraining Loss: 0.457200 \tValidation Loss: 0.112769\n",
      "Epoch: 82 \tTraining Loss: 0.457728 \tValidation Loss: 0.112775\n",
      "Epoch: 83 \tTraining Loss: 0.457117 \tValidation Loss: 0.112732\n",
      "Epoch: 84 \tTraining Loss: 0.456779 \tValidation Loss: 0.112639\n",
      "Epoch: 85 \tTraining Loss: 0.456468 \tValidation Loss: 0.112569\n",
      "Epoch: 86 \tTraining Loss: 0.456525 \tValidation Loss: 0.112633\n",
      "Epoch: 87 \tTraining Loss: 0.454819 \tValidation Loss: 0.112776\n",
      "Epoch: 88 \tTraining Loss: 0.455250 \tValidation Loss: 0.112734\n",
      "Epoch: 89 \tTraining Loss: 0.454847 \tValidation Loss: 0.112594\n",
      "Epoch: 90 \tTraining Loss: 0.454898 \tValidation Loss: 0.112630\n",
      "Epoch: 91 \tTraining Loss: 0.454300 \tValidation Loss: 0.112388\n",
      "Validation loss decreased (0.112526 --> 0.112388).  Saving model ...\n",
      "Epoch: 92 \tTraining Loss: 0.454018 \tValidation Loss: 0.112573\n",
      "Epoch: 93 \tTraining Loss: 0.453860 \tValidation Loss: 0.112835\n",
      "Epoch: 94 \tTraining Loss: 0.454450 \tValidation Loss: 0.112604\n",
      "Epoch: 95 \tTraining Loss: 0.453591 \tValidation Loss: 0.112523\n",
      "Epoch: 96 \tTraining Loss: 0.451873 \tValidation Loss: 0.112434\n",
      "Epoch: 97 \tTraining Loss: 0.452264 \tValidation Loss: 0.112726\n",
      "Epoch: 98 \tTraining Loss: 0.452642 \tValidation Loss: 0.112424\n",
      "Epoch: 99 \tTraining Loss: 0.452221 \tValidation Loss: 0.112537\n",
      "Epoch: 100 \tTraining Loss: 0.451610 \tValidation Loss: 0.112378\n",
      "Validation loss decreased (0.112388 --> 0.112378).  Saving model ...\n",
      "Time elapsed: 423.38 s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "# number of epochs to train the model\n",
    "n_epochs = 100\n",
    "\n",
    "# initialize tracker for minimum validation loss\n",
    "valid_loss_min = np.Inf # set initial \"min\" to infinity\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "  # monitor training loss\n",
    "  train_loss = 0.0\n",
    "  valid_loss = 0.0\n",
    "\n",
    "  ###################\n",
    "  # train the model #\n",
    "  ###################\n",
    "  model_te.train() # prep model for training\n",
    "  for data, target in train_loader_te:\n",
    "    # transfer data and target to GPU\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    # clear the gradients of all optimized variables\n",
    "    optimizer_te.zero_grad()\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output = model_te(data.float())\n",
    "    # calculate the loss\n",
    "    loss = criterion(output, target)\n",
    "    # backward pass: compute gradient of the loss with respect to model parameters\n",
    "    loss.backward()\n",
    "    # perform a single optimization step (parameter update)\n",
    "    optimizer_te.step()\n",
    "    # update running training loss\n",
    "    train_loss += loss.item()*data.size(0)\n",
    "\n",
    "  ######################\n",
    "  # validate the model #\n",
    "  ######################\n",
    "  model_te.eval() # prep model for evaluation\n",
    "  for data, target in valid_loader_te:\n",
    "    # transfer data and target to GPU\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output = model_te(data.float())\n",
    "    # calculate the loss\n",
    "    loss = criterion(output, target)\n",
    "    # update running validation loss\n",
    "    valid_loss += loss.item()*data.size(0)\n",
    "    \n",
    "\n",
    "  # print training/validation statistics\n",
    "  # calculate average loss over an epoch\n",
    "  train_loss = train_loss/len(train_loader_te.dataset)\n",
    "  valid_loss = valid_loss/len(valid_loader_te.dataset)\n",
    "\n",
    "  print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch+1, train_loss, valid_loss))\n",
    "    \n",
    "  # save model if validation loss has decreased\n",
    "  if valid_loss <= valid_loss_min:\n",
    "    print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'\n",
    "        .format(valid_loss_min, valid_loss))\n",
    "    torch.save(model_te.state_dict(), 'model.pt')\n",
    "    valid_loss_min = valid_loss\n",
    "\n",
    "end = time.time()\n",
    "print('Time elapsed: %.2f s' % (end - start))\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "ad_bj7uk3to1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model with the lowest validation loss\n",
    "model_te.load_state_dict(torch.load('model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "knLNz-Jm3to2"
   },
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for out outputs\n",
    "with torch.no_grad():\n",
    "  for data in test_loader_te:\n",
    "    embeddings, labels = data\n",
    "    # calculating outputs by running embeddings through the network\n",
    "    model_te.to(\"cpu\")\n",
    "    outputs = model_te(embeddings.float())\n",
    "    # the class with the highest score is what we choose as prediction\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "yTS2SLDM3to2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy_MLP-model_average-word-vectors_my-own-word2vec-model_ternary-test-set: 76 %\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy_MLP-model_average-word-vectors_my-own-word2vec-model_ternary-test-set: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EaXeACmOwSAW"
   },
   "source": [
    "## b. Use the First 10 Word Embeddings as Input Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EADiuZGi7gKs"
   },
   "source": [
    "## b.1 Use the Pretrained Word2Vec Model to Compute Input Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z2BmLlzPquEX"
   },
   "source": [
    "### b.1.1 Compute Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mr1Ww6_squEY"
   },
   "source": [
    "Input features are the first 10 word embeddings of each review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y52QRMbbqHj2"
   },
   "source": [
    "#### Use **the pretrained model** to concatenate the first 10 word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "Obnbs2MK_RYX"
   },
   "outputs": [],
   "source": [
    "X_train_bi_10_series = X_train_bi.apply(lambda x: word2v_concat(x, wv_google))\n",
    "X_train_bi_10 = np.array(X_train_bi_10_series.values.tolist())\n",
    "\n",
    "X_test_bi_10_series = X_test_bi.apply(lambda x: word2v_concat(x, wv_google))\n",
    "X_test_bi_10 = np.array(X_test_bi_10_series.values.tolist())\n",
    "\n",
    "X_train_te_10_series = X_train_te.apply(lambda x: word2v_concat(x, wv_google))\n",
    "X_train_te_10 = np.array(X_train_te_10_series.values.tolist())\n",
    "\n",
    "X_test_te_10_series = X_test_te.apply(lambda x: word2v_concat(x, wv_google))\n",
    "X_test_te_10 = np.array(X_test_te_10_series.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "x4a5_ma2oQB6"
   },
   "outputs": [],
   "source": [
    "# Apply the idx_nan function to remove the NaN's and corresponding\n",
    "# labels (if any)\n",
    "idx_nan_train_bi = idx_nan(X_train_bi_10)\n",
    "if idx_nan_train_bi != None:\n",
    "    X_train_bi_10 = np.delete(X_train_bi_10, idx_nan_train_bi, 0)\n",
    "    y_train_bi_10 = np.delete(y_train_bi, idx_nan_train_bi)\n",
    "else:\n",
    "    y_train_bi_10 = y_train_bi\n",
    "\n",
    "idx_nan_test_bi = idx_nan(X_test_bi_10)\n",
    "if idx_nan_test_bi != None:\n",
    "    X_test_bi_10 = np.delete(X_test_bi_10, idx_nan_test_bi, 0)\n",
    "    y_test_bi_10 = np.delete(y_test_bi, idx_nan_test_bi)\n",
    "else:\n",
    "    y_test_bi_10 = y_test_bi\n",
    "\n",
    "idx_nan_train_te = idx_nan(X_train_te_10)\n",
    "if idx_nan_train_te != None:\n",
    "    X_train_te_10 = np.delete(X_train_te_10, idx_nan_train_te, 0)\n",
    "    y_train_te_10 = np.delete(y_train_te, idx_nan_train_te)\n",
    "else:\n",
    "    y_train_te_10 = y_train_te\n",
    "\n",
    "idx_nan_test_te = idx_nan(X_test_te_10)\n",
    "if idx_nan_test_te != None:\n",
    "    X_test_te_10 = np.delete(X_test_te_10, idx_nan_test_te, 0)\n",
    "    y_test_te_10 = np.delete(y_test_te, idx_nan_test_te)\n",
    "else:\n",
    "    y_test_te_10 = y_test_te"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ysyMIO4wyfq"
   },
   "source": [
    "### b.1.2 Define Dataset Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "CRsM9tDa3tou"
   },
   "outputs": [],
   "source": [
    "class Train(Dataset):\n",
    "  def __init__(self, xtrain, ytrain):\n",
    "    'Initialization'\n",
    "    self.data = xtrain\n",
    "    self.labels = ytrain\n",
    "\n",
    "  def __len__(self):\n",
    "    'Denotes the total number of samples'\n",
    "    return len(self.data)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    'Generates one sample of data'\n",
    "    X = self.data[index]\n",
    "    y = self.labels[index]\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "lleVDSnK3tou"
   },
   "outputs": [],
   "source": [
    "class Test(Dataset):\n",
    "  def __init__(self, xtest, ytest):\n",
    "    'Initialization'\n",
    "    self.data = xtest\n",
    "    self.labels = ytest\n",
    "\n",
    "  def __len__(self):\n",
    "    'Denotes the total number of samples'\n",
    "    return len(self.data)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    'Generates one sample of data'\n",
    "    X = self.data[index]\n",
    "    y = self.labels[index]\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "01OJrrevwyfr"
   },
   "source": [
    "### b.1.3 Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "-Z5d1gkzwyfs"
   },
   "outputs": [],
   "source": [
    "# use the first 10 Word2Vec vectors as input features\n",
    "train_data_bi_10, test_data_bi_10 = Train(X_train_bi_10, y_train_bi_10-1), Test(X_test_bi_10, y_test_bi_10-1)\n",
    "train_data_te_10, test_data_te_10 = Train(X_train_te_10, y_train_te_10-1), Test(X_test_te_10, y_test_te_10-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IzRzPPrpxlfP"
   },
   "source": [
    "#### Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "szzwaPZyxlfP"
   },
   "outputs": [],
   "source": [
    "# number of subprocesses to use for data loading\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 100\n",
    "# percentage of training set to use as validation\n",
    "valid_size = 0.2\n",
    "\n",
    "# obtain training indices that will be used for validation\n",
    "num_train = len(train_data_bi_10)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "# define samples for obtaining training and validation batches\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "# prepare data loaders\n",
    "train_loader_bi_10 = torch.utils.data.DataLoader(train_data_bi_10, batch_size=batch_size,\n",
    "                                           sampler=train_sampler, num_workers\n",
    "                                           =num_workers)\n",
    "valid_loader_bi_10 = torch.utils.data.DataLoader(train_data_bi_10, batch_size=batch_size,\n",
    "                                           sampler=valid_sampler, num_workers\n",
    "                                           =num_workers)\n",
    "test_loader_bi_10 = torch.utils.data.DataLoader(test_data_bi_10, batch_size=batch_size,\n",
    "                                           num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5G-UOi9vxlfQ"
   },
   "source": [
    "#### Ternary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "19UeA_PFxlfQ"
   },
   "outputs": [],
   "source": [
    "# number of subprocesses to use for data loading\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 100\n",
    "# percentage of training set to use as validation\n",
    "valid_size = 0.2\n",
    "\n",
    "# obtain training indices that will be used for validation\n",
    "num_train = len(train_data_te_10)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "# define samples for obtaining training and validation batches\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "# prepare data loaders\n",
    "train_loader_te_10 = torch.utils.data.DataLoader(train_data_te_10, batch_size=batch_size,\n",
    "                                           sampler=train_sampler, num_workers\n",
    "                                           =num_workers)\n",
    "valid_loader_te_10 = torch.utils.data.DataLoader(train_data_te_10, batch_size=batch_size,\n",
    "                                           sampler=valid_sampler, num_workers\n",
    "                                           =num_workers)\n",
    "test_loader_te_10 = torch.utils.data.DataLoader(test_data_te_10, batch_size=batch_size,\n",
    "                                           num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YuimfTbnwyfs"
   },
   "source": [
    "### b.1.4 Define the MLP architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "s8szxTZu3tow"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "8nTKgZU9bjNH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ThreeLayerMLP(\n",
      "  (linear1): Linear(in_features=3000, out_features=50, bias=True)\n",
      "  (linear2): Linear(in_features=50, out_features=10, bias=True)\n",
      "  (linear3): Linear(in_features=10, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "ThreeLayerMLP(\n",
      "  (linear1): Linear(in_features=3000, out_features=50, bias=True)\n",
      "  (linear2): Linear(in_features=50, out_features=10, bias=True)\n",
      "  (linear3): Linear(in_features=10, out_features=3, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# define the MLP architecture\n",
    "class ThreeLayerMLP(nn.Module):\n",
    "  def __init__(self, D_in, H1, H2, D_out):\n",
    "    super().__init__()\n",
    "    self.linear1 = nn.Linear(D_in, H1)\n",
    "    self.linear2 = nn.Linear(H1, H2)\n",
    "    self.linear3 = nn.Linear(H2, D_out)\n",
    "    # dropout layer (p=0.2)\n",
    "    self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "  def forward(self, x):\n",
    "    # add hidden layer, with relu activation function\n",
    "    h1_relu = F.relu(self.linear1(x))\n",
    "    # add dropout layer\n",
    "    h1_drop = self.dropout(h1_relu)\n",
    "    # add another hidden layer, with relu activation function\n",
    "    h2_relu = F.relu(self.linear2(h1_drop))\n",
    "    # add another dropout layer\n",
    "    h2_drop = self.dropout(h2_relu)\n",
    "    # add output layer\n",
    "    h2_output = self.linear3(h2_drop)\n",
    "\n",
    "    return h2_output\n",
    "\n",
    "# initialize MLP\n",
    "model_bi_10 = ThreeLayerMLP(3000, 50, 10, 2)\n",
    "model_te_10 = ThreeLayerMLP(3000, 50, 10, 3)\n",
    "model_bi_10.to(device)\n",
    "model_te_10.to(device)\n",
    "print(model_bi_10)\n",
    "print(model_te_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "yBVm1BOlbjNI"
   },
   "outputs": [],
   "source": [
    "# specify loss function (categorical cross-entropy)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# specify optimizer (stochastic gradient descent) and learning rate = 0.01\n",
    "optimizer_bi_10 = torch.optim.SGD(model_bi_10.parameters(), lr=0.0075)\n",
    "optimizer_te_10 = torch.optim.SGD(model_te_10.parameters(), lr=0.0075)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YesfIQLfwyft"
   },
   "source": [
    "### b.1.5 Train the Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oi5p8niFwyft"
   },
   "source": [
    "#### Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "-QJ_oFJXy4Jp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.536009 \tValidation Loss: 0.122551\n",
      "Validation loss decreased (inf --> 0.122551).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 0.423745 \tValidation Loss: 0.086349\n",
      "Validation loss decreased (0.122551 --> 0.086349).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 0.347524 \tValidation Loss: 0.076043\n",
      "Validation loss decreased (0.086349 --> 0.076043).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 0.318241 \tValidation Loss: 0.071262\n",
      "Validation loss decreased (0.076043 --> 0.071262).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 0.300019 \tValidation Loss: 0.068563\n",
      "Validation loss decreased (0.071262 --> 0.068563).  Saving model ...\n",
      "Epoch: 6 \tTraining Loss: 0.287864 \tValidation Loss: 0.066847\n",
      "Validation loss decreased (0.068563 --> 0.066847).  Saving model ...\n",
      "Epoch: 7 \tTraining Loss: 0.279781 \tValidation Loss: 0.065718\n",
      "Validation loss decreased (0.066847 --> 0.065718).  Saving model ...\n",
      "Epoch: 8 \tTraining Loss: 0.273702 \tValidation Loss: 0.064874\n",
      "Validation loss decreased (0.065718 --> 0.064874).  Saving model ...\n",
      "Epoch: 9 \tTraining Loss: 0.267329 \tValidation Loss: 0.064183\n",
      "Validation loss decreased (0.064874 --> 0.064183).  Saving model ...\n",
      "Epoch: 10 \tTraining Loss: 0.262865 \tValidation Loss: 0.063784\n",
      "Validation loss decreased (0.064183 --> 0.063784).  Saving model ...\n",
      "Epoch: 11 \tTraining Loss: 0.258482 \tValidation Loss: 0.063391\n",
      "Validation loss decreased (0.063784 --> 0.063391).  Saving model ...\n",
      "Epoch: 12 \tTraining Loss: 0.255378 \tValidation Loss: 0.063021\n",
      "Validation loss decreased (0.063391 --> 0.063021).  Saving model ...\n",
      "Epoch: 13 \tTraining Loss: 0.251731 \tValidation Loss: 0.062566\n",
      "Validation loss decreased (0.063021 --> 0.062566).  Saving model ...\n",
      "Epoch: 14 \tTraining Loss: 0.247164 \tValidation Loss: 0.062500\n",
      "Validation loss decreased (0.062566 --> 0.062500).  Saving model ...\n",
      "Epoch: 15 \tTraining Loss: 0.243875 \tValidation Loss: 0.062173\n",
      "Validation loss decreased (0.062500 --> 0.062173).  Saving model ...\n",
      "Epoch: 16 \tTraining Loss: 0.240595 \tValidation Loss: 0.061916\n",
      "Validation loss decreased (0.062173 --> 0.061916).  Saving model ...\n",
      "Epoch: 17 \tTraining Loss: 0.236635 \tValidation Loss: 0.061791\n",
      "Validation loss decreased (0.061916 --> 0.061791).  Saving model ...\n",
      "Epoch: 18 \tTraining Loss: 0.233682 \tValidation Loss: 0.061415\n",
      "Validation loss decreased (0.061791 --> 0.061415).  Saving model ...\n",
      "Epoch: 19 \tTraining Loss: 0.230486 \tValidation Loss: 0.061324\n",
      "Validation loss decreased (0.061415 --> 0.061324).  Saving model ...\n",
      "Epoch: 20 \tTraining Loss: 0.226116 \tValidation Loss: 0.061291\n",
      "Validation loss decreased (0.061324 --> 0.061291).  Saving model ...\n",
      "Epoch: 21 \tTraining Loss: 0.222910 \tValidation Loss: 0.061239\n",
      "Validation loss decreased (0.061291 --> 0.061239).  Saving model ...\n",
      "Epoch: 22 \tTraining Loss: 0.220196 \tValidation Loss: 0.061146\n",
      "Validation loss decreased (0.061239 --> 0.061146).  Saving model ...\n",
      "Epoch: 23 \tTraining Loss: 0.217447 \tValidation Loss: 0.061235\n",
      "Epoch: 24 \tTraining Loss: 0.211370 \tValidation Loss: 0.061012\n",
      "Validation loss decreased (0.061146 --> 0.061012).  Saving model ...\n",
      "Epoch: 25 \tTraining Loss: 0.208649 \tValidation Loss: 0.061090\n",
      "Epoch: 26 \tTraining Loss: 0.205705 \tValidation Loss: 0.061016\n",
      "Epoch: 27 \tTraining Loss: 0.200649 \tValidation Loss: 0.061074\n",
      "Epoch: 28 \tTraining Loss: 0.197360 \tValidation Loss: 0.061559\n",
      "Epoch: 29 \tTraining Loss: 0.194066 \tValidation Loss: 0.061654\n",
      "Epoch: 30 \tTraining Loss: 0.190532 \tValidation Loss: 0.061424\n",
      "Epoch: 31 \tTraining Loss: 0.186788 \tValidation Loss: 0.061622\n",
      "Epoch: 32 \tTraining Loss: 0.183552 \tValidation Loss: 0.062061\n",
      "Epoch: 33 \tTraining Loss: 0.179139 \tValidation Loss: 0.061777\n",
      "Epoch: 34 \tTraining Loss: 0.176037 \tValidation Loss: 0.062394\n",
      "Epoch: 35 \tTraining Loss: 0.171266 \tValidation Loss: 0.062196\n",
      "Epoch: 36 \tTraining Loss: 0.169221 \tValidation Loss: 0.062858\n",
      "Epoch: 37 \tTraining Loss: 0.165086 \tValidation Loss: 0.063519\n",
      "Epoch: 38 \tTraining Loss: 0.161270 \tValidation Loss: 0.063563\n",
      "Epoch: 39 \tTraining Loss: 0.157678 \tValidation Loss: 0.063592\n",
      "Epoch: 40 \tTraining Loss: 0.154963 \tValidation Loss: 0.064374\n",
      "Epoch: 41 \tTraining Loss: 0.151236 \tValidation Loss: 0.065127\n",
      "Epoch: 42 \tTraining Loss: 0.147803 \tValidation Loss: 0.065077\n",
      "Epoch: 43 \tTraining Loss: 0.145047 \tValidation Loss: 0.065131\n",
      "Epoch: 44 \tTraining Loss: 0.141833 \tValidation Loss: 0.066276\n",
      "Epoch: 45 \tTraining Loss: 0.137991 \tValidation Loss: 0.067502\n",
      "Epoch: 46 \tTraining Loss: 0.135417 \tValidation Loss: 0.067976\n",
      "Epoch: 47 \tTraining Loss: 0.132646 \tValidation Loss: 0.067307\n",
      "Epoch: 48 \tTraining Loss: 0.129844 \tValidation Loss: 0.068349\n",
      "Epoch: 49 \tTraining Loss: 0.126307 \tValidation Loss: 0.068801\n",
      "Epoch: 50 \tTraining Loss: 0.122425 \tValidation Loss: 0.070374\n",
      "Epoch: 51 \tTraining Loss: 0.119875 \tValidation Loss: 0.070191\n",
      "Epoch: 52 \tTraining Loss: 0.117406 \tValidation Loss: 0.071132\n",
      "Epoch: 53 \tTraining Loss: 0.115988 \tValidation Loss: 0.072240\n",
      "Epoch: 54 \tTraining Loss: 0.112446 \tValidation Loss: 0.073148\n",
      "Epoch: 55 \tTraining Loss: 0.109871 \tValidation Loss: 0.072436\n",
      "Epoch: 56 \tTraining Loss: 0.107686 \tValidation Loss: 0.072891\n",
      "Epoch: 57 \tTraining Loss: 0.104723 \tValidation Loss: 0.073950\n",
      "Epoch: 58 \tTraining Loss: 0.101993 \tValidation Loss: 0.075546\n",
      "Epoch: 59 \tTraining Loss: 0.101305 \tValidation Loss: 0.076759\n",
      "Epoch: 60 \tTraining Loss: 0.099376 \tValidation Loss: 0.076075\n",
      "Epoch: 61 \tTraining Loss: 0.097398 \tValidation Loss: 0.077462\n",
      "Epoch: 62 \tTraining Loss: 0.094231 \tValidation Loss: 0.078055\n",
      "Epoch: 63 \tTraining Loss: 0.091933 \tValidation Loss: 0.077730\n",
      "Epoch: 64 \tTraining Loss: 0.090148 \tValidation Loss: 0.079929\n",
      "Epoch: 65 \tTraining Loss: 0.088625 \tValidation Loss: 0.080330\n",
      "Epoch: 66 \tTraining Loss: 0.086619 \tValidation Loss: 0.080266\n",
      "Epoch: 67 \tTraining Loss: 0.084975 \tValidation Loss: 0.081079\n",
      "Epoch: 68 \tTraining Loss: 0.082805 \tValidation Loss: 0.082815\n",
      "Epoch: 69 \tTraining Loss: 0.081980 \tValidation Loss: 0.082034\n",
      "Epoch: 70 \tTraining Loss: 0.080319 \tValidation Loss: 0.084628\n",
      "Epoch: 71 \tTraining Loss: 0.077845 \tValidation Loss: 0.084159\n",
      "Epoch: 72 \tTraining Loss: 0.078598 \tValidation Loss: 0.084589\n",
      "Epoch: 73 \tTraining Loss: 0.075270 \tValidation Loss: 0.085463\n",
      "Epoch: 74 \tTraining Loss: 0.074834 \tValidation Loss: 0.086188\n",
      "Epoch: 75 \tTraining Loss: 0.072688 \tValidation Loss: 0.088585\n",
      "Epoch: 76 \tTraining Loss: 0.071576 \tValidation Loss: 0.088508\n",
      "Epoch: 77 \tTraining Loss: 0.071722 \tValidation Loss: 0.088335\n",
      "Epoch: 78 \tTraining Loss: 0.069442 \tValidation Loss: 0.093022\n",
      "Epoch: 79 \tTraining Loss: 0.068254 \tValidation Loss: 0.091485\n",
      "Epoch: 80 \tTraining Loss: 0.065953 \tValidation Loss: 0.091058\n",
      "Epoch: 81 \tTraining Loss: 0.065347 \tValidation Loss: 0.093462\n",
      "Epoch: 82 \tTraining Loss: 0.064884 \tValidation Loss: 0.095376\n",
      "Epoch: 83 \tTraining Loss: 0.064965 \tValidation Loss: 0.093044\n",
      "Epoch: 84 \tTraining Loss: 0.063162 \tValidation Loss: 0.093253\n",
      "Epoch: 85 \tTraining Loss: 0.062135 \tValidation Loss: 0.094029\n",
      "Epoch: 86 \tTraining Loss: 0.059436 \tValidation Loss: 0.096356\n",
      "Epoch: 87 \tTraining Loss: 0.059906 \tValidation Loss: 0.095408\n",
      "Epoch: 88 \tTraining Loss: 0.059368 \tValidation Loss: 0.097422\n",
      "Epoch: 89 \tTraining Loss: 0.058412 \tValidation Loss: 0.098653\n",
      "Epoch: 90 \tTraining Loss: 0.057022 \tValidation Loss: 0.098439\n",
      "Epoch: 91 \tTraining Loss: 0.057082 \tValidation Loss: 0.097932\n",
      "Epoch: 92 \tTraining Loss: 0.056404 \tValidation Loss: 0.099816\n",
      "Epoch: 93 \tTraining Loss: 0.054997 \tValidation Loss: 0.100771\n",
      "Epoch: 94 \tTraining Loss: 0.054187 \tValidation Loss: 0.103132\n",
      "Epoch: 95 \tTraining Loss: 0.053137 \tValidation Loss: 0.100651\n",
      "Epoch: 96 \tTraining Loss: 0.054066 \tValidation Loss: 0.101793\n",
      "Epoch: 97 \tTraining Loss: 0.051770 \tValidation Loss: 0.103462\n",
      "Epoch: 98 \tTraining Loss: 0.051655 \tValidation Loss: 0.104289\n",
      "Epoch: 99 \tTraining Loss: 0.050891 \tValidation Loss: 0.104964\n",
      "Epoch: 100 \tTraining Loss: 0.051600 \tValidation Loss: 0.104855\n",
      "Time elapsed: 564.01 s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "# number of epochs to train the model\n",
    "n_epochs = 50\n",
    "\n",
    "# initialize tracker for minimum validation loss\n",
    "valid_loss_min = np.Inf # set initial \"min\" to infinity\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "  # monitor training loss\n",
    "  train_loss = 0.0\n",
    "  valid_loss = 0.0\n",
    "\n",
    "  ###################\n",
    "  # train the model #\n",
    "  ###################\n",
    "  model_bi_10.train() # prep model for training\n",
    "  for data, target in train_loader_bi_10:\n",
    "    # transfer data and target to GPU\n",
    "    data, target = data.to(device), target.to(device)  \n",
    "    # clear the gradients of all optimized variables\n",
    "    optimizer_bi_10.zero_grad()\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output = model_bi_10(data.float())\n",
    "    # calculate the loss\n",
    "    loss = criterion(output, target)\n",
    "    # backward pass: compute gradient of the loss with respect to model parameters\n",
    "    loss.backward()\n",
    "    # perform a single optimization step (parameter update)\n",
    "    optimizer_bi_10.step()\n",
    "    # update running training loss\n",
    "    train_loss += loss.item()*data.size(0)\n",
    "\n",
    "  ######################\n",
    "  # validate the model #\n",
    "  ######################\n",
    "  model_bi_10.eval() # prep model for evaluation\n",
    "  for data, target in valid_loader_bi_10:\n",
    "    # transfer data and target to GPU\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output = model_bi_10(data.float())\n",
    "    # calculate the loss\n",
    "    loss = criterion(output, target)\n",
    "    # update running validation loss\n",
    "    valid_loss += loss.item()*data.size(0)\n",
    "    \n",
    "\n",
    "  # print training/validation statistics\n",
    "  # calculate average loss over an epoch\n",
    "  train_loss = train_loss/len(train_loader_bi_10.dataset)\n",
    "  valid_loss = valid_loss/len(valid_loader_bi_10.dataset)\n",
    "\n",
    "  print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch+1, train_loss, valid_loss))\n",
    "    \n",
    "  # save model if validation loss has decreased\n",
    "  if valid_loss <= valid_loss_min:\n",
    "    print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'\n",
    "        .format(valid_loss_min, valid_loss))\n",
    "    torch.save(model_bi_10.state_dict(), 'model.pt')\n",
    "    valid_loss_min = valid_loss\n",
    "\n",
    "end = time.time()\n",
    "print('Time elapsed: %.2f s' % (end - start))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "HjfOrjwVbjNI"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model with the lowest validation loss\n",
    "model_bi_10.load_state_dict(torch.load('model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "yRch1jK2bjNI"
   },
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for out outputs\n",
    "with torch.no_grad():\n",
    "  for data in test_loader_bi_10:\n",
    "    embeddings, labels = data\n",
    "    # calculating outputs by running embeddings through the network\n",
    "    model_bi_10.to(\"cpu\")\n",
    "    outputs = model_bi_10(embeddings.float())\n",
    "    # the class with the highest score is what we choose as prediction\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "1IMdrVNxbjNI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy_MLP-model_first-10-word-vectors_pretrained-word2vec-model_binary-test-set: 86 %\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy_MLP-model_first-10-word-vectors_pretrained-word2vec-model_binary-test-set: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J6qRoi7Nwyfu"
   },
   "source": [
    "#### Ternary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "rItL_Hu4bjNJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.846168 \tValidation Loss: 0.204521\n",
      "Validation loss decreased (inf --> 0.204521).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 0.766529 \tValidation Loss: 0.170479\n",
      "Validation loss decreased (0.204521 --> 0.170479).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 0.678321 \tValidation Loss: 0.156886\n",
      "Validation loss decreased (0.170479 --> 0.156886).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 0.640986 \tValidation Loss: 0.150586\n",
      "Validation loss decreased (0.156886 --> 0.150586).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 0.618264 \tValidation Loss: 0.146684\n",
      "Validation loss decreased (0.150586 --> 0.146684).  Saving model ...\n",
      "Epoch: 6 \tTraining Loss: 0.602868 \tValidation Loss: 0.144028\n",
      "Validation loss decreased (0.146684 --> 0.144028).  Saving model ...\n",
      "Epoch: 7 \tTraining Loss: 0.590251 \tValidation Loss: 0.141658\n",
      "Validation loss decreased (0.144028 --> 0.141658).  Saving model ...\n",
      "Epoch: 8 \tTraining Loss: 0.579080 \tValidation Loss: 0.139294\n",
      "Validation loss decreased (0.141658 --> 0.139294).  Saving model ...\n",
      "Epoch: 9 \tTraining Loss: 0.569317 \tValidation Loss: 0.137432\n",
      "Validation loss decreased (0.139294 --> 0.137432).  Saving model ...\n",
      "Epoch: 10 \tTraining Loss: 0.559563 \tValidation Loss: 0.135777\n",
      "Validation loss decreased (0.137432 --> 0.135777).  Saving model ...\n",
      "Epoch: 11 \tTraining Loss: 0.551630 \tValidation Loss: 0.134265\n",
      "Validation loss decreased (0.135777 --> 0.134265).  Saving model ...\n",
      "Epoch: 12 \tTraining Loss: 0.544020 \tValidation Loss: 0.133111\n",
      "Validation loss decreased (0.134265 --> 0.133111).  Saving model ...\n",
      "Epoch: 13 \tTraining Loss: 0.536530 \tValidation Loss: 0.132025\n",
      "Validation loss decreased (0.133111 --> 0.132025).  Saving model ...\n",
      "Epoch: 14 \tTraining Loss: 0.529376 \tValidation Loss: 0.130940\n",
      "Validation loss decreased (0.132025 --> 0.130940).  Saving model ...\n",
      "Epoch: 15 \tTraining Loss: 0.523278 \tValidation Loss: 0.130034\n",
      "Validation loss decreased (0.130940 --> 0.130034).  Saving model ...\n",
      "Epoch: 16 \tTraining Loss: 0.517398 \tValidation Loss: 0.129433\n",
      "Validation loss decreased (0.130034 --> 0.129433).  Saving model ...\n",
      "Epoch: 17 \tTraining Loss: 0.511325 \tValidation Loss: 0.129001\n",
      "Validation loss decreased (0.129433 --> 0.129001).  Saving model ...\n",
      "Epoch: 18 \tTraining Loss: 0.507025 \tValidation Loss: 0.128350\n",
      "Validation loss decreased (0.129001 --> 0.128350).  Saving model ...\n",
      "Epoch: 19 \tTraining Loss: 0.500900 \tValidation Loss: 0.127896\n",
      "Validation loss decreased (0.128350 --> 0.127896).  Saving model ...\n",
      "Epoch: 20 \tTraining Loss: 0.495959 \tValidation Loss: 0.127507\n",
      "Validation loss decreased (0.127896 --> 0.127507).  Saving model ...\n",
      "Epoch: 21 \tTraining Loss: 0.491190 \tValidation Loss: 0.127363\n",
      "Validation loss decreased (0.127507 --> 0.127363).  Saving model ...\n",
      "Epoch: 22 \tTraining Loss: 0.485357 \tValidation Loss: 0.127083\n",
      "Validation loss decreased (0.127363 --> 0.127083).  Saving model ...\n",
      "Epoch: 23 \tTraining Loss: 0.480709 \tValidation Loss: 0.126781\n",
      "Validation loss decreased (0.127083 --> 0.126781).  Saving model ...\n",
      "Epoch: 24 \tTraining Loss: 0.477050 \tValidation Loss: 0.126657\n",
      "Validation loss decreased (0.126781 --> 0.126657).  Saving model ...\n",
      "Epoch: 25 \tTraining Loss: 0.471640 \tValidation Loss: 0.126562\n",
      "Validation loss decreased (0.126657 --> 0.126562).  Saving model ...\n",
      "Epoch: 26 \tTraining Loss: 0.466567 \tValidation Loss: 0.126470\n",
      "Validation loss decreased (0.126562 --> 0.126470).  Saving model ...\n",
      "Epoch: 27 \tTraining Loss: 0.462610 \tValidation Loss: 0.126264\n",
      "Validation loss decreased (0.126470 --> 0.126264).  Saving model ...\n",
      "Epoch: 28 \tTraining Loss: 0.457906 \tValidation Loss: 0.126916\n",
      "Epoch: 29 \tTraining Loss: 0.454292 \tValidation Loss: 0.126925\n",
      "Epoch: 30 \tTraining Loss: 0.449894 \tValidation Loss: 0.126957\n",
      "Epoch: 31 \tTraining Loss: 0.445063 \tValidation Loss: 0.127216\n",
      "Epoch: 32 \tTraining Loss: 0.439310 \tValidation Loss: 0.126968\n",
      "Epoch: 33 \tTraining Loss: 0.436552 \tValidation Loss: 0.127687\n",
      "Epoch: 34 \tTraining Loss: 0.431908 \tValidation Loss: 0.127676\n",
      "Epoch: 35 \tTraining Loss: 0.426904 \tValidation Loss: 0.127684\n",
      "Epoch: 36 \tTraining Loss: 0.424055 \tValidation Loss: 0.128827\n",
      "Epoch: 37 \tTraining Loss: 0.419114 \tValidation Loss: 0.129217\n",
      "Epoch: 38 \tTraining Loss: 0.415982 \tValidation Loss: 0.128887\n",
      "Epoch: 39 \tTraining Loss: 0.412187 \tValidation Loss: 0.130074\n",
      "Epoch: 40 \tTraining Loss: 0.407360 \tValidation Loss: 0.129904\n",
      "Epoch: 41 \tTraining Loss: 0.403150 \tValidation Loss: 0.130255\n",
      "Epoch: 42 \tTraining Loss: 0.398671 \tValidation Loss: 0.131296\n",
      "Epoch: 43 \tTraining Loss: 0.395453 \tValidation Loss: 0.131403\n",
      "Epoch: 44 \tTraining Loss: 0.391564 \tValidation Loss: 0.132065\n",
      "Epoch: 45 \tTraining Loss: 0.389409 \tValidation Loss: 0.131973\n",
      "Epoch: 46 \tTraining Loss: 0.384993 \tValidation Loss: 0.133111\n",
      "Epoch: 47 \tTraining Loss: 0.381603 \tValidation Loss: 0.133061\n",
      "Epoch: 48 \tTraining Loss: 0.377910 \tValidation Loss: 0.134265\n",
      "Epoch: 49 \tTraining Loss: 0.373646 \tValidation Loss: 0.134562\n",
      "Epoch: 50 \tTraining Loss: 0.370542 \tValidation Loss: 0.135133\n",
      "Epoch: 51 \tTraining Loss: 0.367100 \tValidation Loss: 0.136237\n",
      "Epoch: 52 \tTraining Loss: 0.363530 \tValidation Loss: 0.136990\n",
      "Epoch: 53 \tTraining Loss: 0.359517 \tValidation Loss: 0.137692\n",
      "Epoch: 54 \tTraining Loss: 0.357701 \tValidation Loss: 0.138282\n",
      "Epoch: 55 \tTraining Loss: 0.353077 \tValidation Loss: 0.138803\n",
      "Epoch: 56 \tTraining Loss: 0.351217 \tValidation Loss: 0.140200\n",
      "Epoch: 57 \tTraining Loss: 0.348464 \tValidation Loss: 0.139900\n",
      "Epoch: 58 \tTraining Loss: 0.344716 \tValidation Loss: 0.141654\n",
      "Epoch: 59 \tTraining Loss: 0.341931 \tValidation Loss: 0.141779\n",
      "Epoch: 60 \tTraining Loss: 0.338579 \tValidation Loss: 0.142632\n",
      "Epoch: 61 \tTraining Loss: 0.335390 \tValidation Loss: 0.143822\n",
      "Epoch: 62 \tTraining Loss: 0.332574 \tValidation Loss: 0.144870\n",
      "Epoch: 63 \tTraining Loss: 0.329774 \tValidation Loss: 0.145644\n",
      "Epoch: 64 \tTraining Loss: 0.327404 \tValidation Loss: 0.145948\n",
      "Epoch: 65 \tTraining Loss: 0.324971 \tValidation Loss: 0.146662\n",
      "Epoch: 66 \tTraining Loss: 0.322448 \tValidation Loss: 0.148154\n",
      "Epoch: 67 \tTraining Loss: 0.320830 \tValidation Loss: 0.149586\n",
      "Epoch: 68 \tTraining Loss: 0.317317 \tValidation Loss: 0.149423\n",
      "Epoch: 69 \tTraining Loss: 0.315049 \tValidation Loss: 0.151610\n",
      "Epoch: 70 \tTraining Loss: 0.311146 \tValidation Loss: 0.151230\n",
      "Epoch: 71 \tTraining Loss: 0.311245 \tValidation Loss: 0.152792\n",
      "Epoch: 72 \tTraining Loss: 0.306788 \tValidation Loss: 0.152724\n",
      "Epoch: 73 \tTraining Loss: 0.306917 \tValidation Loss: 0.153180\n",
      "Epoch: 74 \tTraining Loss: 0.302153 \tValidation Loss: 0.154365\n",
      "Epoch: 75 \tTraining Loss: 0.300734 \tValidation Loss: 0.156172\n",
      "Epoch: 76 \tTraining Loss: 0.297921 \tValidation Loss: 0.157426\n",
      "Epoch: 77 \tTraining Loss: 0.296506 \tValidation Loss: 0.160293\n",
      "Epoch: 78 \tTraining Loss: 0.294272 \tValidation Loss: 0.158105\n",
      "Epoch: 79 \tTraining Loss: 0.291276 \tValidation Loss: 0.159341\n",
      "Epoch: 80 \tTraining Loss: 0.291522 \tValidation Loss: 0.159265\n",
      "Epoch: 81 \tTraining Loss: 0.287603 \tValidation Loss: 0.161364\n",
      "Epoch: 82 \tTraining Loss: 0.285683 \tValidation Loss: 0.161579\n",
      "Epoch: 83 \tTraining Loss: 0.285144 \tValidation Loss: 0.162822\n",
      "Epoch: 84 \tTraining Loss: 0.283505 \tValidation Loss: 0.163552\n",
      "Epoch: 85 \tTraining Loss: 0.279635 \tValidation Loss: 0.164782\n",
      "Epoch: 86 \tTraining Loss: 0.278734 \tValidation Loss: 0.166787\n",
      "Epoch: 87 \tTraining Loss: 0.276843 \tValidation Loss: 0.166470\n",
      "Epoch: 88 \tTraining Loss: 0.275404 \tValidation Loss: 0.168077\n",
      "Epoch: 89 \tTraining Loss: 0.274437 \tValidation Loss: 0.167860\n",
      "Epoch: 90 \tTraining Loss: 0.272881 \tValidation Loss: 0.169689\n",
      "Epoch: 91 \tTraining Loss: 0.269631 \tValidation Loss: 0.170061\n",
      "Epoch: 92 \tTraining Loss: 0.267011 \tValidation Loss: 0.171146\n",
      "Epoch: 93 \tTraining Loss: 0.266967 \tValidation Loss: 0.173228\n",
      "Epoch: 94 \tTraining Loss: 0.266749 \tValidation Loss: 0.171802\n",
      "Epoch: 95 \tTraining Loss: 0.262893 \tValidation Loss: 0.174288\n",
      "Epoch: 96 \tTraining Loss: 0.262503 \tValidation Loss: 0.174069\n",
      "Epoch: 97 \tTraining Loss: 0.260001 \tValidation Loss: 0.175928\n",
      "Epoch: 98 \tTraining Loss: 0.259727 \tValidation Loss: 0.176098\n",
      "Epoch: 99 \tTraining Loss: 0.258016 \tValidation Loss: 0.179762\n",
      "Epoch: 100 \tTraining Loss: 0.256084 \tValidation Loss: 0.178049\n",
      "Time elapsed: 708.91 s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "# number of epochs to train the model\n",
    "n_epochs = 50\n",
    "\n",
    "# initialize tracker for minimum validation loss\n",
    "valid_loss_min = np.Inf # set initial \"min\" to infinity\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "  # monitor training loss\n",
    "  train_loss = 0.0\n",
    "  valid_loss = 0.0\n",
    "\n",
    "  ###################\n",
    "  # train the model #\n",
    "  ###################\n",
    "  model_te_10.train() # prep model for training\n",
    "  for data, target in train_loader_te_10:\n",
    "    # transfer data and target to GPU\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    # clear the gradients of all optimized variables\n",
    "    optimizer_te_10.zero_grad()\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output = model_te_10(data.float())\n",
    "    # calculate the loss\n",
    "    loss = criterion(output, target)\n",
    "    # backward pass: compute gradient of the loss with respect to model parameters\n",
    "    loss.backward()\n",
    "    # perform a single optimization step (parameter update)\n",
    "    optimizer_te_10.step()\n",
    "    # update running training loss\n",
    "    train_loss += loss.item()*data.size(0)\n",
    "\n",
    "  ######################\n",
    "  # validate the model #\n",
    "  ######################\n",
    "  model_te_10.eval() # prep model for evaluation\n",
    "  for data, target in valid_loader_te_10:\n",
    "    # transfer data and target to GPU\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output = model_te_10(data.float())\n",
    "    # calculate the loss\n",
    "    loss = criterion(output, target)\n",
    "    # update running validation loss\n",
    "    valid_loss += loss.item()*data.size(0)\n",
    "    \n",
    "\n",
    "  # print training/validation statistics\n",
    "  # calculate average loss over an epoch\n",
    "  train_loss = train_loss/len(train_loader_te_10.dataset)\n",
    "  valid_loss = valid_loss/len(valid_loader_te_10.dataset)\n",
    "\n",
    "  print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch+1, train_loss, valid_loss))\n",
    "    \n",
    "  # save model if validation loss has decreased\n",
    "  if valid_loss <= valid_loss_min:\n",
    "    print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'\n",
    "        .format(valid_loss_min, valid_loss))\n",
    "    torch.save(model_te_10.state_dict(), 'model.pt')\n",
    "    valid_loss_min = valid_loss\n",
    "    \n",
    "end = time.time()\n",
    "print('Time elapsed: %.2f s' % (end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "BXTWn3d6bjNJ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model with the lowest validation loss\n",
    "model_te_10.load_state_dict(torch.load('model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "IbMj4iUobjNJ"
   },
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for out outputs\n",
    "with torch.no_grad():\n",
    "  for data in test_loader_te_10:\n",
    "    embeddings, labels = data\n",
    "    # calculating outputs by running embeddings through the network\n",
    "    model_te_10.to(\"cpu\")\n",
    "    outputs = model_te_10(embeddings.float())\n",
    "    # the class with the highest score is what we choose as prediction\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "jQG6PsN-bjNL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy_MLP-model_first-10-word-vectors_pretrained-word2vec-model_ternary-test-set: 73 %\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy_MLP-model_first-10-word-vectors_pretrained-word2vec-model_ternary-test-set: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CLc1WMmh7HBx"
   },
   "source": [
    "## b.2 Use My Own Word2Vec Model to Compute Input Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JVRF26Bt7HBy"
   },
   "source": [
    "### b.2.1 Compute Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "flaShJOA7HBy"
   },
   "source": [
    "Input features are the first 10 word embeddings of each review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1YUP2R1N7HBy"
   },
   "source": [
    "#### Use **my own model** to concatenate the first 10 word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "brssWaOX7HBy"
   },
   "outputs": [],
   "source": [
    "X_train_bi_10_series = X_train_bi.apply(lambda x: word2v_concat(x, model.wv))\n",
    "X_train_bi_10 = np.array(X_train_bi_10_series.values.tolist())\n",
    "\n",
    "X_test_bi_10_series = X_test_bi.apply(lambda x: word2v_concat(x, model.wv))\n",
    "X_test_bi_10 = np.array(X_test_bi_10_series.values.tolist())\n",
    "\n",
    "X_train_te_10_series = X_train_te.apply(lambda x: word2v_concat(x, model.wv))\n",
    "X_train_te_10 = np.array(X_train_te_10_series.values.tolist())\n",
    "\n",
    "X_test_te_10_series = X_test_te.apply(lambda x: word2v_concat(x, model.wv))\n",
    "X_test_te_10 = np.array(X_test_te_10_series.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "DvRnPWYB7HBy"
   },
   "outputs": [],
   "source": [
    "# Apply the idx_nan function to remove the NaN's and corresponding\n",
    "# labels (if any)\n",
    "idx_nan_train_bi = idx_nan(X_train_bi_10)\n",
    "if idx_nan_train_bi != None:\n",
    "    X_train_bi_10 = np.delete(X_train_bi_10, idx_nan_train_bi, 0)\n",
    "    y_train_bi_10 = np.delete(y_train_bi, idx_nan_train_bi)\n",
    "else:\n",
    "    y_train_bi_10 = y_train_bi\n",
    "\n",
    "idx_nan_test_bi = idx_nan(X_test_bi_10)\n",
    "if idx_nan_test_bi != None:\n",
    "    X_test_bi_10 = np.delete(X_test_bi_10, idx_nan_test_bi, 0)\n",
    "    y_test_bi_10 = np.delete(y_test_bi, idx_nan_test_bi)\n",
    "else:\n",
    "    y_test_bi_10 = y_test_bi\n",
    "\n",
    "idx_nan_train_te = idx_nan(X_train_te_10)\n",
    "if idx_nan_train_te != None:\n",
    "    X_train_te_10 = np.delete(X_train_te_10, idx_nan_train_te, 0)\n",
    "    y_train_te_10 = np.delete(y_train_te, idx_nan_train_te)\n",
    "else:\n",
    "    y_train_te_10 = y_train_te\n",
    "\n",
    "idx_nan_test_te = idx_nan(X_test_te_10)\n",
    "if idx_nan_test_te != None:\n",
    "    X_test_te_10 = np.delete(X_test_te_10, idx_nan_test_te, 0)\n",
    "    y_test_te_10 = np.delete(y_test_te, idx_nan_test_te)\n",
    "else:\n",
    "    y_test_te_10 = y_test_te"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AlzG6Skd7HBz"
   },
   "source": [
    "### b.2.2 Define Dataset Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "CRsM9tDa3tou"
   },
   "outputs": [],
   "source": [
    "class Train(Dataset):\n",
    "  def __init__(self, xtrain, ytrain):\n",
    "    'Initialization'\n",
    "    self.data = xtrain\n",
    "    self.labels = ytrain\n",
    "\n",
    "  def __len__(self):\n",
    "    'Denotes the total number of samples'\n",
    "    return len(self.data)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    'Generates one sample of data'\n",
    "    X = self.data[index]\n",
    "    y = self.labels[index]\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "lleVDSnK3tou"
   },
   "outputs": [],
   "source": [
    "class Test(Dataset):\n",
    "  def __init__(self, xtest, ytest):\n",
    "    'Initialization'\n",
    "    self.data = xtest\n",
    "    self.labels = ytest\n",
    "\n",
    "  def __len__(self):\n",
    "    'Denotes the total number of samples'\n",
    "    return len(self.data)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    'Generates one sample of data'\n",
    "    X = self.data[index]\n",
    "    y = self.labels[index]\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LGhKR84G7HBz"
   },
   "source": [
    "### b.2.3 Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "5ivKipvr7HBz"
   },
   "outputs": [],
   "source": [
    "# use the first 10 Word2Vec vectors as input features\n",
    "train_data_bi_10, test_data_bi_10 = Train(X_train_bi_10, y_train_bi_10-1), Test(X_test_bi_10, y_test_bi_10-1)\n",
    "train_data_te_10, test_data_te_10 = Train(X_train_te_10, y_train_te_10-1), Test(X_test_te_10, y_test_te_10-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q_eptqkk7HB0"
   },
   "source": [
    "#### Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "90kvgPDd7HB0"
   },
   "outputs": [],
   "source": [
    "# number of subprocesses to use for data loading\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 100\n",
    "# percentage of training set to use as validation\n",
    "valid_size = 0.2\n",
    "\n",
    "# obtain training indices that will be used for validation\n",
    "num_train = len(train_data_bi_10)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "# define samples for obtaining training and validation batches\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "# prepare data loaders\n",
    "train_loader_bi_10 = torch.utils.data.DataLoader(train_data_bi_10, batch_size=batch_size,\n",
    "                                           sampler=train_sampler, num_workers\n",
    "                                           =num_workers)\n",
    "valid_loader_bi_10 = torch.utils.data.DataLoader(train_data_bi_10, batch_size=batch_size,\n",
    "                                           sampler=valid_sampler, num_workers\n",
    "                                           =num_workers)\n",
    "test_loader_bi_10 = torch.utils.data.DataLoader(test_data_bi_10, batch_size=batch_size,\n",
    "                                           num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GITTZV867HB0"
   },
   "source": [
    "#### Ternary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "3ejspkZg7HB0"
   },
   "outputs": [],
   "source": [
    "# number of subprocesses to use for data loading\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 100\n",
    "# percentage of training set to use as validation\n",
    "valid_size = 0.2\n",
    "\n",
    "# obtain training indices that will be used for validation\n",
    "num_train = len(train_data_te_10)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "# define samples for obtaining training and validation batches\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "# prepare data loaders\n",
    "train_loader_te_10 = torch.utils.data.DataLoader(train_data_te_10, batch_size=batch_size,\n",
    "                                           sampler=train_sampler, num_workers\n",
    "                                           =num_workers)\n",
    "valid_loader_te_10 = torch.utils.data.DataLoader(train_data_te_10, batch_size=batch_size,\n",
    "                                           sampler=valid_sampler, num_workers\n",
    "                                           =num_workers)\n",
    "test_loader_te_10 = torch.utils.data.DataLoader(test_data_te_10, batch_size=batch_size,\n",
    "                                           num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v1WjSHa67HB0"
   },
   "source": [
    "### b.2.4 Define the MLP architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "s8szxTZu3tow"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "fQ_XQJU07HB0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ThreeLayerMLP(\n",
      "  (linear1): Linear(in_features=3000, out_features=50, bias=True)\n",
      "  (linear2): Linear(in_features=50, out_features=10, bias=True)\n",
      "  (linear3): Linear(in_features=10, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "ThreeLayerMLP(\n",
      "  (linear1): Linear(in_features=3000, out_features=50, bias=True)\n",
      "  (linear2): Linear(in_features=50, out_features=10, bias=True)\n",
      "  (linear3): Linear(in_features=10, out_features=3, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# define the MLP architecture\n",
    "class ThreeLayerMLP(nn.Module):\n",
    "  def __init__(self, D_in, H1, H2, D_out):\n",
    "    super().__init__()\n",
    "    self.linear1 = nn.Linear(D_in, H1)\n",
    "    self.linear2 = nn.Linear(H1, H2)\n",
    "    self.linear3 = nn.Linear(H2, D_out)\n",
    "    # dropout layer (p=0.2)\n",
    "    self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "  def forward(self, x):\n",
    "    # add hidden layer, with relu activation function\n",
    "    h1_relu = F.relu(self.linear1(x))\n",
    "    # add dropout layer\n",
    "    h1_drop = self.dropout(h1_relu)\n",
    "    # add another hidden layer, with relu activation function\n",
    "    h2_relu = F.relu(self.linear2(h1_drop))\n",
    "    # add another dropout layer\n",
    "    h2_drop = self.dropout(h2_relu)\n",
    "    # add output layer\n",
    "    h2_output = self.linear3(h2_drop)\n",
    "\n",
    "    return h2_output\n",
    "\n",
    "# initialize MLP\n",
    "model_bi_10 = ThreeLayerMLP(3000, 50, 10, 2)\n",
    "model_te_10 = ThreeLayerMLP(3000, 50, 10, 3)\n",
    "model_bi_10.to(device)\n",
    "model_te_10.to(device)\n",
    "print(model_bi_10)\n",
    "print(model_te_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "B_ygGdZg7HB0"
   },
   "outputs": [],
   "source": [
    "# specify loss function (categorical cross-entropy)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# specify optimizer (stochastic gradient descent) and learning rate = 0.01\n",
    "optimizer_bi_10 = torch.optim.SGD(model_bi_10.parameters(), lr=0.0075)\n",
    "optimizer_te_10 = torch.optim.SGD(model_te_10.parameters(), lr=0.0075)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oQ9mMUZV7HB0"
   },
   "source": [
    "### b.2.5 Train the Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gol-ze3w7HB1"
   },
   "source": [
    "#### Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "-0rzGRJF7HB1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.393802 \tValidation Loss: 0.069251\n",
      "Validation loss decreased (inf --> 0.069251).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 0.282807 \tValidation Loss: 0.063720\n",
      "Validation loss decreased (0.069251 --> 0.063720).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 0.261787 \tValidation Loss: 0.061607\n",
      "Validation loss decreased (0.063720 --> 0.061607).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 0.249446 \tValidation Loss: 0.060795\n",
      "Validation loss decreased (0.061607 --> 0.060795).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 0.240217 \tValidation Loss: 0.060237\n",
      "Validation loss decreased (0.060795 --> 0.060237).  Saving model ...\n",
      "Epoch: 6 \tTraining Loss: 0.230787 \tValidation Loss: 0.060130\n",
      "Validation loss decreased (0.060237 --> 0.060130).  Saving model ...\n",
      "Epoch: 7 \tTraining Loss: 0.221878 \tValidation Loss: 0.059952\n",
      "Validation loss decreased (0.060130 --> 0.059952).  Saving model ...\n",
      "Epoch: 8 \tTraining Loss: 0.213783 \tValidation Loss: 0.060323\n",
      "Epoch: 9 \tTraining Loss: 0.206755 \tValidation Loss: 0.060887\n",
      "Epoch: 10 \tTraining Loss: 0.199473 \tValidation Loss: 0.060819\n",
      "Epoch: 11 \tTraining Loss: 0.191726 \tValidation Loss: 0.061269\n",
      "Epoch: 12 \tTraining Loss: 0.184261 \tValidation Loss: 0.062745\n",
      "Epoch: 13 \tTraining Loss: 0.178922 \tValidation Loss: 0.062917\n",
      "Epoch: 14 \tTraining Loss: 0.171570 \tValidation Loss: 0.063840\n",
      "Epoch: 15 \tTraining Loss: 0.166377 \tValidation Loss: 0.065282\n",
      "Epoch: 16 \tTraining Loss: 0.159396 \tValidation Loss: 0.066423\n",
      "Epoch: 17 \tTraining Loss: 0.152954 \tValidation Loss: 0.068093\n",
      "Epoch: 18 \tTraining Loss: 0.147995 \tValidation Loss: 0.068527\n",
      "Epoch: 19 \tTraining Loss: 0.142578 \tValidation Loss: 0.071074\n",
      "Epoch: 20 \tTraining Loss: 0.136048 \tValidation Loss: 0.072137\n",
      "Epoch: 21 \tTraining Loss: 0.133057 \tValidation Loss: 0.073319\n",
      "Epoch: 22 \tTraining Loss: 0.127488 \tValidation Loss: 0.075136\n",
      "Epoch: 23 \tTraining Loss: 0.123205 \tValidation Loss: 0.077003\n",
      "Epoch: 24 \tTraining Loss: 0.118439 \tValidation Loss: 0.078257\n",
      "Epoch: 25 \tTraining Loss: 0.115339 \tValidation Loss: 0.080102\n",
      "Epoch: 26 \tTraining Loss: 0.112929 \tValidation Loss: 0.081814\n",
      "Epoch: 27 \tTraining Loss: 0.107490 \tValidation Loss: 0.084736\n",
      "Epoch: 28 \tTraining Loss: 0.103432 \tValidation Loss: 0.085495\n",
      "Epoch: 29 \tTraining Loss: 0.102363 \tValidation Loss: 0.086717\n",
      "Epoch: 30 \tTraining Loss: 0.098659 \tValidation Loss: 0.089024\n",
      "Epoch: 31 \tTraining Loss: 0.095733 \tValidation Loss: 0.089594\n",
      "Epoch: 32 \tTraining Loss: 0.092838 \tValidation Loss: 0.093100\n",
      "Epoch: 33 \tTraining Loss: 0.091085 \tValidation Loss: 0.093127\n",
      "Epoch: 34 \tTraining Loss: 0.089561 \tValidation Loss: 0.094901\n",
      "Epoch: 35 \tTraining Loss: 0.087495 \tValidation Loss: 0.095525\n",
      "Epoch: 36 \tTraining Loss: 0.083787 \tValidation Loss: 0.098508\n",
      "Epoch: 37 \tTraining Loss: 0.081709 \tValidation Loss: 0.099853\n",
      "Epoch: 38 \tTraining Loss: 0.080438 \tValidation Loss: 0.100904\n",
      "Epoch: 39 \tTraining Loss: 0.078275 \tValidation Loss: 0.102171\n",
      "Epoch: 40 \tTraining Loss: 0.076264 \tValidation Loss: 0.103871\n",
      "Epoch: 41 \tTraining Loss: 0.075965 \tValidation Loss: 0.103230\n",
      "Epoch: 42 \tTraining Loss: 0.073567 \tValidation Loss: 0.105615\n",
      "Epoch: 43 \tTraining Loss: 0.073372 \tValidation Loss: 0.105714\n",
      "Epoch: 44 \tTraining Loss: 0.071379 \tValidation Loss: 0.109157\n",
      "Epoch: 45 \tTraining Loss: 0.070263 \tValidation Loss: 0.109322\n",
      "Epoch: 46 \tTraining Loss: 0.068288 \tValidation Loss: 0.111313\n",
      "Epoch: 47 \tTraining Loss: 0.067745 \tValidation Loss: 0.111784\n",
      "Epoch: 48 \tTraining Loss: 0.066024 \tValidation Loss: 0.112273\n",
      "Epoch: 49 \tTraining Loss: 0.063627 \tValidation Loss: 0.113243\n",
      "Epoch: 50 \tTraining Loss: 0.063367 \tValidation Loss: 0.117025\n",
      "Time elapsed: 268.87 s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "# number of epochs to train the model\n",
    "n_epochs = 25\n",
    "\n",
    "# initialize tracker for minimum validation loss\n",
    "valid_loss_min = np.Inf # set initial \"min\" to infinity\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "  # monitor training loss\n",
    "  train_loss = 0.0\n",
    "  valid_loss = 0.0\n",
    "\n",
    "  ###################\n",
    "  # train the model #\n",
    "  ###################\n",
    "  model_bi_10.train() # prep model for training\n",
    "  for data, target in train_loader_bi_10:\n",
    "    # transfer data and target to GPU\n",
    "    data, target = data.to(device), target.to(device)  \n",
    "    # clear the gradients of all optimized variables\n",
    "    optimizer_bi_10.zero_grad()\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output = model_bi_10(data.float())\n",
    "    # calculate the loss\n",
    "    loss = criterion(output, target)\n",
    "    # backward pass: compute gradient of the loss with respect to model parameters\n",
    "    loss.backward()\n",
    "    # perform a single optimization step (parameter update)\n",
    "    optimizer_bi_10.step()\n",
    "    # update running training loss\n",
    "    train_loss += loss.item()*data.size(0)\n",
    "\n",
    "  ######################\n",
    "  # validate the model #\n",
    "  ######################\n",
    "  model_bi_10.eval() # prep model for evaluation\n",
    "  for data, target in valid_loader_bi_10:\n",
    "    # transfer data and target to GPU\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output = model_bi_10(data.float())\n",
    "    # calculate the loss\n",
    "    loss = criterion(output, target)\n",
    "    # update running validation loss\n",
    "    valid_loss += loss.item()*data.size(0)\n",
    "    \n",
    "\n",
    "  # print training/validation statistics\n",
    "  # calculate average loss over an epoch\n",
    "  train_loss = train_loss/len(train_loader_bi_10.dataset)\n",
    "  valid_loss = valid_loss/len(valid_loader_bi_10.dataset)\n",
    "\n",
    "  print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch+1, train_loss, valid_loss))\n",
    "    \n",
    "  # save model if validation loss has decreased\n",
    "  if valid_loss <= valid_loss_min:\n",
    "    print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'\n",
    "        .format(valid_loss_min, valid_loss))\n",
    "    torch.save(model_bi_10.state_dict(), 'model.pt')\n",
    "    valid_loss_min = valid_loss\n",
    "\n",
    "end = time.time()\n",
    "print('Time elapsed: %.2f s' % (end - start))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "MX-jNtzt7HB1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model with the lowest validation loss\n",
    "model_bi_10.load_state_dict(torch.load('model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "hoUzbrcJ7HB1"
   },
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for out outputs\n",
    "with torch.no_grad():\n",
    "  for data in test_loader_bi_10:\n",
    "    embeddings, labels = data\n",
    "    # calculating outputs by running embeddings through the network\n",
    "    model_bi_10.to(\"cpu\")\n",
    "    outputs = model_bi_10(embeddings.float())\n",
    "    # the class with the highest score is what we choose as prediction\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "aEYLbFli7HB1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy_MLP-model_first-10-word-vectors_my-own-word2vec-model_binary-test-set: 86 %\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy_MLP-model_first-10-word-vectors_my-own-word2vec-model_binary-test-set: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0q1cj2jG7HB1"
   },
   "source": [
    "#### Ternary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "v8VFnt2X7HB1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.649743 \tValidation Loss: 0.133979\n",
      "Validation loss decreased (inf --> 0.133979).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 0.548585 \tValidation Loss: 0.128245\n",
      "Validation loss decreased (0.133979 --> 0.128245).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 0.524940 \tValidation Loss: 0.125868\n",
      "Validation loss decreased (0.128245 --> 0.125868).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 0.509741 \tValidation Loss: 0.124714\n",
      "Validation loss decreased (0.125868 --> 0.124714).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 0.498374 \tValidation Loss: 0.124227\n",
      "Validation loss decreased (0.124714 --> 0.124227).  Saving model ...\n",
      "Epoch: 6 \tTraining Loss: 0.488815 \tValidation Loss: 0.123925\n",
      "Validation loss decreased (0.124227 --> 0.123925).  Saving model ...\n",
      "Epoch: 7 \tTraining Loss: 0.480778 \tValidation Loss: 0.123784\n",
      "Validation loss decreased (0.123925 --> 0.123784).  Saving model ...\n",
      "Epoch: 8 \tTraining Loss: 0.472478 \tValidation Loss: 0.123818\n",
      "Epoch: 9 \tTraining Loss: 0.463942 \tValidation Loss: 0.124098\n",
      "Epoch: 10 \tTraining Loss: 0.457359 \tValidation Loss: 0.124342\n",
      "Epoch: 11 \tTraining Loss: 0.450956 \tValidation Loss: 0.124681\n",
      "Epoch: 12 \tTraining Loss: 0.442268 \tValidation Loss: 0.125711\n",
      "Epoch: 13 \tTraining Loss: 0.436082 \tValidation Loss: 0.126581\n",
      "Epoch: 14 \tTraining Loss: 0.428534 \tValidation Loss: 0.126692\n",
      "Epoch: 15 \tTraining Loss: 0.423638 \tValidation Loss: 0.127320\n",
      "Epoch: 16 \tTraining Loss: 0.416257 \tValidation Loss: 0.128937\n",
      "Epoch: 17 \tTraining Loss: 0.410349 \tValidation Loss: 0.129732\n",
      "Epoch: 18 \tTraining Loss: 0.403882 \tValidation Loss: 0.131013\n",
      "Epoch: 19 \tTraining Loss: 0.399000 \tValidation Loss: 0.132097\n",
      "Epoch: 20 \tTraining Loss: 0.392913 \tValidation Loss: 0.133299\n",
      "Epoch: 21 \tTraining Loss: 0.389302 \tValidation Loss: 0.133723\n",
      "Epoch: 22 \tTraining Loss: 0.381841 \tValidation Loss: 0.134702\n",
      "Epoch: 23 \tTraining Loss: 0.377543 \tValidation Loss: 0.136137\n",
      "Epoch: 24 \tTraining Loss: 0.374192 \tValidation Loss: 0.137287\n",
      "Epoch: 25 \tTraining Loss: 0.368880 \tValidation Loss: 0.138315\n",
      "Epoch: 26 \tTraining Loss: 0.364207 \tValidation Loss: 0.140042\n",
      "Epoch: 27 \tTraining Loss: 0.360624 \tValidation Loss: 0.142466\n",
      "Epoch: 28 \tTraining Loss: 0.356126 \tValidation Loss: 0.142967\n",
      "Epoch: 29 \tTraining Loss: 0.353533 \tValidation Loss: 0.144077\n",
      "Epoch: 30 \tTraining Loss: 0.350217 \tValidation Loss: 0.144246\n",
      "Epoch: 31 \tTraining Loss: 0.344414 \tValidation Loss: 0.147102\n",
      "Epoch: 32 \tTraining Loss: 0.341689 \tValidation Loss: 0.148348\n",
      "Epoch: 33 \tTraining Loss: 0.337803 \tValidation Loss: 0.148795\n",
      "Epoch: 34 \tTraining Loss: 0.334426 \tValidation Loss: 0.151231\n",
      "Epoch: 35 \tTraining Loss: 0.329922 \tValidation Loss: 0.152318\n",
      "Epoch: 36 \tTraining Loss: 0.328174 \tValidation Loss: 0.153114\n",
      "Epoch: 37 \tTraining Loss: 0.323980 \tValidation Loss: 0.155010\n",
      "Epoch: 38 \tTraining Loss: 0.322510 \tValidation Loss: 0.155936\n",
      "Epoch: 39 \tTraining Loss: 0.319993 \tValidation Loss: 0.157482\n",
      "Epoch: 40 \tTraining Loss: 0.316305 \tValidation Loss: 0.158027\n",
      "Epoch: 41 \tTraining Loss: 0.312679 \tValidation Loss: 0.159788\n",
      "Epoch: 42 \tTraining Loss: 0.311405 \tValidation Loss: 0.160474\n",
      "Epoch: 43 \tTraining Loss: 0.307822 \tValidation Loss: 0.161101\n",
      "Epoch: 44 \tTraining Loss: 0.304613 \tValidation Loss: 0.163159\n",
      "Epoch: 45 \tTraining Loss: 0.302623 \tValidation Loss: 0.163962\n",
      "Epoch: 46 \tTraining Loss: 0.300240 \tValidation Loss: 0.165386\n",
      "Epoch: 47 \tTraining Loss: 0.298251 \tValidation Loss: 0.166649\n",
      "Epoch: 48 \tTraining Loss: 0.296482 \tValidation Loss: 0.167755\n",
      "Epoch: 49 \tTraining Loss: 0.294287 \tValidation Loss: 0.168291\n",
      "Epoch: 50 \tTraining Loss: 0.291448 \tValidation Loss: 0.170741\n",
      "Time elapsed: 365.94 s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "# number of epochs to train the model\n",
    "n_epochs = 25\n",
    "\n",
    "# initialize tracker for minimum validation loss\n",
    "valid_loss_min = np.Inf # set initial \"min\" to infinity\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "  # monitor training loss\n",
    "  train_loss = 0.0\n",
    "  valid_loss = 0.0\n",
    "\n",
    "  ###################\n",
    "  # train the model #\n",
    "  ###################\n",
    "  model_te_10.train() # prep model for training\n",
    "  for data, target in train_loader_te_10:\n",
    "    # transfer data and target to GPU\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    # clear the gradients of all optimized variables\n",
    "    optimizer_te_10.zero_grad()\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output = model_te_10(data.float())\n",
    "    # calculate the loss\n",
    "    loss = criterion(output, target)\n",
    "    # backward pass: compute gradient of the loss with respect to model parameters\n",
    "    loss.backward()\n",
    "    # perform a single optimization step (parameter update)\n",
    "    optimizer_te_10.step()\n",
    "    # update running training loss\n",
    "    train_loss += loss.item()*data.size(0)\n",
    "\n",
    "  ######################\n",
    "  # validate the model #\n",
    "  ######################\n",
    "  model_te_10.eval() # prep model for evaluation\n",
    "  for data, target in valid_loader_te_10:\n",
    "    # transfer data and target to GPU\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output = model_te_10(data.float())\n",
    "    # calculate the loss\n",
    "    loss = criterion(output, target)\n",
    "    # update running validation loss\n",
    "    valid_loss += loss.item()*data.size(0)\n",
    "    \n",
    "\n",
    "  # print training/validation statistics\n",
    "  # calculate average loss over an epoch\n",
    "  train_loss = train_loss/len(train_loader_te_10.dataset)\n",
    "  valid_loss = valid_loss/len(valid_loader_te_10.dataset)\n",
    "\n",
    "  print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch+1, train_loss, valid_loss))\n",
    "    \n",
    "  # save model if validation loss has decreased\n",
    "  if valid_loss <= valid_loss_min:\n",
    "    print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'\n",
    "        .format(valid_loss_min, valid_loss))\n",
    "    torch.save(model_te_10.state_dict(), 'model.pt')\n",
    "    valid_loss_min = valid_loss\n",
    "    \n",
    "end = time.time()\n",
    "print('Time elapsed: %.2f s' % (end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "aJtbocCp7HB1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model with the lowest validation loss\n",
    "model_te_10.load_state_dict(torch.load('model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "tNTcD0b57HB1"
   },
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for out outputs\n",
    "with torch.no_grad():\n",
    "  for data in test_loader_te_10:\n",
    "    embeddings, labels = data\n",
    "    # calculating outputs by running embeddings through the network\n",
    "    model_te_10.to(\"cpu\")\n",
    "    outputs = model_te_10(embeddings.float())\n",
    "    # the class with the highest score is what we choose as prediction\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "ju0AM6Iy7HB2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy_MLP-model_first-10-word-vectors_my-own-word2vec-model_ternary-test-set: 73 %\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy_MLP-model_first-10-word-vectors_my-own-word2vec-model_ternary-test-set: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B0Bd4v6wykHy"
   },
   "source": [
    "# 5. RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "87vK_jAHykHy"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "XzgtXqNOykHy"
   },
   "outputs": [],
   "source": [
    "X_train_bi,  X_test_bi, y_train_bi, y_test_bi = X_train, X_test, y_train, y_test\n",
    "X_train_te,  X_test_te, y_train_te, y_test_te = X_train_te, X_test_te, y_train_te, y_test_te"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KsGcz9Oe0wbD"
   },
   "source": [
    "# a. Simple RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "43v7NpFOykHz"
   },
   "source": [
    "## a.1 Use Pretrained Word2Vec as Input Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ao28_7SxykHz"
   },
   "source": [
    "### a.1.1 Compute Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aJ9i2bZpykHz"
   },
   "source": [
    "#### Use **the pretrained model** to compute word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "5gS6HwQCykHz"
   },
   "outputs": [],
   "source": [
    "# Apply the word2v_seq function to compute the word vectors of reviews (up to\n",
    "# 50 words)\n",
    "X_train_bi_rnn_series = X_train_bi.apply(lambda x: word2v_seq(x, wv_google))\n",
    "X_train_bi_rnn = np.array(X_train_bi_rnn_series.values.tolist())\n",
    "\n",
    "X_test_bi_rnn_series = X_test_bi.apply(lambda x: word2v_seq(x, wv_google))\n",
    "X_test_bi_rnn = np.array(X_test_bi_rnn_series.values.tolist())\n",
    "\n",
    "X_train_te_rnn_series = X_train_te.apply(lambda x: word2v_seq(x, wv_google))\n",
    "X_train_te_rnn = np.array(X_train_te_rnn_series.values.tolist())\n",
    "\n",
    "X_test_te_rnn_series = X_test_te.apply(lambda x: word2v_seq(x, wv_google))\n",
    "X_test_te_rnn = np.array(X_test_te_rnn_series.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "K309iJs8ykH0"
   },
   "outputs": [],
   "source": [
    "# Apply the idx_nan function to remove the NaN's and corresponding\n",
    "# labels (if any)\n",
    "idx_nan_train_bi = idx_nan(X_train_bi_rnn)\n",
    "if idx_nan_train_bi != None:\n",
    "  X_train_bi_rnn = np.delete(X_train_bi_rnn, idx_nan_train_bi, 0)\n",
    "  y_train_bi_rnn = np.delete(y_train_bi, idx_nan_train_bi)\n",
    "else:\n",
    "  y_train_bi_rnn = y_train_bi\n",
    "\n",
    "idx_nan_test_bi = idx_nan(X_test_bi_rnn)\n",
    "if idx_nan_test_bi != None:\n",
    "  X_test_bi_rnn = np.delete(X_test_bi_rnn, idx_nan_test_bi, 0)\n",
    "  y_test_bi_rnn = np.delete(y_test_bi, idx_nan_test_bi)\n",
    "else:\n",
    "  y_test_bi_rnn = y_test_bi\n",
    "\n",
    "idx_nan_train_te = idx_nan(X_train_te_rnn)\n",
    "if idx_nan_train_te != None:\n",
    "  X_train_te_rnn = np.delete(X_train_te_rnn, idx_nan_train_te, 0)\n",
    "  y_train_te_rnn = np.delete(y_train_te, idx_nan_train_te)\n",
    "else:\n",
    "  y_train_te_rnn = y_train_te\n",
    "\n",
    "idx_nan_test_te = idx_nan(X_test_te_rnn)\n",
    "if idx_nan_test_te != None:\n",
    "  X_test_te_rnn = np.delete(X_test_te_rnn, idx_nan_test_te, 0)\n",
    "  y_test_te_rnn = np.delete(y_test_te, idx_nan_test_te)\n",
    "else:\n",
    "  y_test_te_rnn = y_test_te"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rENytJTaykH0"
   },
   "source": [
    "### a.1.2 Define Dataset Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "1RMIXrN7ykH0"
   },
   "outputs": [],
   "source": [
    "class Train(Dataset):\n",
    "  def __init__(self, xtrain, ytrain):\n",
    "    'Initialization'\n",
    "    self.data = xtrain\n",
    "    self.labels = ytrain\n",
    "\n",
    "  def __len__(self):\n",
    "    'Denotes the total number of samples'\n",
    "    return len(self.data)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    'Generates one sample of data'\n",
    "    X = self.data[index]\n",
    "    y = self.labels[index]\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "z9bzvyduykH0"
   },
   "outputs": [],
   "source": [
    "class Test(Dataset):\n",
    "  def __init__(self, xtest, ytest):\n",
    "    'Initialization'\n",
    "    self.data = xtest\n",
    "    self.labels = ytest\n",
    "\n",
    "  def __len__(self):\n",
    "    'Denotes the total number of samples'\n",
    "    return len(self.data)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    'Generates one sample of data'\n",
    "    X = self.data[index]\n",
    "    y = self.labels[index]\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jlMxXxQeykH1"
   },
   "source": [
    "### a.1.3 Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "LSREvb1zykH1"
   },
   "outputs": [],
   "source": [
    "# use word vectors of word sequences as input features\n",
    "train_data_bi_rnn, test_data_bi_rnn = Train(X_train_bi_rnn, y_train_bi_rnn-1), Test(X_test_bi_rnn, y_test_bi_rnn-1)\n",
    "train_data_te_rnn, test_data_te_rnn = Train(X_train_te_rnn, y_train_te_rnn-1), Test(X_test_te_rnn, y_test_te_rnn-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7zP5ZMchykH1"
   },
   "source": [
    "#### Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "zLT7RadzykH2"
   },
   "outputs": [],
   "source": [
    "# number of subprocesses to use for data loading\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 500\n",
    "# percentage of training set to use as validation\n",
    "valid_size = 0.2\n",
    "\n",
    "# obtain training indices that will be used for validation\n",
    "num_train = len(train_data_bi_rnn)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "# define samples for obtaining training and validation batches\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "# prepare data loaders\n",
    "train_loader_bi_rnn = torch.utils.data.DataLoader(train_data_bi_rnn, batch_size=batch_size,\n",
    "                                           sampler=train_sampler, num_workers\n",
    "                                           =num_workers)\n",
    "valid_loader_bi_rnn = torch.utils.data.DataLoader(train_data_bi_rnn, batch_size=batch_size,\n",
    "                                           sampler=valid_sampler, num_workers\n",
    "                                           =num_workers)\n",
    "test_loader_bi_rnn = torch.utils.data.DataLoader(test_data_bi_rnn, batch_size=batch_size,\n",
    "                                           num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nisvqM0hykH2"
   },
   "source": [
    "#### Ternary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "hXA9X3CUykH2"
   },
   "outputs": [],
   "source": [
    "# number of subprocesses to use for data loading\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 500\n",
    "# percentage of training set to use as validation\n",
    "valid_size = 0.2\n",
    "\n",
    "# obtain training indices that will be used for validation\n",
    "num_train = len(train_data_te_rnn)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "# define samples for obtaining training and validation batches\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "# prepare data loaders\n",
    "train_loader_te_rnn = torch.utils.data.DataLoader(train_data_te_rnn, batch_size=batch_size,\n",
    "                                           sampler=train_sampler, num_workers\n",
    "                                           =num_workers)\n",
    "valid_loader_te_rnn = torch.utils.data.DataLoader(train_data_te_rnn, batch_size=batch_size,\n",
    "                                           sampler=valid_sampler, num_workers\n",
    "                                           =num_workers)\n",
    "test_loader_te_rnn = torch.utils.data.DataLoader(test_data_te_rnn, batch_size=batch_size,\n",
    "                                           num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WEFSnWewykH2"
   },
   "source": [
    "### a.1.4 Define the RNN architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "Ol7XvrIwykH2"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5ybmpoybykH2",
    "outputId": "97ff14a9-fabc-408b-98fe-99850b52e18a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNNModel(\n",
      "  (rnn): RNN(300, 50, batch_first=True)\n",
      "  (fc): Linear(in_features=50, out_features=2, bias=True)\n",
      ")\n",
      "RNNModel(\n",
      "  (rnn): RNN(300, 50, batch_first=True)\n",
      "  (fc): Linear(in_features=50, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# define the RNN architecture\n",
    "class RNNModel(nn.Module):\n",
    "  def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
    "    super().__init__()\n",
    "    \n",
    "    # Number of hidden dimensions\n",
    "    self.hidden_dim = hidden_dim\n",
    "    \n",
    "    # Number of hidden layers\n",
    "    self.layer_dim = layer_dim\n",
    "    \n",
    "    # RNN\n",
    "    self.rnn = nn.RNN(input_dim, hidden_dim, layer_dim, batch_first=True,\n",
    "                     nonlinearity='relu')\n",
    "    # Output layer\n",
    "    self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "  def forward(self, x):\n",
    "    \n",
    "    # Initialize hidden state with zeros\n",
    "    h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).to(device)\n",
    "    \n",
    "    # One time step\n",
    "    out, hn = self.rnn(x, h0)\n",
    "    out = self.fc(out[:, -1, :])\n",
    "    return out\n",
    "\n",
    "# initialize RNN\n",
    "model_bi_rnn = RNNModel(300, 50, 1, 2)\n",
    "model_te_rnn = RNNModel(300, 50, 1, 3)\n",
    "model_bi_rnn.cuda()\n",
    "model_te_rnn.cuda()\n",
    "print(model_bi_rnn)\n",
    "print(model_te_rnn)\n",
    "\n",
    "# specify loss function (categorical cross-entropy)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# specify optimizer (stochastic gradient descent) and learning rate = 0.01\n",
    "optimizer_bi_rnn = torch.optim.SGD(model_bi_rnn.parameters(), lr=0.0075)\n",
    "optimizer_te_rnn = torch.optim.SGD(model_te_rnn.parameters(), lr=0.0075)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OuOgYEigykH2"
   },
   "source": [
    "### a.1.5 Train the Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mzb-zxhMykH3"
   },
   "source": [
    "#### Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "c9UCz4U3ykH3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.554288 \tValidation Loss: 0.138367\n",
      "Validation loss decreased (inf --> 0.138367).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 0.552944 \tValidation Loss: 0.138032\n",
      "Validation loss decreased (0.138367 --> 0.138032).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 0.551711 \tValidation Loss: 0.137717\n",
      "Validation loss decreased (0.138032 --> 0.137717).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 0.550529 \tValidation Loss: 0.137406\n",
      "Validation loss decreased (0.137717 --> 0.137406).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 0.549357 \tValidation Loss: 0.137094\n",
      "Validation loss decreased (0.137406 --> 0.137094).  Saving model ...\n",
      "Epoch: 6 \tTraining Loss: 0.548185 \tValidation Loss: 0.136781\n",
      "Validation loss decreased (0.137094 --> 0.136781).  Saving model ...\n",
      "Epoch: 7 \tTraining Loss: 0.547030 \tValidation Loss: 0.136468\n",
      "Validation loss decreased (0.136781 --> 0.136468).  Saving model ...\n",
      "Epoch: 8 \tTraining Loss: 0.545856 \tValidation Loss: 0.136138\n",
      "Validation loss decreased (0.136468 --> 0.136138).  Saving model ...\n",
      "Epoch: 9 \tTraining Loss: 0.544608 \tValidation Loss: 0.135784\n",
      "Validation loss decreased (0.136138 --> 0.135784).  Saving model ...\n",
      "Epoch: 10 \tTraining Loss: 0.543240 \tValidation Loss: 0.135389\n",
      "Validation loss decreased (0.135784 --> 0.135389).  Saving model ...\n",
      "Epoch: 11 \tTraining Loss: 0.541709 \tValidation Loss: 0.134937\n",
      "Validation loss decreased (0.135389 --> 0.134937).  Saving model ...\n",
      "Epoch: 12 \tTraining Loss: 0.539839 \tValidation Loss: 0.134354\n",
      "Validation loss decreased (0.134937 --> 0.134354).  Saving model ...\n",
      "Epoch: 13 \tTraining Loss: 0.537140 \tValidation Loss: 0.133426\n",
      "Validation loss decreased (0.134354 --> 0.133426).  Saving model ...\n",
      "Epoch: 14 \tTraining Loss: 0.531258 \tValidation Loss: 0.130569\n",
      "Validation loss decreased (0.133426 --> 0.130569).  Saving model ...\n",
      "Epoch: 15 \tTraining Loss: 0.469176 \tValidation Loss: 0.117706\n",
      "Validation loss decreased (0.130569 --> 0.117706).  Saving model ...\n",
      "Epoch: 16 \tTraining Loss: 0.399873 \tValidation Loss: 0.091996\n",
      "Validation loss decreased (0.117706 --> 0.091996).  Saving model ...\n",
      "Epoch: 17 \tTraining Loss: 0.373182 \tValidation Loss: 0.087603\n",
      "Validation loss decreased (0.091996 --> 0.087603).  Saving model ...\n",
      "Epoch: 18 \tTraining Loss: 0.358492 \tValidation Loss: 0.084742\n",
      "Validation loss decreased (0.087603 --> 0.084742).  Saving model ...\n",
      "Epoch: 19 \tTraining Loss: 0.345499 \tValidation Loss: 0.081722\n",
      "Validation loss decreased (0.084742 --> 0.081722).  Saving model ...\n",
      "Epoch: 20 \tTraining Loss: 0.335376 \tValidation Loss: 0.077976\n",
      "Validation loss decreased (0.081722 --> 0.077976).  Saving model ...\n",
      "Epoch: 21 \tTraining Loss: 0.324932 \tValidation Loss: 0.076673\n",
      "Validation loss decreased (0.077976 --> 0.076673).  Saving model ...\n",
      "Epoch: 22 \tTraining Loss: 0.315817 \tValidation Loss: 0.075492\n",
      "Validation loss decreased (0.076673 --> 0.075492).  Saving model ...\n",
      "Epoch: 23 \tTraining Loss: 0.310902 \tValidation Loss: 0.072720\n",
      "Validation loss decreased (0.075492 --> 0.072720).  Saving model ...\n",
      "Epoch: 24 \tTraining Loss: 0.299948 \tValidation Loss: 0.072560\n",
      "Validation loss decreased (0.072720 --> 0.072560).  Saving model ...\n",
      "Epoch: 25 \tTraining Loss: 0.294812 \tValidation Loss: 0.069740\n",
      "Validation loss decreased (0.072560 --> 0.069740).  Saving model ...\n",
      "Epoch: 26 \tTraining Loss: 0.288879 \tValidation Loss: 0.076545\n",
      "Epoch: 27 \tTraining Loss: 0.281731 \tValidation Loss: 0.067576\n",
      "Validation loss decreased (0.069740 --> 0.067576).  Saving model ...\n",
      "Epoch: 28 \tTraining Loss: 0.278117 \tValidation Loss: 0.066785\n",
      "Validation loss decreased (0.067576 --> 0.066785).  Saving model ...\n",
      "Epoch: 29 \tTraining Loss: 0.274461 \tValidation Loss: 0.070263\n",
      "Epoch: 30 \tTraining Loss: 0.270142 \tValidation Loss: 0.065813\n",
      "Validation loss decreased (0.066785 --> 0.065813).  Saving model ...\n",
      "Epoch: 31 \tTraining Loss: 0.267100 \tValidation Loss: 0.068998\n",
      "Epoch: 32 \tTraining Loss: 0.264084 \tValidation Loss: 0.070050\n",
      "Epoch: 33 \tTraining Loss: 0.263878 \tValidation Loss: 0.064446\n",
      "Validation loss decreased (0.065813 --> 0.064446).  Saving model ...\n",
      "Epoch: 34 \tTraining Loss: 0.259392 \tValidation Loss: 0.066032\n",
      "Epoch: 35 \tTraining Loss: 0.260917 \tValidation Loss: 0.063674\n",
      "Validation loss decreased (0.064446 --> 0.063674).  Saving model ...\n",
      "Epoch: 36 \tTraining Loss: 0.257767 \tValidation Loss: 0.063914\n",
      "Epoch: 37 \tTraining Loss: 0.255655 \tValidation Loss: 0.066086\n",
      "Epoch: 38 \tTraining Loss: 0.254212 \tValidation Loss: 0.062799\n",
      "Validation loss decreased (0.063674 --> 0.062799).  Saving model ...\n",
      "Epoch: 39 \tTraining Loss: 0.253268 \tValidation Loss: 0.063775\n",
      "Epoch: 40 \tTraining Loss: 0.251878 \tValidation Loss: 0.062293\n",
      "Validation loss decreased (0.062799 --> 0.062293).  Saving model ...\n",
      "Epoch: 41 \tTraining Loss: 0.250547 \tValidation Loss: 0.062240\n",
      "Validation loss decreased (0.062293 --> 0.062240).  Saving model ...\n",
      "Epoch: 42 \tTraining Loss: 0.248008 \tValidation Loss: 0.061831\n",
      "Validation loss decreased (0.062240 --> 0.061831).  Saving model ...\n",
      "Epoch: 43 \tTraining Loss: 0.248653 \tValidation Loss: 0.061571\n",
      "Validation loss decreased (0.061831 --> 0.061571).  Saving model ...\n",
      "Epoch: 44 \tTraining Loss: 0.247463 \tValidation Loss: 0.061899\n",
      "Epoch: 45 \tTraining Loss: 0.245202 \tValidation Loss: 0.061194\n",
      "Validation loss decreased (0.061571 --> 0.061194).  Saving model ...\n",
      "Epoch: 46 \tTraining Loss: 0.245238 \tValidation Loss: 0.062058\n",
      "Epoch: 47 \tTraining Loss: 0.241955 \tValidation Loss: 0.061073\n",
      "Validation loss decreased (0.061194 --> 0.061073).  Saving model ...\n",
      "Epoch: 48 \tTraining Loss: 0.241901 \tValidation Loss: 0.060232\n",
      "Validation loss decreased (0.061073 --> 0.060232).  Saving model ...\n",
      "Epoch: 49 \tTraining Loss: 0.241725 \tValidation Loss: 0.060063\n",
      "Validation loss decreased (0.060232 --> 0.060063).  Saving model ...\n",
      "Epoch: 50 \tTraining Loss: 0.240282 \tValidation Loss: 0.059983\n",
      "Validation loss decreased (0.060063 --> 0.059983).  Saving model ...\n",
      "Time elapsed: 369.47 s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "# number of epochs to train the model\n",
    "n_epochs = 50\n",
    "\n",
    "# initialize tracker for minimum validation loss\n",
    "valid_loss_min = np.Inf # set initial \"min\" to infinity\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "  # monitor training loss\n",
    "  train_loss = 0.0\n",
    "  valid_loss = 0.0\n",
    "\n",
    "  ###################\n",
    "  # train the model #\n",
    "  ###################\n",
    "  model_bi_rnn.train() # prep model for training\n",
    "  for data, target in train_loader_bi_rnn:\n",
    "    # transfer data and target to GPU\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    # clear the gradients of all optimized variables\n",
    "    optimizer_bi_rnn.zero_grad()\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output = model_bi_rnn(data.float())\n",
    "    # calculate the loss\n",
    "    loss = criterion(output, target)\n",
    "    # backward pass: compute gradient of the loss with respect to model parameters\n",
    "    loss.backward()\n",
    "    # perform a single optimization step (parameter update)\n",
    "    optimizer_bi_rnn.step()\n",
    "    # update running training loss\n",
    "    train_loss += loss.item()*data.size(0)\n",
    "\n",
    "  ######################\n",
    "  # validate the model #\n",
    "  ######################\n",
    "  model_bi_rnn.eval() # prep model for evaluation\n",
    "  for data, target in valid_loader_bi_rnn:\n",
    "    # transfer data and target to GPU\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output = model_bi_rnn(data.float())\n",
    "    # calculate the loss\n",
    "    loss = criterion(output, target)\n",
    "    # update running validation loss\n",
    "    valid_loss += loss.item()*data.size(0)\n",
    "    \n",
    "\n",
    "  # print training/validation statistics\n",
    "  # calculate average loss over an epoch\n",
    "  train_loss = train_loss/len(train_loader_bi_rnn.dataset)\n",
    "  valid_loss = valid_loss/len(valid_loader_bi_rnn.dataset)\n",
    "\n",
    "  print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch+1, train_loss, valid_loss))\n",
    "    \n",
    "  # save model if validation loss has decreased\n",
    "  if valid_loss <= valid_loss_min:\n",
    "    print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'\n",
    "        .format(valid_loss_min, valid_loss))\n",
    "    torch.save(model_bi_rnn.state_dict(), 'model.pt')\n",
    "    valid_loss_min = valid_loss\n",
    "\n",
    "end = time.time()\n",
    "print('Time elapsed: %.2f s' % (end - start))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "fUAU0da5ykH3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model with the lowest validation loss\n",
    "model_bi_rnn.load_state_dict(torch.load('model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "7YIA136gykH3"
   },
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for out outputs\n",
    "with torch.no_grad():\n",
    "  for data in test_loader_bi_rnn:\n",
    "    embeddings, labels = data\n",
    "    # transfer data and target to GPU\n",
    "    embeddings, labels = embeddings.to(device), labels.to(device)\n",
    "    # calculating outputs by running embeddings through the network\n",
    "    model_bi_rnn.to(device)\n",
    "    outputs = model_bi_rnn(embeddings.float())\n",
    "    # the class with the highest score is what we choose as prediction\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "SfL7nF4CykH3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy_simple-RNN-model_pretrained-word2vec_binary-test-set: 87 %\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy_simple-RNN-model_pretrained-word2vec_binary-test-set: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "00YbUsccykH3"
   },
   "source": [
    "#### Ternary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U1iDi49hykH3",
    "outputId": "5abff7fa-3e40-45f7-9fc3-54b49c665e9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.863061 \tValidation Loss: 0.212058\n",
      "Validation loss decreased (inf --> 0.212058).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 0.845899 \tValidation Loss: 0.210832\n",
      "Validation loss decreased (0.212058 --> 0.210832).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 0.843662 \tValidation Loss: 0.210509\n",
      "Validation loss decreased (0.210832 --> 0.210509).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 0.842674 \tValidation Loss: 0.210271\n",
      "Validation loss decreased (0.210509 --> 0.210271).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 0.841810 \tValidation Loss: 0.210067\n",
      "Validation loss decreased (0.210271 --> 0.210067).  Saving model ...\n",
      "Epoch: 6 \tTraining Loss: 0.840992 \tValidation Loss: 0.209873\n",
      "Validation loss decreased (0.210067 --> 0.209873).  Saving model ...\n",
      "Epoch: 7 \tTraining Loss: 0.840235 \tValidation Loss: 0.209680\n",
      "Validation loss decreased (0.209873 --> 0.209680).  Saving model ...\n",
      "Epoch: 8 \tTraining Loss: 0.839475 \tValidation Loss: 0.209493\n",
      "Validation loss decreased (0.209680 --> 0.209493).  Saving model ...\n",
      "Epoch: 9 \tTraining Loss: 0.838715 \tValidation Loss: 0.209301\n",
      "Validation loss decreased (0.209493 --> 0.209301).  Saving model ...\n",
      "Epoch: 10 \tTraining Loss: 0.837961 \tValidation Loss: 0.209120\n",
      "Validation loss decreased (0.209301 --> 0.209120).  Saving model ...\n",
      "Epoch: 11 \tTraining Loss: 0.837214 \tValidation Loss: 0.208934\n",
      "Validation loss decreased (0.209120 --> 0.208934).  Saving model ...\n",
      "Epoch: 12 \tTraining Loss: 0.836461 \tValidation Loss: 0.208745\n",
      "Validation loss decreased (0.208934 --> 0.208745).  Saving model ...\n",
      "Epoch: 13 \tTraining Loss: 0.835702 \tValidation Loss: 0.208548\n",
      "Validation loss decreased (0.208745 --> 0.208548).  Saving model ...\n",
      "Epoch: 14 \tTraining Loss: 0.834911 \tValidation Loss: 0.208352\n",
      "Validation loss decreased (0.208548 --> 0.208352).  Saving model ...\n",
      "Epoch: 15 \tTraining Loss: 0.834052 \tValidation Loss: 0.208132\n",
      "Validation loss decreased (0.208352 --> 0.208132).  Saving model ...\n",
      "Epoch: 16 \tTraining Loss: 0.833073 \tValidation Loss: 0.207876\n",
      "Validation loss decreased (0.208132 --> 0.207876).  Saving model ...\n",
      "Epoch: 17 \tTraining Loss: 0.831864 \tValidation Loss: 0.207532\n",
      "Validation loss decreased (0.207876 --> 0.207532).  Saving model ...\n",
      "Epoch: 18 \tTraining Loss: 0.830199 \tValidation Loss: 0.207032\n",
      "Validation loss decreased (0.207532 --> 0.207032).  Saving model ...\n",
      "Epoch: 19 \tTraining Loss: 0.827229 \tValidation Loss: 0.205925\n",
      "Validation loss decreased (0.207032 --> 0.205925).  Saving model ...\n",
      "Epoch: 20 \tTraining Loss: 0.808592 \tValidation Loss: 0.187068\n",
      "Validation loss decreased (0.205925 --> 0.187068).  Saving model ...\n",
      "Epoch: 21 \tTraining Loss: 0.726040 \tValidation Loss: 0.174334\n",
      "Validation loss decreased (0.187068 --> 0.174334).  Saving model ...\n",
      "Epoch: 22 \tTraining Loss: 0.693804 \tValidation Loss: 0.176794\n",
      "Epoch: 23 \tTraining Loss: 0.677149 \tValidation Loss: 0.164269\n",
      "Validation loss decreased (0.174334 --> 0.164269).  Saving model ...\n",
      "Epoch: 24 \tTraining Loss: 0.663868 \tValidation Loss: 0.170751\n",
      "Epoch: 25 \tTraining Loss: 0.654383 \tValidation Loss: 0.166081\n",
      "Epoch: 26 \tTraining Loss: 0.645598 \tValidation Loss: 0.157324\n",
      "Validation loss decreased (0.164269 --> 0.157324).  Saving model ...\n",
      "Epoch: 27 \tTraining Loss: 0.636282 \tValidation Loss: 0.166180\n",
      "Epoch: 28 \tTraining Loss: 0.631313 \tValidation Loss: 0.152544\n",
      "Validation loss decreased (0.157324 --> 0.152544).  Saving model ...\n",
      "Epoch: 29 \tTraining Loss: 0.624893 \tValidation Loss: 0.153586\n",
      "Epoch: 30 \tTraining Loss: 0.615928 \tValidation Loss: 0.154271\n",
      "Epoch: 31 \tTraining Loss: 0.610185 \tValidation Loss: 0.150213\n",
      "Validation loss decreased (0.152544 --> 0.150213).  Saving model ...\n",
      "Epoch: 32 \tTraining Loss: 0.604258 \tValidation Loss: 0.149201\n",
      "Validation loss decreased (0.150213 --> 0.149201).  Saving model ...\n",
      "Epoch: 33 \tTraining Loss: 0.594561 \tValidation Loss: 0.146171\n",
      "Validation loss decreased (0.149201 --> 0.146171).  Saving model ...\n",
      "Epoch: 34 \tTraining Loss: 0.592030 \tValidation Loss: 0.152136\n",
      "Epoch: 35 \tTraining Loss: 0.586022 \tValidation Loss: 0.143984\n",
      "Validation loss decreased (0.146171 --> 0.143984).  Saving model ...\n",
      "Epoch: 36 \tTraining Loss: 0.581496 \tValidation Loss: 0.143506\n",
      "Validation loss decreased (0.143984 --> 0.143506).  Saving model ...\n",
      "Epoch: 37 \tTraining Loss: 0.577481 \tValidation Loss: 0.145704\n",
      "Epoch: 38 \tTraining Loss: 0.575917 \tValidation Loss: 0.141625\n",
      "Validation loss decreased (0.143506 --> 0.141625).  Saving model ...\n",
      "Epoch: 39 \tTraining Loss: 0.570474 \tValidation Loss: 0.140458\n",
      "Validation loss decreased (0.141625 --> 0.140458).  Saving model ...\n",
      "Epoch: 40 \tTraining Loss: 0.566213 \tValidation Loss: 0.139518\n",
      "Validation loss decreased (0.140458 --> 0.139518).  Saving model ...\n",
      "Epoch: 41 \tTraining Loss: 0.564710 \tValidation Loss: 0.139863\n",
      "Epoch: 42 \tTraining Loss: 0.562288 \tValidation Loss: 0.142088\n",
      "Epoch: 43 \tTraining Loss: 0.557945 \tValidation Loss: 0.137626\n",
      "Validation loss decreased (0.139518 --> 0.137626).  Saving model ...\n",
      "Epoch: 44 \tTraining Loss: 0.557210 \tValidation Loss: 0.138358\n",
      "Epoch: 45 \tTraining Loss: 0.552666 \tValidation Loss: 0.144056\n",
      "Epoch: 46 \tTraining Loss: 0.552425 \tValidation Loss: 0.135896\n",
      "Validation loss decreased (0.137626 --> 0.135896).  Saving model ...\n",
      "Epoch: 47 \tTraining Loss: 0.549639 \tValidation Loss: 0.140540\n",
      "Epoch: 48 \tTraining Loss: 0.548957 \tValidation Loss: 0.136038\n",
      "Epoch: 49 \tTraining Loss: 0.547070 \tValidation Loss: 0.137606\n",
      "Epoch: 50 \tTraining Loss: 0.542609 \tValidation Loss: 0.142621\n",
      "Time elapsed: 644.10 s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "# number of epochs to train the model\n",
    "n_epochs = 50\n",
    "\n",
    "# initialize tracker for minimum validation loss\n",
    "valid_loss_min = np.Inf # set initial \"min\" to infinity\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "  # monitor training loss\n",
    "  train_loss = 0.0\n",
    "  valid_loss = 0.0\n",
    "\n",
    "  ###################\n",
    "  # train the model #\n",
    "  ###################\n",
    "  model_te_rnn.train() # prep model for training\n",
    "  for data, target in train_loader_te_rnn:\n",
    "    # transfer data and target to GPU\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    # clear the gradients of all optimized variables\n",
    "    optimizer_te_rnn.zero_grad()\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output = model_te_rnn(data.float())\n",
    "    # calculate the loss\n",
    "    loss = criterion(output, target)\n",
    "    # backward pass: compute gradient of the loss with respect to model parameters\n",
    "    loss.backward()\n",
    "    # perform a single optimization step (parameter update)\n",
    "    optimizer_te_rnn.step()\n",
    "    # update running training loss\n",
    "    train_loss += loss.item()*data.size(0)\n",
    "\n",
    "  ######################\n",
    "  # validate the model #\n",
    "  ######################\n",
    "  model_te_rnn.eval() # prep model for evaluation\n",
    "  for data, target in valid_loader_te_rnn:\n",
    "    # transfer data and target to GPU\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output = model_te_rnn(data.float())\n",
    "    # calculate the loss\n",
    "    loss = criterion(output, target)\n",
    "    # update running validation loss\n",
    "    valid_loss += loss.item()*data.size(0)\n",
    "    \n",
    "\n",
    "  # print training/validation statistics\n",
    "  # calculate average loss over an epoch\n",
    "  train_loss = train_loss/len(train_loader_te_rnn.dataset)\n",
    "  valid_loss = valid_loss/len(valid_loader_te_rnn.dataset)\n",
    "\n",
    "  print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch+1, train_loss, valid_loss))\n",
    "    \n",
    "  # save model if validation loss has decreased\n",
    "  if valid_loss <= valid_loss_min:\n",
    "    print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'\n",
    "        .format(valid_loss_min, valid_loss))\n",
    "    torch.save(model_te_rnn.state_dict(), 'model.pt')\n",
    "    valid_loss_min = valid_loss\n",
    "\n",
    "end = time.time()\n",
    "print('Time elapsed: %.2f s' % (end - start))\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c-hEy3YwykH4",
    "outputId": "35631f7a-f832-410c-8d74-5800eaea7d2f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model with the lowest validation loss\n",
    "model_te_rnn.load_state_dict(torch.load('model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "ISYVYl-9ykH4"
   },
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for out outputs\n",
    "with torch.no_grad():\n",
    "  for data in test_loader_te_rnn:\n",
    "    embeddings, labels = data\n",
    "    # transfer data and target to GPU\n",
    "    embeddings, labels = embeddings.to(device), labels.to(device)\n",
    "    # calculating outputs by running embeddings through the network\n",
    "    model_te_rnn.to(device)\n",
    "    outputs = model_te_rnn(embeddings.float())\n",
    "    # the class with the highest score is what we choose as prediction\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UFyewYVjykH4",
    "outputId": "7df998f4-a103-4478-8886-0bab97a3c08e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy_simple-RNN-model_pretrained-word2vec_ternary-test-set: 70 %\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy_simple-RNN-model_pretrained-word2vec_ternary-test-set: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ElMosPhq2x30"
   },
   "source": [
    "## a.2 Use My Own Word2Vec as Input Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YnIuPznE0Jvw"
   },
   "source": [
    "### a.2.1 Compute Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "itgK_N7l0Jv0"
   },
   "source": [
    "#### Use **my own model** to compute word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "9UpoksT50Jv1"
   },
   "outputs": [],
   "source": [
    "# Apply the word2v_seq function to compute the word vectors of reviews (up to\n",
    "# 50 words)\n",
    "X_train_bi_rnn_series = X_train_bi.apply(lambda x: word2v_seq(x, model.wv))\n",
    "X_train_bi_rnn = np.array(X_train_bi_rnn_series.values.tolist())\n",
    "\n",
    "X_test_bi_rnn_series = X_test_bi.apply(lambda x: word2v_seq(x, model.wv))\n",
    "X_test_bi_rnn = np.array(X_test_bi_rnn_series.values.tolist())\n",
    "\n",
    "X_train_te_rnn_series = X_train_te.apply(lambda x: word2v_seq(x, model.wv))\n",
    "X_train_te_rnn = np.array(X_train_te_rnn_series.values.tolist())\n",
    "\n",
    "X_test_te_rnn_series = X_test_te.apply(lambda x: word2v_seq(x, model.wv))\n",
    "X_test_te_rnn = np.array(X_test_te_rnn_series.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "TW2k5rg30Jv3"
   },
   "outputs": [],
   "source": [
    "# Apply the idx_nan function to remove the NaN's and corresponding\n",
    "# labels (if any)\n",
    "idx_nan_train_bi = idx_nan(X_train_bi_rnn)\n",
    "if idx_nan_train_bi != None:\n",
    "  X_train_bi_rnn = np.delete(X_train_bi_rnn, idx_nan_train_bi, 0)\n",
    "  y_train_bi_rnn = np.delete(y_train_bi, idx_nan_train_bi)\n",
    "else:\n",
    "  y_train_bi_rnn = y_train_bi\n",
    "\n",
    "idx_nan_test_bi = idx_nan(X_test_bi_rnn)\n",
    "if idx_nan_test_bi != None:\n",
    "  X_test_bi_rnn = np.delete(X_test_bi_rnn, idx_nan_test_bi, 0)\n",
    "  y_test_bi_rnn = np.delete(y_test_bi, idx_nan_test_bi)\n",
    "else:\n",
    "  y_test_bi_rnn = y_test_bi\n",
    "\n",
    "idx_nan_train_te = idx_nan(X_train_te_rnn)\n",
    "if idx_nan_train_te != None:\n",
    "  X_train_te_rnn = np.delete(X_train_te_rnn, idx_nan_train_te, 0)\n",
    "  y_train_te_rnn = np.delete(y_train_te, idx_nan_train_te)\n",
    "else:\n",
    "  y_train_te_rnn = y_train_te\n",
    "\n",
    "idx_nan_test_te = idx_nan(X_test_te_rnn)\n",
    "if idx_nan_test_te != None:\n",
    "  X_test_te_rnn = np.delete(X_test_te_rnn, idx_nan_test_te, 0)\n",
    "  y_test_te_rnn = np.delete(y_test_te, idx_nan_test_te)\n",
    "else:\n",
    "  y_test_te_rnn = y_test_te"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F8e5MB7t0Jv4"
   },
   "source": [
    "### a.2.2 Define Dataset Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "KrvRAm0h0Jv4"
   },
   "outputs": [],
   "source": [
    "class Train(Dataset):\n",
    "  def __init__(self, xtrain, ytrain):\n",
    "    'Initialization'\n",
    "    self.data = xtrain\n",
    "    self.labels = ytrain\n",
    "\n",
    "  def __len__(self):\n",
    "    'Denotes the total number of samples'\n",
    "    return len(self.data)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    'Generates one sample of data'\n",
    "    X = self.data[index]\n",
    "    y = self.labels[index]\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "BvM1s8c_0Jv5"
   },
   "outputs": [],
   "source": [
    "class Test(Dataset):\n",
    "  def __init__(self, xtest, ytest):\n",
    "    'Initialization'\n",
    "    self.data = xtest\n",
    "    self.labels = ytest\n",
    "\n",
    "  def __len__(self):\n",
    "    'Denotes the total number of samples'\n",
    "    return len(self.data)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    'Generates one sample of data'\n",
    "    X = self.data[index]\n",
    "    y = self.labels[index]\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sAUCW5mY0Jv5"
   },
   "source": [
    "### a.2.3 Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "YlfkOF6s0Jv6"
   },
   "outputs": [],
   "source": [
    "# use word vectors of word sequences as input features\n",
    "train_data_bi_rnn, test_data_bi_rnn = Train(X_train_bi_rnn, y_train_bi_rnn-1), Test(X_test_bi_rnn, y_test_bi_rnn-1)\n",
    "train_data_te_rnn, test_data_te_rnn = Train(X_train_te_rnn, y_train_te_rnn-1), Test(X_test_te_rnn, y_test_te_rnn-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tmXmNvMK0Jv7"
   },
   "source": [
    "#### Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "G71-9QyZ0Jv7"
   },
   "outputs": [],
   "source": [
    "# number of subprocesses to use for data loading\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 500\n",
    "# percentage of training set to use as validation\n",
    "valid_size = 0.2\n",
    "\n",
    "# obtain training indices that will be used for validation\n",
    "num_train = len(train_data_bi_rnn)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "# define samples for obtaining training and validation batches\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "# prepare data loaders\n",
    "train_loader_bi_rnn = torch.utils.data.DataLoader(train_data_bi_rnn, batch_size=batch_size,\n",
    "                                           sampler=train_sampler, num_workers\n",
    "                                           =num_workers)\n",
    "valid_loader_bi_rnn = torch.utils.data.DataLoader(train_data_bi_rnn, batch_size=batch_size,\n",
    "                                           sampler=valid_sampler, num_workers\n",
    "                                           =num_workers)\n",
    "test_loader_bi_rnn = torch.utils.data.DataLoader(test_data_bi_rnn, batch_size=batch_size,\n",
    "                                           num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dI6SaaDm0Jv8"
   },
   "source": [
    "#### Ternary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "oA5DXJBv0Jv8"
   },
   "outputs": [],
   "source": [
    "# number of subprocesses to use for data loading\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 500\n",
    "# percentage of training set to use as validation\n",
    "valid_size = 0.2\n",
    "\n",
    "# obtain training indices that will be used for validation\n",
    "num_train = len(train_data_te_rnn)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "# define samples for obtaining training and validation batches\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "# prepare data loaders\n",
    "train_loader_te_rnn = torch.utils.data.DataLoader(train_data_te_rnn, batch_size=batch_size,\n",
    "                                           sampler=train_sampler, num_workers\n",
    "                                           =num_workers)\n",
    "valid_loader_te_rnn = torch.utils.data.DataLoader(train_data_te_rnn, batch_size=batch_size,\n",
    "                                           sampler=valid_sampler, num_workers\n",
    "                                           =num_workers)\n",
    "test_loader_te_rnn = torch.utils.data.DataLoader(test_data_te_rnn, batch_size=batch_size,\n",
    "                                           num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Duqzfc_B0Jv9"
   },
   "source": [
    "### a.2.4 Define the RNN architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "GCmLYPma0Jv9"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sr4C9dsx0Jv9",
    "outputId": "97ff14a9-fabc-408b-98fe-99850b52e18a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNNModel(\n",
      "  (rnn): RNN(300, 50, batch_first=True)\n",
      "  (fc): Linear(in_features=50, out_features=2, bias=True)\n",
      ")\n",
      "RNNModel(\n",
      "  (rnn): RNN(300, 50, batch_first=True)\n",
      "  (fc): Linear(in_features=50, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# define the RNN architecture\n",
    "class RNNModel(nn.Module):\n",
    "  def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
    "    super().__init__()\n",
    "    \n",
    "    # Number of hidden dimensions\n",
    "    self.hidden_dim = hidden_dim\n",
    "    \n",
    "    # Number of hidden layers\n",
    "    self.layer_dim = layer_dim\n",
    "    \n",
    "    # RNN\n",
    "    self.rnn = nn.RNN(input_dim, hidden_dim, layer_dim, batch_first=True,\n",
    "                     nonlinearity='relu')\n",
    "    # Output layer\n",
    "    self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "  def forward(self, x):\n",
    "    \n",
    "    # Initialize hidden state with zeros\n",
    "    h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).to(device)\n",
    "    \n",
    "    # One time step\n",
    "    out, hn = self.rnn(x, h0)\n",
    "    out = self.fc(out[:, -1, :])\n",
    "    return out\n",
    "\n",
    "# initialize RNN\n",
    "model_bi_rnn = RNNModel(300, 50, 1, 2)\n",
    "model_te_rnn = RNNModel(300, 50, 1, 3)\n",
    "model_bi_rnn.cuda()\n",
    "model_te_rnn.cuda()\n",
    "print(model_bi_rnn)\n",
    "print(model_te_rnn)\n",
    "\n",
    "# specify loss function (categorical cross-entropy)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# specify optimizer (stochastic gradient descent) and learning rate = 0.01\n",
    "optimizer_bi_rnn = torch.optim.SGD(model_bi_rnn.parameters(), lr=0.0075)\n",
    "optimizer_te_rnn = torch.optim.SGD(model_te_rnn.parameters(), lr=0.0075)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l_fvox0o0Jv-"
   },
   "source": [
    "### a.2.5 Train the Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7j56CF3d0Jv-"
   },
   "source": [
    "#### Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "P1abfmZu0Jv-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.567245 \tValidation Loss: 0.137885\n",
      "Validation loss decreased (inf --> 0.137885).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 0.549039 \tValidation Loss: 0.136514\n",
      "Validation loss decreased (0.137885 --> 0.136514).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 0.545568 \tValidation Loss: 0.135932\n",
      "Validation loss decreased (0.136514 --> 0.135932).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 0.543697 \tValidation Loss: 0.135564\n",
      "Validation loss decreased (0.135932 --> 0.135564).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 0.542440 \tValidation Loss: 0.135295\n",
      "Validation loss decreased (0.135564 --> 0.135295).  Saving model ...\n",
      "Epoch: 6 \tTraining Loss: 0.541367 \tValidation Loss: 0.135055\n",
      "Validation loss decreased (0.135295 --> 0.135055).  Saving model ...\n",
      "Epoch: 7 \tTraining Loss: 0.540358 \tValidation Loss: 0.134801\n",
      "Validation loss decreased (0.135055 --> 0.134801).  Saving model ...\n",
      "Epoch: 8 \tTraining Loss: 0.539379 \tValidation Loss: 0.134616\n",
      "Validation loss decreased (0.134801 --> 0.134616).  Saving model ...\n",
      "Epoch: 9 \tTraining Loss: 0.538373 \tValidation Loss: 0.134342\n",
      "Validation loss decreased (0.134616 --> 0.134342).  Saving model ...\n",
      "Epoch: 10 \tTraining Loss: 0.537336 \tValidation Loss: 0.134043\n",
      "Validation loss decreased (0.134342 --> 0.134043).  Saving model ...\n",
      "Epoch: 11 \tTraining Loss: 0.536159 \tValidation Loss: 0.133753\n",
      "Validation loss decreased (0.134043 --> 0.133753).  Saving model ...\n",
      "Epoch: 12 \tTraining Loss: 0.534882 \tValidation Loss: 0.133432\n",
      "Validation loss decreased (0.133753 --> 0.133432).  Saving model ...\n",
      "Epoch: 13 \tTraining Loss: 0.533371 \tValidation Loss: 0.133031\n",
      "Validation loss decreased (0.133432 --> 0.133031).  Saving model ...\n",
      "Epoch: 14 \tTraining Loss: 0.531451 \tValidation Loss: 0.132510\n",
      "Validation loss decreased (0.133031 --> 0.132510).  Saving model ...\n",
      "Epoch: 15 \tTraining Loss: 0.528780 \tValidation Loss: 0.131739\n",
      "Validation loss decreased (0.132510 --> 0.131739).  Saving model ...\n",
      "Epoch: 16 \tTraining Loss: 0.524111 \tValidation Loss: 0.129969\n",
      "Validation loss decreased (0.131739 --> 0.129969).  Saving model ...\n",
      "Epoch: 17 \tTraining Loss: 0.501936 \tValidation Loss: 0.115725\n",
      "Validation loss decreased (0.129969 --> 0.115725).  Saving model ...\n",
      "Epoch: 18 \tTraining Loss: 0.432265 \tValidation Loss: 0.102739\n",
      "Validation loss decreased (0.115725 --> 0.102739).  Saving model ...\n",
      "Epoch: 19 \tTraining Loss: 0.366786 \tValidation Loss: 0.080925\n",
      "Validation loss decreased (0.102739 --> 0.080925).  Saving model ...\n",
      "Epoch: 20 \tTraining Loss: 0.315425 \tValidation Loss: 0.073194\n",
      "Validation loss decreased (0.080925 --> 0.073194).  Saving model ...\n",
      "Epoch: 21 \tTraining Loss: 0.291085 \tValidation Loss: 0.068641\n",
      "Validation loss decreased (0.073194 --> 0.068641).  Saving model ...\n",
      "Epoch: 22 \tTraining Loss: 0.276183 \tValidation Loss: 0.065785\n",
      "Validation loss decreased (0.068641 --> 0.065785).  Saving model ...\n",
      "Epoch: 23 \tTraining Loss: 0.265732 \tValidation Loss: 0.064229\n",
      "Validation loss decreased (0.065785 --> 0.064229).  Saving model ...\n",
      "Epoch: 24 \tTraining Loss: 0.258703 \tValidation Loss: 0.062726\n",
      "Validation loss decreased (0.064229 --> 0.062726).  Saving model ...\n",
      "Epoch: 25 \tTraining Loss: 0.252884 \tValidation Loss: 0.061556\n",
      "Validation loss decreased (0.062726 --> 0.061556).  Saving model ...\n",
      "Epoch: 26 \tTraining Loss: 0.248437 \tValidation Loss: 0.060614\n",
      "Validation loss decreased (0.061556 --> 0.060614).  Saving model ...\n",
      "Epoch: 27 \tTraining Loss: 0.245082 \tValidation Loss: 0.060140\n",
      "Validation loss decreased (0.060614 --> 0.060140).  Saving model ...\n",
      "Epoch: 28 \tTraining Loss: 0.241716 \tValidation Loss: 0.059900\n",
      "Validation loss decreased (0.060140 --> 0.059900).  Saving model ...\n",
      "Epoch: 29 \tTraining Loss: 0.239390 \tValidation Loss: 0.059039\n",
      "Validation loss decreased (0.059900 --> 0.059039).  Saving model ...\n",
      "Epoch: 30 \tTraining Loss: 0.237538 \tValidation Loss: 0.059165\n",
      "Epoch: 31 \tTraining Loss: 0.235072 \tValidation Loss: 0.058144\n",
      "Validation loss decreased (0.059039 --> 0.058144).  Saving model ...\n",
      "Epoch: 32 \tTraining Loss: 0.233375 \tValidation Loss: 0.057758\n",
      "Validation loss decreased (0.058144 --> 0.057758).  Saving model ...\n",
      "Epoch: 33 \tTraining Loss: 0.231716 \tValidation Loss: 0.057367\n",
      "Validation loss decreased (0.057758 --> 0.057367).  Saving model ...\n",
      "Epoch: 34 \tTraining Loss: 0.230402 \tValidation Loss: 0.057263\n",
      "Validation loss decreased (0.057367 --> 0.057263).  Saving model ...\n",
      "Epoch: 35 \tTraining Loss: 0.228871 \tValidation Loss: 0.056737\n",
      "Validation loss decreased (0.057263 --> 0.056737).  Saving model ...\n",
      "Epoch: 36 \tTraining Loss: 0.227434 \tValidation Loss: 0.056726\n",
      "Validation loss decreased (0.056737 --> 0.056726).  Saving model ...\n",
      "Epoch: 37 \tTraining Loss: 0.226046 \tValidation Loss: 0.056538\n",
      "Validation loss decreased (0.056726 --> 0.056538).  Saving model ...\n",
      "Epoch: 38 \tTraining Loss: 0.224861 \tValidation Loss: 0.056928\n",
      "Epoch: 39 \tTraining Loss: 0.224003 \tValidation Loss: 0.055924\n",
      "Validation loss decreased (0.056538 --> 0.055924).  Saving model ...\n",
      "Epoch: 40 \tTraining Loss: 0.222639 \tValidation Loss: 0.056510\n",
      "Epoch: 41 \tTraining Loss: 0.221844 \tValidation Loss: 0.056191\n",
      "Epoch: 42 \tTraining Loss: 0.220662 \tValidation Loss: 0.055616\n",
      "Validation loss decreased (0.055924 --> 0.055616).  Saving model ...\n",
      "Epoch: 43 \tTraining Loss: 0.219970 \tValidation Loss: 0.055581\n",
      "Validation loss decreased (0.055616 --> 0.055581).  Saving model ...\n",
      "Epoch: 44 \tTraining Loss: 0.219224 \tValidation Loss: 0.055036\n",
      "Validation loss decreased (0.055581 --> 0.055036).  Saving model ...\n",
      "Epoch: 45 \tTraining Loss: 0.218283 \tValidation Loss: 0.054915\n",
      "Validation loss decreased (0.055036 --> 0.054915).  Saving model ...\n",
      "Epoch: 46 \tTraining Loss: 0.217239 \tValidation Loss: 0.054873\n",
      "Validation loss decreased (0.054915 --> 0.054873).  Saving model ...\n",
      "Epoch: 47 \tTraining Loss: 0.216686 \tValidation Loss: 0.054661\n",
      "Validation loss decreased (0.054873 --> 0.054661).  Saving model ...\n",
      "Epoch: 48 \tTraining Loss: 0.215800 \tValidation Loss: 0.054627\n",
      "Validation loss decreased (0.054661 --> 0.054627).  Saving model ...\n",
      "Epoch: 49 \tTraining Loss: 0.215198 \tValidation Loss: 0.054411\n",
      "Validation loss decreased (0.054627 --> 0.054411).  Saving model ...\n",
      "Epoch: 50 \tTraining Loss: 0.214361 \tValidation Loss: 0.054265\n",
      "Validation loss decreased (0.054411 --> 0.054265).  Saving model ...\n",
      "Time elapsed: 356.14 s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "# number of epochs to train the model\n",
    "n_epochs = 50\n",
    "\n",
    "# initialize tracker for minimum validation loss\n",
    "valid_loss_min = np.Inf # set initial \"min\" to infinity\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "  # monitor training loss\n",
    "  train_loss = 0.0\n",
    "  valid_loss = 0.0\n",
    "\n",
    "  ###################\n",
    "  # train the model #\n",
    "  ###################\n",
    "  model_bi_rnn.train() # prep model for training\n",
    "  for data, target in train_loader_bi_rnn:\n",
    "    # transfer data and target to GPU\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    # clear the gradients of all optimized variables\n",
    "    optimizer_bi_rnn.zero_grad()\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output = model_bi_rnn(data.float())\n",
    "    # calculate the loss\n",
    "    loss = criterion(output, target)\n",
    "    # backward pass: compute gradient of the loss with respect to model parameters\n",
    "    loss.backward()\n",
    "    # perform a single optimization step (parameter update)\n",
    "    optimizer_bi_rnn.step()\n",
    "    # update running training loss\n",
    "    train_loss += loss.item()*data.size(0)\n",
    "\n",
    "  ######################\n",
    "  # validate the model #\n",
    "  ######################\n",
    "  model_bi_rnn.eval() # prep model for evaluation\n",
    "  for data, target in valid_loader_bi_rnn:\n",
    "    # transfer data and target to GPU\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output = model_bi_rnn(data.float())\n",
    "    # calculate the loss\n",
    "    loss = criterion(output, target)\n",
    "    # update running validation loss\n",
    "    valid_loss += loss.item()*data.size(0)\n",
    "    \n",
    "\n",
    "  # print training/validation statistics\n",
    "  # calculate average loss over an epoch\n",
    "  train_loss = train_loss/len(train_loader_bi_rnn.dataset)\n",
    "  valid_loss = valid_loss/len(valid_loader_bi_rnn.dataset)\n",
    "\n",
    "  print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch+1, train_loss, valid_loss))\n",
    "    \n",
    "  # save model if validation loss has decreased\n",
    "  if valid_loss <= valid_loss_min:\n",
    "    print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'\n",
    "        .format(valid_loss_min, valid_loss))\n",
    "    torch.save(model_bi_rnn.state_dict(), 'model.pt')\n",
    "    valid_loss_min = valid_loss\n",
    "\n",
    "end = time.time()\n",
    "print('Time elapsed: %.2f s' % (end - start))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "iW_4TVob0JwA"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model with the lowest validation loss\n",
    "model_bi_rnn.load_state_dict(torch.load('model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "uY_vf2eJ0JwB"
   },
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for out outputs\n",
    "with torch.no_grad():\n",
    "  for data in test_loader_bi_rnn:\n",
    "    embeddings, labels = data\n",
    "    # transfer data and target to GPU\n",
    "    embeddings, labels = embeddings.to(device), labels.to(device)\n",
    "    # calculating outputs by running embeddings through the network\n",
    "    model_bi_rnn.to(device)\n",
    "    outputs = model_bi_rnn(embeddings.float())\n",
    "    # the class with the highest score is what we choose as prediction\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "Qz_zc3rB0JwB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy_simple-RNN-model_my-own-word2vec_binary-test-set: 88 %\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy_simple-RNN-model_my-own-word2vec_binary-test-set: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k7F059k10JwB"
   },
   "source": [
    "#### Ternary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S0Gse3r60JwB",
    "outputId": "5abff7fa-3e40-45f7-9fc3-54b49c665e9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.881824 \tValidation Loss: 0.214326\n",
      "Validation loss decreased (inf --> 0.214326).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 0.851143 \tValidation Loss: 0.211236\n",
      "Validation loss decreased (0.214326 --> 0.211236).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 0.842753 \tValidation Loss: 0.209822\n",
      "Validation loss decreased (0.211236 --> 0.209822).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 0.838469 \tValidation Loss: 0.209077\n",
      "Validation loss decreased (0.209822 --> 0.209077).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 0.835683 \tValidation Loss: 0.208474\n",
      "Validation loss decreased (0.209077 --> 0.208474).  Saving model ...\n",
      "Epoch: 6 \tTraining Loss: 0.833569 \tValidation Loss: 0.207950\n",
      "Validation loss decreased (0.208474 --> 0.207950).  Saving model ...\n",
      "Epoch: 7 \tTraining Loss: 0.831697 \tValidation Loss: 0.207525\n",
      "Validation loss decreased (0.207950 --> 0.207525).  Saving model ...\n",
      "Epoch: 8 \tTraining Loss: 0.829937 \tValidation Loss: 0.207102\n",
      "Validation loss decreased (0.207525 --> 0.207102).  Saving model ...\n",
      "Epoch: 9 \tTraining Loss: 0.828146 \tValidation Loss: 0.206641\n",
      "Validation loss decreased (0.207102 --> 0.206641).  Saving model ...\n",
      "Epoch: 10 \tTraining Loss: 0.826112 \tValidation Loss: 0.206056\n",
      "Validation loss decreased (0.206641 --> 0.206056).  Saving model ...\n",
      "Epoch: 11 \tTraining Loss: 0.823345 \tValidation Loss: 0.205266\n",
      "Validation loss decreased (0.206056 --> 0.205266).  Saving model ...\n",
      "Epoch: 12 \tTraining Loss: 0.818186 \tValidation Loss: 0.203021\n",
      "Validation loss decreased (0.205266 --> 0.203021).  Saving model ...\n",
      "Epoch: 13 \tTraining Loss: 0.790346 \tValidation Loss: 0.188770\n",
      "Validation loss decreased (0.203021 --> 0.188770).  Saving model ...\n",
      "Epoch: 14 \tTraining Loss: 0.736569 \tValidation Loss: 0.172776\n",
      "Validation loss decreased (0.188770 --> 0.172776).  Saving model ...\n",
      "Epoch: 15 \tTraining Loss: 0.674139 \tValidation Loss: 0.158371\n",
      "Validation loss decreased (0.172776 --> 0.158371).  Saving model ...\n",
      "Epoch: 16 \tTraining Loss: 0.622574 \tValidation Loss: 0.150137\n",
      "Validation loss decreased (0.158371 --> 0.150137).  Saving model ...\n",
      "Epoch: 17 \tTraining Loss: 0.595861 \tValidation Loss: 0.145793\n",
      "Validation loss decreased (0.150137 --> 0.145793).  Saving model ...\n",
      "Epoch: 18 \tTraining Loss: 0.583000 \tValidation Loss: 0.142767\n",
      "Validation loss decreased (0.145793 --> 0.142767).  Saving model ...\n",
      "Epoch: 19 \tTraining Loss: 0.573141 \tValidation Loss: 0.141320\n",
      "Validation loss decreased (0.142767 --> 0.141320).  Saving model ...\n",
      "Epoch: 20 \tTraining Loss: 0.567204 \tValidation Loss: 0.139809\n",
      "Validation loss decreased (0.141320 --> 0.139809).  Saving model ...\n",
      "Epoch: 21 \tTraining Loss: 0.561173 \tValidation Loss: 0.138880\n",
      "Validation loss decreased (0.139809 --> 0.138880).  Saving model ...\n",
      "Epoch: 22 \tTraining Loss: 0.556964 \tValidation Loss: 0.137449\n",
      "Validation loss decreased (0.138880 --> 0.137449).  Saving model ...\n",
      "Epoch: 23 \tTraining Loss: 0.552395 \tValidation Loss: 0.137921\n",
      "Epoch: 24 \tTraining Loss: 0.549412 \tValidation Loss: 0.135187\n",
      "Validation loss decreased (0.137449 --> 0.135187).  Saving model ...\n",
      "Epoch: 25 \tTraining Loss: 0.546523 \tValidation Loss: 0.134725\n",
      "Validation loss decreased (0.135187 --> 0.134725).  Saving model ...\n",
      "Epoch: 26 \tTraining Loss: 0.544156 \tValidation Loss: 0.135983\n",
      "Epoch: 27 \tTraining Loss: 0.540916 \tValidation Loss: 0.134238\n",
      "Validation loss decreased (0.134725 --> 0.134238).  Saving model ...\n",
      "Epoch: 28 \tTraining Loss: 0.538384 \tValidation Loss: 0.132894\n",
      "Validation loss decreased (0.134238 --> 0.132894).  Saving model ...\n",
      "Epoch: 29 \tTraining Loss: 0.536329 \tValidation Loss: 0.132398\n",
      "Validation loss decreased (0.132894 --> 0.132398).  Saving model ...\n",
      "Epoch: 30 \tTraining Loss: 0.533929 \tValidation Loss: 0.132299\n",
      "Validation loss decreased (0.132398 --> 0.132299).  Saving model ...\n",
      "Epoch: 31 \tTraining Loss: 0.531899 \tValidation Loss: 0.131822\n",
      "Validation loss decreased (0.132299 --> 0.131822).  Saving model ...\n",
      "Epoch: 32 \tTraining Loss: 0.530297 \tValidation Loss: 0.134503\n",
      "Epoch: 33 \tTraining Loss: 0.528499 \tValidation Loss: 0.131181\n",
      "Validation loss decreased (0.131822 --> 0.131181).  Saving model ...\n",
      "Epoch: 34 \tTraining Loss: 0.527547 \tValidation Loss: 0.130792\n",
      "Validation loss decreased (0.131181 --> 0.130792).  Saving model ...\n",
      "Epoch: 35 \tTraining Loss: 0.525441 \tValidation Loss: 0.134069\n",
      "Epoch: 36 \tTraining Loss: 0.523936 \tValidation Loss: 0.129732\n",
      "Validation loss decreased (0.130792 --> 0.129732).  Saving model ...\n",
      "Epoch: 37 \tTraining Loss: 0.522886 \tValidation Loss: 0.129379\n",
      "Validation loss decreased (0.129732 --> 0.129379).  Saving model ...\n",
      "Epoch: 38 \tTraining Loss: 0.521647 \tValidation Loss: 0.130389\n",
      "Epoch: 39 \tTraining Loss: 0.519973 \tValidation Loss: 0.130003\n",
      "Epoch: 40 \tTraining Loss: 0.519175 \tValidation Loss: 0.128716\n",
      "Validation loss decreased (0.129379 --> 0.128716).  Saving model ...\n",
      "Epoch: 41 \tTraining Loss: 0.517374 \tValidation Loss: 0.128861\n",
      "Epoch: 42 \tTraining Loss: 0.516564 \tValidation Loss: 0.128379\n",
      "Validation loss decreased (0.128716 --> 0.128379).  Saving model ...\n",
      "Epoch: 43 \tTraining Loss: 0.515127 \tValidation Loss: 0.130782\n",
      "Epoch: 44 \tTraining Loss: 0.514537 \tValidation Loss: 0.131275\n",
      "Epoch: 45 \tTraining Loss: 0.513247 \tValidation Loss: 0.128709\n",
      "Epoch: 46 \tTraining Loss: 0.512165 \tValidation Loss: 0.127486\n",
      "Validation loss decreased (0.128379 --> 0.127486).  Saving model ...\n",
      "Epoch: 47 \tTraining Loss: 0.511498 \tValidation Loss: 0.127162\n",
      "Validation loss decreased (0.127486 --> 0.127162).  Saving model ...\n",
      "Epoch: 48 \tTraining Loss: 0.510079 \tValidation Loss: 0.128115\n",
      "Epoch: 49 \tTraining Loss: 0.509107 \tValidation Loss: 0.126932\n",
      "Validation loss decreased (0.127162 --> 0.126932).  Saving model ...\n",
      "Epoch: 50 \tTraining Loss: 0.508356 \tValidation Loss: 0.128330\n",
      "Time elapsed: 504.83 s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "# number of epochs to train the model\n",
    "n_epochs = 50\n",
    "\n",
    "# initialize tracker for minimum validation loss\n",
    "valid_loss_min = np.Inf # set initial \"min\" to infinity\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "  # monitor training loss\n",
    "  train_loss = 0.0\n",
    "  valid_loss = 0.0\n",
    "\n",
    "  ###################\n",
    "  # train the model #\n",
    "  ###################\n",
    "  model_te_rnn.train() # prep model for training\n",
    "  for data, target in train_loader_te_rnn:\n",
    "    # transfer data and target to GPU\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    # clear the gradients of all optimized variables\n",
    "    optimizer_te_rnn.zero_grad()\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output = model_te_rnn(data.float())\n",
    "    # calculate the loss\n",
    "    loss = criterion(output, target)\n",
    "    # backward pass: compute gradient of the loss with respect to model parameters\n",
    "    loss.backward()\n",
    "    # perform a single optimization step (parameter update)\n",
    "    optimizer_te_rnn.step()\n",
    "    # update running training loss\n",
    "    train_loss += loss.item()*data.size(0)\n",
    "\n",
    "  ######################\n",
    "  # validate the model #\n",
    "  ######################\n",
    "  model_te_rnn.eval() # prep model for evaluation\n",
    "  for data, target in valid_loader_te_rnn:\n",
    "    # transfer data and target to GPU\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output = model_te_rnn(data.float())\n",
    "    # calculate the loss\n",
    "    loss = criterion(output, target)\n",
    "    # update running validation loss\n",
    "    valid_loss += loss.item()*data.size(0)\n",
    "    \n",
    "\n",
    "  # print training/validation statistics\n",
    "  # calculate average loss over an epoch\n",
    "  train_loss = train_loss/len(train_loader_te_rnn.dataset)\n",
    "  valid_loss = valid_loss/len(valid_loader_te_rnn.dataset)\n",
    "\n",
    "  print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch+1, train_loss, valid_loss))\n",
    "    \n",
    "  # save model if validation loss has decreased\n",
    "  if valid_loss <= valid_loss_min:\n",
    "    print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'\n",
    "        .format(valid_loss_min, valid_loss))\n",
    "    torch.save(model_te_rnn.state_dict(), 'model.pt')\n",
    "    valid_loss_min = valid_loss\n",
    "\n",
    "end = time.time()\n",
    "print('Time elapsed: %.2f s' % (end - start))\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sw01NfSZ0JwC",
    "outputId": "35631f7a-f832-410c-8d74-5800eaea7d2f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model with the lowest validation loss\n",
    "model_te_rnn.load_state_dict(torch.load('model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "7bJyS80q0JwC"
   },
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for out outputs\n",
    "with torch.no_grad():\n",
    "  for data in test_loader_te_rnn:\n",
    "    embeddings, labels = data\n",
    "    # transfer data and target to GPU\n",
    "    embeddings, labels = embeddings.to(device), labels.to(device)\n",
    "    # calculating outputs by running embeddings through the network\n",
    "    model_te_rnn.to(device)\n",
    "    outputs = model_te_rnn(embeddings.float())\n",
    "    # the class with the highest score is what we choose as prediction\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lk1Qc1qW0JwD",
    "outputId": "7df998f4-a103-4478-8886-0bab97a3c08e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy_simple-RNN-model_my-own-word2vec_ternary-test-set: 72 %\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy_simple-RNN-model_my-own-word2vec_ternary-test-set: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UmOe8mqpnPDf"
   },
   "source": [
    "# b. Gated RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YxNRgpadnPDf"
   },
   "source": [
    "## b.1 Use Pretrained Word2Vec as Input Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gpfd8bA5nPDg"
   },
   "source": [
    "### b.1.1 Compute Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ezg84DOFnPDg"
   },
   "source": [
    "#### Use **the pretrained model** to compute word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "brhM_6NdnPDg"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'X_train_te_gru_series = X_train_te.apply(lambda x: word2v_seq(x, wv_google))\\nX_train_te_gru = np.array(X_train_te_gru_series.values.tolist())\\n\\nX_test_te_gru_series = X_test_te.apply(lambda x: word2v_seq(x, wv_google))\\nX_test_te_gru = np.array(X_test_te_gru_series.values.tolist())'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the word2v_seq function to compute the word vectors of reviews (up to\n",
    "# 50 words)\n",
    "X_train_bi_gru_series = X_train_bi.apply(lambda x: word2v_seq(x, wv_google))\n",
    "X_train_bi_gru = np.array(X_train_bi_gru_series.values.tolist())\n",
    "\n",
    "X_test_bi_gru_series = X_test_bi.apply(lambda x: word2v_seq(x, wv_google))\n",
    "X_test_bi_gru = np.array(X_test_bi_gru_series.values.tolist())\n",
    "\n",
    "X_train_te_gru_series = X_train_te.apply(lambda x: word2v_seq(x, wv_google))\n",
    "X_train_te_gru = np.array(X_train_te_gru_series.values.tolist())\n",
    "\n",
    "X_test_te_gru_series = X_test_te.apply(lambda x: word2v_seq(x, wv_google))\n",
    "X_test_te_gru = np.array(X_test_te_gru_series.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "bYMaL1punPDg"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'idx_nan_train_te = idx_nan(X_train_te_gru)\\nif idx_nan_train_te != None:\\n  X_train_te_gru = np.delete(X_train_te_gru, idx_nan_train_te, 0)\\n  y_train_te_gru = np.delete(y_train_te, idx_nan_train_te)\\nelse:\\n  y_train_te_gru = y_train_te\\n\\nidx_nan_test_te = idx_nan(X_test_te_gru)\\nif idx_nan_test_te != None:\\n  X_test_te_gru = np.delete(X_test_te_gru, idx_nan_test_te, 0)\\n  y_test_te_gru = np.delete(y_test_te, idx_nan_test_te)\\nelse:\\n  y_test_te_gru = y_test_te'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the idx_nan function to remove the NaN's and corresponding\n",
    "# labels (if any)\n",
    "idx_nan_train_bi = idx_nan(X_train_bi_gru)\n",
    "if idx_nan_train_bi != None:\n",
    "  X_train_bi_gru = np.delete(X_train_bi_gru, idx_nan_train_bi, 0)\n",
    "  y_train_bi_gru = np.delete(y_train_bi, idx_nan_train_bi)\n",
    "else:\n",
    "  y_train_bi_gru = y_train_bi\n",
    "\n",
    "idx_nan_test_bi = idx_nan(X_test_bi_gru)\n",
    "if idx_nan_test_bi != None:\n",
    "  X_test_bi_gru = np.delete(X_test_bi_gru, idx_nan_test_bi, 0)\n",
    "  y_test_bi_gru = np.delete(y_test_bi, idx_nan_test_bi)\n",
    "else:\n",
    "  y_test_bi_gru = y_test_bi\n",
    "\n",
    "idx_nan_train_te = idx_nan(X_train_te_gru)\n",
    "if idx_nan_train_te != None:\n",
    "  X_train_te_gru = np.delete(X_train_te_gru, idx_nan_train_te, 0)\n",
    "  y_train_te_gru = np.delete(y_train_te, idx_nan_train_te)\n",
    "else:\n",
    "  y_train_te_gru = y_train_te\n",
    "\n",
    "idx_nan_test_te = idx_nan(X_test_te_gru)\n",
    "if idx_nan_test_te != None:\n",
    "  X_test_te_gru = np.delete(X_test_te_gru, idx_nan_test_te, 0)\n",
    "  y_test_te_gru = np.delete(y_test_te, idx_nan_test_te)\n",
    "else:\n",
    "  y_test_te_gru = y_test_te"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L3KjT-pCnPDg"
   },
   "source": [
    "### b.1.2 Define Dataset Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "fLTleKkMnPDg"
   },
   "outputs": [],
   "source": [
    "class Train(Dataset):\n",
    "  def __init__(self, xtrain, ytrain):\n",
    "    'Initialization'\n",
    "    self.data = xtrain\n",
    "    self.labels = ytrain\n",
    "\n",
    "  def __len__(self):\n",
    "    'Denotes the total number of samples'\n",
    "    return len(self.data)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    'Generates one sample of data'\n",
    "    X = self.data[index]\n",
    "    y = self.labels[index]\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "ErelhbhQnPDg"
   },
   "outputs": [],
   "source": [
    "class Test(Dataset):\n",
    "  def __init__(self, xtest, ytest):\n",
    "    'Initialization'\n",
    "    self.data = xtest\n",
    "    self.labels = ytest\n",
    "\n",
    "  def __len__(self):\n",
    "    'Denotes the total number of samples'\n",
    "    return len(self.data)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    'Generates one sample of data'\n",
    "    X = self.data[index]\n",
    "    y = self.labels[index]\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "le-51PdAnPDg"
   },
   "source": [
    "### b.1.3 Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "zYUl3adXnPDh"
   },
   "outputs": [],
   "source": [
    "# use word vectors of word sequences as input features\n",
    "train_data_bi_gru, test_data_bi_gru = Train(X_train_bi_gru, y_train_bi_gru-1), Test(X_test_bi_gru, y_test_bi_gru-1)\n",
    "train_data_te_gru, test_data_te_gru = Train(X_train_te_gru, y_train_te_gru-1), Test(X_test_te_gru, y_test_te_gru-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XOhryN50nPDh"
   },
   "source": [
    "#### Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "JgFJ3-VRnPDh"
   },
   "outputs": [],
   "source": [
    "# number of subprocesses to use for data loading\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 100\n",
    "# percentage of training set to use as validation\n",
    "valid_size = 0.2\n",
    "\n",
    "# obtain training indices that will be used for validation\n",
    "num_train = len(train_data_bi_gru)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "# define samples for obtaining training and validation batches\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "# prepare data loaders\n",
    "train_loader_bi_gru = torch.utils.data.DataLoader(train_data_bi_gru, batch_size=batch_size,\n",
    "                                           sampler=train_sampler, num_workers\n",
    "                                           =num_workers)\n",
    "valid_loader_bi_gru = torch.utils.data.DataLoader(train_data_bi_gru, batch_size=batch_size,\n",
    "                                           sampler=valid_sampler, num_workers\n",
    "                                           =num_workers)\n",
    "test_loader_bi_gru = torch.utils.data.DataLoader(test_data_bi_gru, batch_size=batch_size,\n",
    "                                           num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9yFtQn8-nPDh"
   },
   "source": [
    "#### Ternary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "kDfbTvqWnPDh"
   },
   "outputs": [],
   "source": [
    "# number of subprocesses to use for data loading\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 100\n",
    "# percentage of training set to use as validation\n",
    "valid_size = 0.2\n",
    "\n",
    "# obtain training indices that will be used for validation\n",
    "num_train = len(train_data_te_gru)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "# define samples for obtaining training and validation batches\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "# prepare data loaders\n",
    "train_loader_te_gru = torch.utils.data.DataLoader(train_data_te_gru, batch_size=batch_size,\n",
    "                                           sampler=train_sampler, num_workers\n",
    "                                           =num_workers)\n",
    "valid_loader_te_gru = torch.utils.data.DataLoader(train_data_te_gru, batch_size=batch_size,\n",
    "                                           sampler=valid_sampler, num_workers\n",
    "                                           =num_workers)\n",
    "test_loader_te_gru = torch.utils.data.DataLoader(test_data_te_gru, batch_size=batch_size,\n",
    "                                           num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I6Whod_9nPDi"
   },
   "source": [
    "### b.1.4 Define the GRU architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "M32tmMAvnPDi"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_jmr7Mj1nPDi",
    "outputId": "97ff14a9-fabc-408b-98fe-99850b52e18a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRUModel(\n",
      "  (gru): GRU(300, 50, batch_first=True)\n",
      "  (fc): Linear(in_features=50, out_features=2, bias=True)\n",
      ")\n",
      "GRUModel(\n",
      "  (gru): GRU(300, 50, batch_first=True)\n",
      "  (fc): Linear(in_features=50, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# define the GRU architecture\n",
    "class GRUModel(nn.Module):\n",
    "  def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
    "    super().__init__()\n",
    "    \n",
    "    # Number of hidden dimensions\n",
    "    self.hidden_dim = hidden_dim\n",
    "    \n",
    "    # Number of hidden layers\n",
    "    self.layer_dim = layer_dim\n",
    "    \n",
    "    # GRU\n",
    "    self.gru = nn.GRU(input_dim, hidden_dim, layer_dim, batch_first=True)\n",
    "    # Output layer\n",
    "    self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "  def forward(self, x):\n",
    "    \n",
    "    # Initialize hidden state with zeros\n",
    "    h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).to(device)\n",
    "    \n",
    "    # One time step\n",
    "    out, hn = self.gru(x, h0)\n",
    "    out = self.fc(out[:, -1, :])\n",
    "    return out\n",
    "\n",
    "# initialize GRU\n",
    "model_bi_gru = GRUModel(300, 50, 1, 2)\n",
    "model_te_gru = GRUModel(300, 50, 1, 3)\n",
    "model_bi_gru.cuda()\n",
    "model_te_gru.cuda()\n",
    "print(model_bi_gru)\n",
    "print(model_te_gru)\n",
    "\n",
    "# specify loss function (categorical cross-entropy)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# specify optimizer (stochastic gradient descent) and learning rate = 0.01\n",
    "optimizer_bi_gru = torch.optim.SGD(model_bi_gru.parameters(), lr=0.0075)\n",
    "optimizer_te_gru = torch.optim.SGD(model_te_gru.parameters(), lr=0.0075)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JolfF1JmnPDi"
   },
   "source": [
    "### b.1.5 Train the Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6fLzh4AanPDi"
   },
   "source": [
    "#### Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "4zIVMLrGnPDj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.552100 \tValidation Loss: 0.137085\n",
      "Validation loss decreased (inf --> 0.137085).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 0.544563 \tValidation Loss: 0.135447\n",
      "Validation loss decreased (0.137085 --> 0.135447).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 0.537686 \tValidation Loss: 0.133837\n",
      "Validation loss decreased (0.135447 --> 0.133837).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 0.530632 \tValidation Loss: 0.131987\n",
      "Validation loss decreased (0.133837 --> 0.131987).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 0.520159 \tValidation Loss: 0.128083\n",
      "Validation loss decreased (0.131987 --> 0.128083).  Saving model ...\n",
      "Epoch: 6 \tTraining Loss: 0.446340 \tValidation Loss: 0.089533\n",
      "Validation loss decreased (0.128083 --> 0.089533).  Saving model ...\n",
      "Epoch: 7 \tTraining Loss: 0.328485 \tValidation Loss: 0.084651\n",
      "Validation loss decreased (0.089533 --> 0.084651).  Saving model ...\n",
      "Epoch: 8 \tTraining Loss: 0.303914 \tValidation Loss: 0.073963\n",
      "Validation loss decreased (0.084651 --> 0.073963).  Saving model ...\n",
      "Epoch: 9 \tTraining Loss: 0.294422 \tValidation Loss: 0.072475\n",
      "Validation loss decreased (0.073963 --> 0.072475).  Saving model ...\n",
      "Epoch: 10 \tTraining Loss: 0.288204 \tValidation Loss: 0.070906\n",
      "Validation loss decreased (0.072475 --> 0.070906).  Saving model ...\n",
      "Epoch: 11 \tTraining Loss: 0.282115 \tValidation Loss: 0.069960\n",
      "Validation loss decreased (0.070906 --> 0.069960).  Saving model ...\n",
      "Epoch: 12 \tTraining Loss: 0.279493 \tValidation Loss: 0.069013\n",
      "Validation loss decreased (0.069960 --> 0.069013).  Saving model ...\n",
      "Epoch: 13 \tTraining Loss: 0.274066 \tValidation Loss: 0.072063\n",
      "Epoch: 14 \tTraining Loss: 0.271068 \tValidation Loss: 0.066838\n",
      "Validation loss decreased (0.069013 --> 0.066838).  Saving model ...\n",
      "Epoch: 15 \tTraining Loss: 0.265828 \tValidation Loss: 0.065683\n",
      "Validation loss decreased (0.066838 --> 0.065683).  Saving model ...\n",
      "Epoch: 16 \tTraining Loss: 0.263007 \tValidation Loss: 0.068862\n",
      "Epoch: 17 \tTraining Loss: 0.259160 \tValidation Loss: 0.064382\n",
      "Validation loss decreased (0.065683 --> 0.064382).  Saving model ...\n",
      "Epoch: 18 \tTraining Loss: 0.256658 \tValidation Loss: 0.063451\n",
      "Validation loss decreased (0.064382 --> 0.063451).  Saving model ...\n",
      "Epoch: 19 \tTraining Loss: 0.253310 \tValidation Loss: 0.062640\n",
      "Validation loss decreased (0.063451 --> 0.062640).  Saving model ...\n",
      "Epoch: 20 \tTraining Loss: 0.250511 \tValidation Loss: 0.066605\n",
      "Epoch: 21 \tTraining Loss: 0.248500 \tValidation Loss: 0.061498\n",
      "Validation loss decreased (0.062640 --> 0.061498).  Saving model ...\n",
      "Epoch: 22 \tTraining Loss: 0.246167 \tValidation Loss: 0.062343\n",
      "Epoch: 23 \tTraining Loss: 0.243553 \tValidation Loss: 0.061344\n",
      "Validation loss decreased (0.061498 --> 0.061344).  Saving model ...\n",
      "Epoch: 24 \tTraining Loss: 0.241743 \tValidation Loss: 0.060062\n",
      "Validation loss decreased (0.061344 --> 0.060062).  Saving model ...\n",
      "Epoch: 25 \tTraining Loss: 0.240373 \tValidation Loss: 0.059883\n",
      "Validation loss decreased (0.060062 --> 0.059883).  Saving model ...\n",
      "Epoch: 26 \tTraining Loss: 0.237762 \tValidation Loss: 0.061729\n",
      "Epoch: 27 \tTraining Loss: 0.236852 \tValidation Loss: 0.061512\n",
      "Epoch: 28 \tTraining Loss: 0.235189 \tValidation Loss: 0.060212\n",
      "Epoch: 29 \tTraining Loss: 0.233487 \tValidation Loss: 0.059588\n",
      "Validation loss decreased (0.059883 --> 0.059588).  Saving model ...\n",
      "Epoch: 30 \tTraining Loss: 0.231900 \tValidation Loss: 0.057739\n",
      "Validation loss decreased (0.059588 --> 0.057739).  Saving model ...\n",
      "Epoch: 31 \tTraining Loss: 0.230158 \tValidation Loss: 0.059574\n",
      "Epoch: 32 \tTraining Loss: 0.229086 \tValidation Loss: 0.057171\n",
      "Validation loss decreased (0.057739 --> 0.057171).  Saving model ...\n",
      "Epoch: 33 \tTraining Loss: 0.227565 \tValidation Loss: 0.056803\n",
      "Validation loss decreased (0.057171 --> 0.056803).  Saving model ...\n",
      "Epoch: 34 \tTraining Loss: 0.226381 \tValidation Loss: 0.057203\n",
      "Epoch: 35 \tTraining Loss: 0.225369 \tValidation Loss: 0.056230\n",
      "Validation loss decreased (0.056803 --> 0.056230).  Saving model ...\n",
      "Epoch: 36 \tTraining Loss: 0.224432 \tValidation Loss: 0.060599\n",
      "Epoch: 37 \tTraining Loss: 0.222539 \tValidation Loss: 0.055724\n",
      "Validation loss decreased (0.056230 --> 0.055724).  Saving model ...\n",
      "Epoch: 38 \tTraining Loss: 0.222072 \tValidation Loss: 0.055594\n",
      "Validation loss decreased (0.055724 --> 0.055594).  Saving model ...\n",
      "Epoch: 39 \tTraining Loss: 0.220670 \tValidation Loss: 0.056065\n",
      "Epoch: 40 \tTraining Loss: 0.220134 \tValidation Loss: 0.055074\n",
      "Validation loss decreased (0.055594 --> 0.055074).  Saving model ...\n",
      "Epoch: 41 \tTraining Loss: 0.218536 \tValidation Loss: 0.059874\n",
      "Epoch: 42 \tTraining Loss: 0.219260 \tValidation Loss: 0.054617\n",
      "Validation loss decreased (0.055074 --> 0.054617).  Saving model ...\n",
      "Epoch: 43 \tTraining Loss: 0.217838 \tValidation Loss: 0.054658\n",
      "Epoch: 44 \tTraining Loss: 0.216649 \tValidation Loss: 0.059308\n",
      "Epoch: 45 \tTraining Loss: 0.215689 \tValidation Loss: 0.054393\n",
      "Validation loss decreased (0.054617 --> 0.054393).  Saving model ...\n",
      "Epoch: 46 \tTraining Loss: 0.214618 \tValidation Loss: 0.053954\n",
      "Validation loss decreased (0.054393 --> 0.053954).  Saving model ...\n",
      "Epoch: 47 \tTraining Loss: 0.213591 \tValidation Loss: 0.066493\n",
      "Epoch: 48 \tTraining Loss: 0.214057 \tValidation Loss: 0.053539\n",
      "Validation loss decreased (0.053954 --> 0.053539).  Saving model ...\n",
      "Epoch: 49 \tTraining Loss: 0.212485 \tValidation Loss: 0.053397\n",
      "Validation loss decreased (0.053539 --> 0.053397).  Saving model ...\n",
      "Epoch: 50 \tTraining Loss: 0.211884 \tValidation Loss: 0.053557\n",
      "Epoch: 51 \tTraining Loss: 0.210767 \tValidation Loss: 0.053146\n",
      "Validation loss decreased (0.053397 --> 0.053146).  Saving model ...\n",
      "Epoch: 52 \tTraining Loss: 0.211299 \tValidation Loss: 0.053024\n",
      "Validation loss decreased (0.053146 --> 0.053024).  Saving model ...\n",
      "Epoch: 53 \tTraining Loss: 0.209445 \tValidation Loss: 0.055574\n",
      "Epoch: 54 \tTraining Loss: 0.209165 \tValidation Loss: 0.052644\n",
      "Validation loss decreased (0.053024 --> 0.052644).  Saving model ...\n",
      "Epoch: 55 \tTraining Loss: 0.207944 \tValidation Loss: 0.052662\n",
      "Epoch: 56 \tTraining Loss: 0.207889 \tValidation Loss: 0.052820\n",
      "Epoch: 57 \tTraining Loss: 0.207073 \tValidation Loss: 0.055544\n",
      "Epoch: 58 \tTraining Loss: 0.206188 \tValidation Loss: 0.052204\n",
      "Validation loss decreased (0.052644 --> 0.052204).  Saving model ...\n",
      "Epoch: 59 \tTraining Loss: 0.205286 \tValidation Loss: 0.053575\n",
      "Epoch: 60 \tTraining Loss: 0.205879 \tValidation Loss: 0.052354\n",
      "Epoch: 61 \tTraining Loss: 0.204849 \tValidation Loss: 0.052198\n",
      "Validation loss decreased (0.052204 --> 0.052198).  Saving model ...\n",
      "Epoch: 62 \tTraining Loss: 0.204420 \tValidation Loss: 0.054697\n",
      "Epoch: 63 \tTraining Loss: 0.203644 \tValidation Loss: 0.051546\n",
      "Validation loss decreased (0.052198 --> 0.051546).  Saving model ...\n",
      "Epoch: 64 \tTraining Loss: 0.202837 \tValidation Loss: 0.051485\n",
      "Validation loss decreased (0.051546 --> 0.051485).  Saving model ...\n",
      "Epoch: 65 \tTraining Loss: 0.202289 \tValidation Loss: 0.052681\n",
      "Epoch: 66 \tTraining Loss: 0.201224 \tValidation Loss: 0.051745\n",
      "Epoch: 67 \tTraining Loss: 0.201599 \tValidation Loss: 0.051661\n",
      "Epoch: 68 \tTraining Loss: 0.201318 \tValidation Loss: 0.051234\n",
      "Validation loss decreased (0.051485 --> 0.051234).  Saving model ...\n",
      "Epoch: 69 \tTraining Loss: 0.200360 \tValidation Loss: 0.051117\n",
      "Validation loss decreased (0.051234 --> 0.051117).  Saving model ...\n",
      "Epoch: 70 \tTraining Loss: 0.199533 \tValidation Loss: 0.051516\n",
      "Epoch: 71 \tTraining Loss: 0.198987 \tValidation Loss: 0.051761\n",
      "Epoch: 72 \tTraining Loss: 0.198984 \tValidation Loss: 0.050699\n",
      "Validation loss decreased (0.051117 --> 0.050699).  Saving model ...\n",
      "Epoch: 73 \tTraining Loss: 0.197695 \tValidation Loss: 0.050607\n",
      "Validation loss decreased (0.050699 --> 0.050607).  Saving model ...\n",
      "Epoch: 74 \tTraining Loss: 0.197069 \tValidation Loss: 0.050457\n",
      "Validation loss decreased (0.050607 --> 0.050457).  Saving model ...\n",
      "Epoch: 75 \tTraining Loss: 0.197175 \tValidation Loss: 0.052015\n",
      "Epoch: 76 \tTraining Loss: 0.197447 \tValidation Loss: 0.050233\n",
      "Validation loss decreased (0.050457 --> 0.050233).  Saving model ...\n",
      "Epoch: 77 \tTraining Loss: 0.196301 \tValidation Loss: 0.050882\n",
      "Epoch: 78 \tTraining Loss: 0.195566 \tValidation Loss: 0.050219\n",
      "Validation loss decreased (0.050233 --> 0.050219).  Saving model ...\n",
      "Epoch: 79 \tTraining Loss: 0.195422 \tValidation Loss: 0.050153\n",
      "Validation loss decreased (0.050219 --> 0.050153).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 80 \tTraining Loss: 0.194636 \tValidation Loss: 0.050319\n",
      "Epoch: 81 \tTraining Loss: 0.194179 \tValidation Loss: 0.052145\n",
      "Epoch: 82 \tTraining Loss: 0.193637 \tValidation Loss: 0.050706\n",
      "Epoch: 83 \tTraining Loss: 0.194137 \tValidation Loss: 0.049737\n",
      "Validation loss decreased (0.050153 --> 0.049737).  Saving model ...\n",
      "Epoch: 84 \tTraining Loss: 0.193104 \tValidation Loss: 0.052036\n",
      "Epoch: 85 \tTraining Loss: 0.193005 \tValidation Loss: 0.053755\n",
      "Epoch: 86 \tTraining Loss: 0.192201 \tValidation Loss: 0.051867\n",
      "Epoch: 87 \tTraining Loss: 0.191476 \tValidation Loss: 0.049603\n",
      "Validation loss decreased (0.049737 --> 0.049603).  Saving model ...\n",
      "Epoch: 88 \tTraining Loss: 0.191578 \tValidation Loss: 0.049745\n",
      "Epoch: 89 \tTraining Loss: 0.191468 \tValidation Loss: 0.049435\n",
      "Validation loss decreased (0.049603 --> 0.049435).  Saving model ...\n",
      "Epoch: 90 \tTraining Loss: 0.190345 \tValidation Loss: 0.051674\n",
      "Epoch: 91 \tTraining Loss: 0.190640 \tValidation Loss: 0.049236\n",
      "Validation loss decreased (0.049435 --> 0.049236).  Saving model ...\n",
      "Epoch: 92 \tTraining Loss: 0.189890 \tValidation Loss: 0.057253\n",
      "Epoch: 93 \tTraining Loss: 0.189837 \tValidation Loss: 0.049205\n",
      "Validation loss decreased (0.049236 --> 0.049205).  Saving model ...\n",
      "Epoch: 94 \tTraining Loss: 0.189379 \tValidation Loss: 0.050170\n",
      "Epoch: 95 \tTraining Loss: 0.189187 \tValidation Loss: 0.049057\n",
      "Validation loss decreased (0.049205 --> 0.049057).  Saving model ...\n",
      "Epoch: 96 \tTraining Loss: 0.188841 \tValidation Loss: 0.049920\n",
      "Epoch: 97 \tTraining Loss: 0.188045 \tValidation Loss: 0.050600\n",
      "Epoch: 98 \tTraining Loss: 0.187710 \tValidation Loss: 0.050832\n",
      "Epoch: 99 \tTraining Loss: 0.187562 \tValidation Loss: 0.051035\n",
      "Epoch: 100 \tTraining Loss: 0.187372 \tValidation Loss: 0.049234\n",
      "Time elapsed: 1145.68 s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "# number of epochs to train the model\n",
    "n_epochs = 100\n",
    "\n",
    "# initialize tracker for minimum validation loss\n",
    "valid_loss_min = np.Inf # set initial \"min\" to infinity\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "  # monitor training loss\n",
    "  train_loss = 0.0\n",
    "  valid_loss = 0.0\n",
    "\n",
    "  ###################\n",
    "  # train the model #\n",
    "  ###################\n",
    "  model_bi_gru.train() # prep model for training\n",
    "  for data, target in train_loader_bi_gru:\n",
    "    # transfer data and target to GPU\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    # clear the gradients of all optimized variables\n",
    "    optimizer_bi_gru.zero_grad()\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output = model_bi_gru(data.float())\n",
    "    # calculate the loss\n",
    "    loss = criterion(output, target)\n",
    "    # backward pass: compute gradient of the loss with respect to model parameters\n",
    "    loss.backward()\n",
    "    # perform a single optimization step (parameter update)\n",
    "    optimizer_bi_gru.step()\n",
    "    # update running training loss\n",
    "    train_loss += loss.item()*data.size(0)\n",
    "\n",
    "  ######################\n",
    "  # validate the model #\n",
    "  ######################\n",
    "  model_bi_gru.eval() # prep model for evaluation\n",
    "  for data, target in valid_loader_bi_gru:\n",
    "    # transfer data and target to GPU\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output = model_bi_gru(data.float())\n",
    "    # calculate the loss\n",
    "    loss = criterion(output, target)\n",
    "    # update running validation loss\n",
    "    valid_loss += loss.item()*data.size(0)\n",
    "    \n",
    "\n",
    "  # print training/validation statistics\n",
    "  # calculate average loss over an epoch\n",
    "  train_loss = train_loss/len(train_loader_bi_gru.dataset)\n",
    "  valid_loss = valid_loss/len(valid_loader_bi_gru.dataset)\n",
    "\n",
    "  print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch+1, train_loss, valid_loss))\n",
    "    \n",
    "  # save model if validation loss has decreased\n",
    "  if valid_loss <= valid_loss_min:\n",
    "    print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'\n",
    "        .format(valid_loss_min, valid_loss))\n",
    "    torch.save(model_bi_gru.state_dict(), 'model.pt')\n",
    "    valid_loss_min = valid_loss\n",
    "\n",
    "end = time.time()\n",
    "print('Time elapsed: %.2f s' % (end - start))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "IgUOH-IhnPDj"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model with the lowest validation loss\n",
    "model_bi_gru.load_state_dict(torch.load('model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "_RvfVyinnPDj"
   },
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for out outputs\n",
    "with torch.no_grad():\n",
    "  for data in test_loader_bi_gru:\n",
    "    embeddings, labels = data\n",
    "    # transfer data and target to GPU\n",
    "    embeddings, labels = embeddings.to(device), labels.to(device)\n",
    "    # calculating outputs by running embeddings through the network\n",
    "    model_bi_gru.to(device)\n",
    "    outputs = model_bi_gru(embeddings.float())\n",
    "    # the class with the highest score is what we choose as prediction\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "3tTl4ERHnPDj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy_GRU-model_pretrained-word2vec_binary-test-set: 89 %\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy_GRU-model_pretrained-word2vec_binary-test-set: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DJN2HFHenPDj"
   },
   "source": [
    "#### Ternary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJZJgeNrnPDj",
    "outputId": "5abff7fa-3e40-45f7-9fc3-54b49c665e9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.845490 \tValidation Loss: 0.210254\n",
      "Validation loss decreased (inf --> 0.210254).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 0.838754 \tValidation Loss: 0.209159\n",
      "Validation loss decreased (0.210254 --> 0.209159).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 0.834537 \tValidation Loss: 0.208128\n",
      "Validation loss decreased (0.209159 --> 0.208128).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 0.830333 \tValidation Loss: 0.207067\n",
      "Validation loss decreased (0.208128 --> 0.207067).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 0.825780 \tValidation Loss: 0.205835\n",
      "Validation loss decreased (0.207067 --> 0.205835).  Saving model ...\n",
      "Epoch: 6 \tTraining Loss: 0.819808 \tValidation Loss: 0.203992\n",
      "Validation loss decreased (0.205835 --> 0.203992).  Saving model ...\n",
      "Epoch: 7 \tTraining Loss: 0.804928 \tValidation Loss: 0.194501\n",
      "Validation loss decreased (0.203992 --> 0.194501).  Saving model ...\n",
      "Epoch: 8 \tTraining Loss: 0.688595 \tValidation Loss: 0.164172\n",
      "Validation loss decreased (0.194501 --> 0.164172).  Saving model ...\n",
      "Epoch: 9 \tTraining Loss: 0.631471 \tValidation Loss: 0.162676\n",
      "Validation loss decreased (0.164172 --> 0.162676).  Saving model ...\n",
      "Epoch: 10 \tTraining Loss: 0.613669 \tValidation Loss: 0.151925\n",
      "Validation loss decreased (0.162676 --> 0.151925).  Saving model ...\n",
      "Epoch: 11 \tTraining Loss: 0.602264 \tValidation Loss: 0.152223\n",
      "Epoch: 12 \tTraining Loss: 0.592254 \tValidation Loss: 0.147104\n",
      "Validation loss decreased (0.151925 --> 0.147104).  Saving model ...\n",
      "Epoch: 13 \tTraining Loss: 0.583478 \tValidation Loss: 0.145416\n",
      "Validation loss decreased (0.147104 --> 0.145416).  Saving model ...\n",
      "Epoch: 14 \tTraining Loss: 0.577297 \tValidation Loss: 0.147601\n",
      "Epoch: 15 \tTraining Loss: 0.571316 \tValidation Loss: 0.143857\n",
      "Validation loss decreased (0.145416 --> 0.143857).  Saving model ...\n",
      "Epoch: 16 \tTraining Loss: 0.565749 \tValidation Loss: 0.141671\n",
      "Validation loss decreased (0.143857 --> 0.141671).  Saving model ...\n",
      "Epoch: 17 \tTraining Loss: 0.560377 \tValidation Loss: 0.141182\n",
      "Validation loss decreased (0.141671 --> 0.141182).  Saving model ...\n",
      "Epoch: 18 \tTraining Loss: 0.555998 \tValidation Loss: 0.139262\n",
      "Validation loss decreased (0.141182 --> 0.139262).  Saving model ...\n",
      "Epoch: 19 \tTraining Loss: 0.551697 \tValidation Loss: 0.138275\n",
      "Validation loss decreased (0.139262 --> 0.138275).  Saving model ...\n",
      "Epoch: 20 \tTraining Loss: 0.547286 \tValidation Loss: 0.137141\n",
      "Validation loss decreased (0.138275 --> 0.137141).  Saving model ...\n",
      "Epoch: 21 \tTraining Loss: 0.543201 \tValidation Loss: 0.139971\n",
      "Epoch: 22 \tTraining Loss: 0.538234 \tValidation Loss: 0.135404\n",
      "Validation loss decreased (0.137141 --> 0.135404).  Saving model ...\n",
      "Epoch: 23 \tTraining Loss: 0.535492 \tValidation Loss: 0.139198\n",
      "Epoch: 24 \tTraining Loss: 0.532220 \tValidation Loss: 0.133594\n",
      "Validation loss decreased (0.135404 --> 0.133594).  Saving model ...\n",
      "Epoch: 25 \tTraining Loss: 0.528557 \tValidation Loss: 0.132930\n",
      "Validation loss decreased (0.133594 --> 0.132930).  Saving model ...\n",
      "Epoch: 26 \tTraining Loss: 0.525887 \tValidation Loss: 0.132185\n",
      "Validation loss decreased (0.132930 --> 0.132185).  Saving model ...\n",
      "Epoch: 27 \tTraining Loss: 0.522583 \tValidation Loss: 0.133107\n",
      "Epoch: 28 \tTraining Loss: 0.519552 \tValidation Loss: 0.130835\n",
      "Validation loss decreased (0.132185 --> 0.130835).  Saving model ...\n",
      "Epoch: 29 \tTraining Loss: 0.516249 \tValidation Loss: 0.130564\n",
      "Validation loss decreased (0.130835 --> 0.130564).  Saving model ...\n",
      "Epoch: 30 \tTraining Loss: 0.513808 \tValidation Loss: 0.129229\n",
      "Validation loss decreased (0.130564 --> 0.129229).  Saving model ...\n",
      "Epoch: 31 \tTraining Loss: 0.510323 \tValidation Loss: 0.132220\n",
      "Epoch: 32 \tTraining Loss: 0.506631 \tValidation Loss: 0.128759\n",
      "Validation loss decreased (0.129229 --> 0.128759).  Saving model ...\n",
      "Epoch: 33 \tTraining Loss: 0.504199 \tValidation Loss: 0.129739\n",
      "Epoch: 34 \tTraining Loss: 0.500307 \tValidation Loss: 0.126405\n",
      "Validation loss decreased (0.128759 --> 0.126405).  Saving model ...\n",
      "Epoch: 35 \tTraining Loss: 0.497348 \tValidation Loss: 0.125155\n",
      "Validation loss decreased (0.126405 --> 0.125155).  Saving model ...\n",
      "Epoch: 36 \tTraining Loss: 0.494471 \tValidation Loss: 0.124802\n",
      "Validation loss decreased (0.125155 --> 0.124802).  Saving model ...\n",
      "Epoch: 37 \tTraining Loss: 0.491890 \tValidation Loss: 0.123821\n",
      "Validation loss decreased (0.124802 --> 0.123821).  Saving model ...\n",
      "Epoch: 38 \tTraining Loss: 0.488549 \tValidation Loss: 0.125786\n",
      "Epoch: 39 \tTraining Loss: 0.486344 \tValidation Loss: 0.124179\n",
      "Epoch: 40 \tTraining Loss: 0.484476 \tValidation Loss: 0.123407\n",
      "Validation loss decreased (0.123821 --> 0.123407).  Saving model ...\n",
      "Epoch: 41 \tTraining Loss: 0.482587 \tValidation Loss: 0.122448\n",
      "Validation loss decreased (0.123407 --> 0.122448).  Saving model ...\n",
      "Epoch: 42 \tTraining Loss: 0.481167 \tValidation Loss: 0.121461\n",
      "Validation loss decreased (0.122448 --> 0.121461).  Saving model ...\n",
      "Epoch: 43 \tTraining Loss: 0.479285 \tValidation Loss: 0.121381\n",
      "Validation loss decreased (0.121461 --> 0.121381).  Saving model ...\n",
      "Epoch: 44 \tTraining Loss: 0.477652 \tValidation Loss: 0.125198\n",
      "Epoch: 45 \tTraining Loss: 0.476012 \tValidation Loss: 0.120474\n",
      "Validation loss decreased (0.121381 --> 0.120474).  Saving model ...\n",
      "Epoch: 46 \tTraining Loss: 0.474484 \tValidation Loss: 0.121063\n",
      "Epoch: 47 \tTraining Loss: 0.473098 \tValidation Loss: 0.120902\n",
      "Epoch: 48 \tTraining Loss: 0.471820 \tValidation Loss: 0.119515\n",
      "Validation loss decreased (0.120474 --> 0.119515).  Saving model ...\n",
      "Epoch: 49 \tTraining Loss: 0.470411 \tValidation Loss: 0.119745\n",
      "Epoch: 50 \tTraining Loss: 0.469053 \tValidation Loss: 0.122989\n",
      "Epoch: 51 \tTraining Loss: 0.468027 \tValidation Loss: 0.121440\n",
      "Epoch: 52 \tTraining Loss: 0.467162 \tValidation Loss: 0.118546\n",
      "Validation loss decreased (0.119515 --> 0.118546).  Saving model ...\n",
      "Epoch: 53 \tTraining Loss: 0.465791 \tValidation Loss: 0.122065\n",
      "Epoch: 54 \tTraining Loss: 0.465259 \tValidation Loss: 0.118278\n",
      "Validation loss decreased (0.118546 --> 0.118278).  Saving model ...\n",
      "Epoch: 55 \tTraining Loss: 0.464240 \tValidation Loss: 0.119933\n",
      "Epoch: 56 \tTraining Loss: 0.463142 \tValidation Loss: 0.119672\n",
      "Epoch: 57 \tTraining Loss: 0.461897 \tValidation Loss: 0.125022\n",
      "Epoch: 58 \tTraining Loss: 0.461295 \tValidation Loss: 0.117570\n",
      "Validation loss decreased (0.118278 --> 0.117570).  Saving model ...\n",
      "Epoch: 59 \tTraining Loss: 0.460465 \tValidation Loss: 0.118418\n",
      "Epoch: 60 \tTraining Loss: 0.460260 \tValidation Loss: 0.119576\n",
      "Epoch: 61 \tTraining Loss: 0.459033 \tValidation Loss: 0.116932\n",
      "Validation loss decreased (0.117570 --> 0.116932).  Saving model ...\n",
      "Epoch: 62 \tTraining Loss: 0.458306 \tValidation Loss: 0.117637\n",
      "Epoch: 63 \tTraining Loss: 0.457380 \tValidation Loss: 0.121009\n",
      "Epoch: 64 \tTraining Loss: 0.456402 \tValidation Loss: 0.118546\n",
      "Epoch: 65 \tTraining Loss: 0.455558 \tValidation Loss: 0.120360\n",
      "Epoch: 66 \tTraining Loss: 0.454799 \tValidation Loss: 0.116332\n",
      "Validation loss decreased (0.116932 --> 0.116332).  Saving model ...\n",
      "Epoch: 67 \tTraining Loss: 0.454480 \tValidation Loss: 0.115960\n",
      "Validation loss decreased (0.116332 --> 0.115960).  Saving model ...\n",
      "Epoch: 68 \tTraining Loss: 0.454057 \tValidation Loss: 0.116078\n",
      "Epoch: 69 \tTraining Loss: 0.453114 \tValidation Loss: 0.116609\n",
      "Epoch: 70 \tTraining Loss: 0.452606 \tValidation Loss: 0.116663\n",
      "Epoch: 71 \tTraining Loss: 0.451488 \tValidation Loss: 0.117768\n",
      "Epoch: 72 \tTraining Loss: 0.451381 \tValidation Loss: 0.116036\n",
      "Epoch: 73 \tTraining Loss: 0.450533 \tValidation Loss: 0.120056\n",
      "Epoch: 74 \tTraining Loss: 0.449751 \tValidation Loss: 0.115381\n",
      "Validation loss decreased (0.115960 --> 0.115381).  Saving model ...\n",
      "Epoch: 75 \tTraining Loss: 0.449046 \tValidation Loss: 0.116521\n",
      "Epoch: 76 \tTraining Loss: 0.449041 \tValidation Loss: 0.116472\n",
      "Epoch: 77 \tTraining Loss: 0.448588 \tValidation Loss: 0.116969\n",
      "Epoch: 78 \tTraining Loss: 0.447480 \tValidation Loss: 0.114987\n",
      "Validation loss decreased (0.115381 --> 0.114987).  Saving model ...\n",
      "Epoch: 79 \tTraining Loss: 0.447847 \tValidation Loss: 0.115068\n",
      "Epoch: 80 \tTraining Loss: 0.446921 \tValidation Loss: 0.117607\n",
      "Epoch: 81 \tTraining Loss: 0.445985 \tValidation Loss: 0.114510\n",
      "Validation loss decreased (0.114987 --> 0.114510).  Saving model ...\n",
      "Epoch: 82 \tTraining Loss: 0.445165 \tValidation Loss: 0.115230\n",
      "Epoch: 83 \tTraining Loss: 0.445311 \tValidation Loss: 0.114613\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 84 \tTraining Loss: 0.444551 \tValidation Loss: 0.114443\n",
      "Validation loss decreased (0.114510 --> 0.114443).  Saving model ...\n",
      "Epoch: 85 \tTraining Loss: 0.444171 \tValidation Loss: 0.115136\n",
      "Epoch: 86 \tTraining Loss: 0.443719 \tValidation Loss: 0.114293\n",
      "Validation loss decreased (0.114443 --> 0.114293).  Saving model ...\n",
      "Epoch: 87 \tTraining Loss: 0.443348 \tValidation Loss: 0.121042\n",
      "Epoch: 88 \tTraining Loss: 0.442575 \tValidation Loss: 0.114196\n",
      "Validation loss decreased (0.114293 --> 0.114196).  Saving model ...\n",
      "Epoch: 89 \tTraining Loss: 0.442312 \tValidation Loss: 0.117209\n",
      "Epoch: 90 \tTraining Loss: 0.441734 \tValidation Loss: 0.113793\n",
      "Validation loss decreased (0.114196 --> 0.113793).  Saving model ...\n",
      "Epoch: 91 \tTraining Loss: 0.441756 \tValidation Loss: 0.116821\n",
      "Epoch: 92 \tTraining Loss: 0.440962 \tValidation Loss: 0.114528\n",
      "Epoch: 93 \tTraining Loss: 0.440570 \tValidation Loss: 0.113993\n",
      "Epoch: 94 \tTraining Loss: 0.440336 \tValidation Loss: 0.114022\n",
      "Epoch: 95 \tTraining Loss: 0.439753 \tValidation Loss: 0.113709\n",
      "Validation loss decreased (0.113793 --> 0.113709).  Saving model ...\n",
      "Epoch: 96 \tTraining Loss: 0.439588 \tValidation Loss: 0.113476\n",
      "Validation loss decreased (0.113709 --> 0.113476).  Saving model ...\n",
      "Epoch: 97 \tTraining Loss: 0.438625 \tValidation Loss: 0.114559\n",
      "Epoch: 98 \tTraining Loss: 0.438401 \tValidation Loss: 0.113385\n",
      "Validation loss decreased (0.113476 --> 0.113385).  Saving model ...\n",
      "Epoch: 99 \tTraining Loss: 0.438010 \tValidation Loss: 0.113715\n",
      "Epoch: 100 \tTraining Loss: 0.437774 \tValidation Loss: 0.113597\n",
      "Time elapsed: 1512.47 s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "# number of epochs to train the model\n",
    "n_epochs = 100\n",
    "\n",
    "# initialize tracker for minimum validation loss\n",
    "valid_loss_min = np.Inf # set initial \"min\" to infinity\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "  # monitor training loss\n",
    "  train_loss = 0.0\n",
    "  valid_loss = 0.0\n",
    "\n",
    "  ###################\n",
    "  # train the model #\n",
    "  ###################\n",
    "  model_te_gru.train() # prep model for training\n",
    "  for data, target in train_loader_te_gru:\n",
    "    # transfer data and target to GPU\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    # clear the gradients of all optimized variables\n",
    "    optimizer_te_gru.zero_grad()\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output = model_te_gru(data.float())\n",
    "    # calculate the loss\n",
    "    loss = criterion(output, target)\n",
    "    # backward pass: compute gradient of the loss with respect to model parameters\n",
    "    loss.backward()\n",
    "    # perform a single optimization step (parameter update)\n",
    "    optimizer_te_gru.step()\n",
    "    # update running training loss\n",
    "    train_loss += loss.item()*data.size(0)\n",
    "\n",
    "  ######################\n",
    "  # validate the model #\n",
    "  ######################\n",
    "  model_te_gru.eval() # prep model for evaluation\n",
    "  for data, target in valid_loader_te_gru:\n",
    "    # transfer data and target to GPU\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output = model_te_gru(data.float())\n",
    "    # calculate the loss\n",
    "    loss = criterion(output, target)\n",
    "    # update running validation loss\n",
    "    valid_loss += loss.item()*data.size(0)\n",
    "    \n",
    "\n",
    "  # print training/validation statistics\n",
    "  # calculate average loss over an epoch\n",
    "  train_loss = train_loss/len(train_loader_te_gru.dataset)\n",
    "  valid_loss = valid_loss/len(valid_loader_te_gru.dataset)\n",
    "\n",
    "  print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch+1, train_loss, valid_loss))\n",
    "    \n",
    "  # save model if validation loss has decreased\n",
    "  if valid_loss <= valid_loss_min:\n",
    "    print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'\n",
    "        .format(valid_loss_min, valid_loss))\n",
    "    torch.save(model_te_gru.state_dict(), 'model.pt')\n",
    "    valid_loss_min = valid_loss\n",
    "\n",
    "end = time.time()\n",
    "print('Time elapsed: %.2f s' % (end - start))\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T3afWnqTnPDj",
    "outputId": "35631f7a-f832-410c-8d74-5800eaea7d2f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model with the lowest validation loss\n",
    "model_te_gru.load_state_dict(torch.load('model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "Aq1tIDIAnPDj"
   },
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for out outputs\n",
    "with torch.no_grad():\n",
    "  for data in test_loader_te_gru:\n",
    "    embeddings, labels = data\n",
    "    # transfer data and target to GPU\n",
    "    embeddings, labels = embeddings.to(device), labels.to(device)\n",
    "    # calculating outputs by running embeddings through the network\n",
    "    model_te_gru.to(device)\n",
    "    outputs = model_te_gru(embeddings.float())\n",
    "    # the class with the highest score is what we choose as prediction\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3lMOWq54nPDk",
    "outputId": "7df998f4-a103-4478-8886-0bab97a3c08e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy_GRU-model_pretrained-word2vec_ternary-test-set: 76 %\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy_GRU-model_pretrained-word2vec_ternary-test-set: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kQmrEy8_nPDk"
   },
   "source": [
    "## b.2 Use My Own Word2Vec as Input Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AmTKX-RtnPDk"
   },
   "source": [
    "### b.2.1 Compute Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mHQkBo23nPDk"
   },
   "source": [
    "#### Use **my own model** to compute word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "id": "Fnq1wz4CnPDk",
    "outputId": "04171307-a913-42d7-d52b-c0b367f8a413"
   },
   "outputs": [],
   "source": [
    "# Apply the word2v_seq function to compute the word vectors of reviews (up to\n",
    "# 50 words)\n",
    "X_train_bi_gru_series = X_train_bi.apply(lambda x: word2v_seq(x, model.wv))\n",
    "X_train_bi_gru = np.array(X_train_bi_gru_series.values.tolist())\n",
    "\n",
    "X_test_bi_gru_series = X_test_bi.apply(lambda x: word2v_seq(x, model.wv))\n",
    "X_test_bi_gru = np.array(X_test_bi_gru_series.values.tolist())\n",
    "\n",
    "X_train_te_gru_series = X_train_te.apply(lambda x: word2v_seq(x, model.wv))\n",
    "X_train_te_gru = np.array(X_train_te_gru_series.values.tolist())\n",
    "\n",
    "X_test_te_gru_series = X_test_te.apply(lambda x: word2v_seq(x, model.wv))\n",
    "X_test_te_gru = np.array(X_test_te_gru_series.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "id": "LJCreMmEnPDk",
    "outputId": "25dd3462-3adf-4482-bb15-9685cfe3e3c1"
   },
   "outputs": [],
   "source": [
    "# Apply the idx_nan function to remove the NaN's and corresponding\n",
    "# labels (if any)\n",
    "idx_nan_train_bi = idx_nan(X_train_bi_gru)\n",
    "if idx_nan_train_bi != None:\n",
    "  X_train_bi_gru = np.delete(X_train_bi_gru, idx_nan_train_bi, 0)\n",
    "  y_train_bi_gru = np.delete(y_train_bi, idx_nan_train_bi)\n",
    "else:\n",
    "  y_train_bi_gru = y_train_bi\n",
    "\n",
    "idx_nan_test_bi = idx_nan(X_test_bi_gru)\n",
    "if idx_nan_test_bi != None:\n",
    "  X_test_bi_gru = np.delete(X_test_bi_gru, idx_nan_test_bi, 0)\n",
    "  y_test_bi_gru = np.delete(y_test_bi, idx_nan_test_bi)\n",
    "else:\n",
    "  y_test_bi_gru = y_test_bi\n",
    "\n",
    "idx_nan_train_te = idx_nan(X_train_te_gru)\n",
    "if idx_nan_train_te != None:\n",
    "  X_train_te_gru = np.delete(X_train_te_gru, idx_nan_train_te, 0)\n",
    "  y_train_te_gru = np.delete(y_train_te, idx_nan_train_te)\n",
    "else:\n",
    "  y_train_te_gru = y_train_te\n",
    "\n",
    "idx_nan_test_te = idx_nan(X_test_te_gru)\n",
    "if idx_nan_test_te != None:\n",
    "  X_test_te_gru = np.delete(X_test_te_gru, idx_nan_test_te, 0)\n",
    "  y_test_te_gru = np.delete(y_test_te, idx_nan_test_te)\n",
    "else:\n",
    "  y_test_te_gru = y_test_te"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5WWAu46anPDk"
   },
   "source": [
    "### b.2.2 Define Dataset Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "CNDkowSunPDk"
   },
   "outputs": [],
   "source": [
    "class Train(Dataset):\n",
    "  def __init__(self, xtrain, ytrain):\n",
    "    'Initialization'\n",
    "    self.data = xtrain\n",
    "    self.labels = ytrain\n",
    "\n",
    "  def __len__(self):\n",
    "    'Denotes the total number of samples'\n",
    "    return len(self.data)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    'Generates one sample of data'\n",
    "    X = self.data[index]\n",
    "    y = self.labels[index]\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "13ZrSmhrnPDl"
   },
   "outputs": [],
   "source": [
    "class Test(Dataset):\n",
    "  def __init__(self, xtest, ytest):\n",
    "    'Initialization'\n",
    "    self.data = xtest\n",
    "    self.labels = ytest\n",
    "\n",
    "  def __len__(self):\n",
    "    'Denotes the total number of samples'\n",
    "    return len(self.data)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    'Generates one sample of data'\n",
    "    X = self.data[index]\n",
    "    y = self.labels[index]\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hKELU0wonPDl"
   },
   "source": [
    "### b.2.3 Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "V-9iYwn1nPDl"
   },
   "outputs": [],
   "source": [
    "# use word vectors of word sequences as input features\n",
    "train_data_bi_gru, test_data_bi_gru = Train(X_train_bi_gru, y_train_bi_gru-1), Test(X_test_bi_gru, y_test_bi_gru-1)\n",
    "train_data_te_gru, test_data_te_gru = Train(X_train_te_gru, y_train_te_gru-1), Test(X_test_te_gru, y_test_te_gru-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uGFOlidhnPDl"
   },
   "source": [
    "#### Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "-WZ9x04OnPDl"
   },
   "outputs": [],
   "source": [
    "# number of subprocesses to use for data loading\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 100\n",
    "# percentage of training set to use as validation\n",
    "valid_size = 0.2\n",
    "\n",
    "# obtain training indices that will be used for validation\n",
    "num_train = len(train_data_bi_gru)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "# define samples for obtaining training and validation batches\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "# prepare data loaders\n",
    "train_loader_bi_gru = torch.utils.data.DataLoader(train_data_bi_gru, batch_size=batch_size,\n",
    "                                           sampler=train_sampler, num_workers\n",
    "                                           =num_workers)\n",
    "valid_loader_bi_gru = torch.utils.data.DataLoader(train_data_bi_gru, batch_size=batch_size,\n",
    "                                           sampler=valid_sampler, num_workers\n",
    "                                           =num_workers)\n",
    "test_loader_bi_gru = torch.utils.data.DataLoader(test_data_bi_gru, batch_size=batch_size,\n",
    "                                           num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I6nbbGHknPDl"
   },
   "source": [
    "#### Ternary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "ZUbzp8A2nPDl"
   },
   "outputs": [],
   "source": [
    "# number of subprocesses to use for data loading\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 100\n",
    "# percentage of training set to use as validation\n",
    "valid_size = 0.2\n",
    "\n",
    "# obtain training indices that will be used for validation\n",
    "num_train = len(train_data_te_gru)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "# define samples for obtaining training and validation batches\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "# prepare data loaders\n",
    "train_loader_te_gru = torch.utils.data.DataLoader(train_data_te_gru, batch_size=batch_size,\n",
    "                                           sampler=train_sampler, num_workers\n",
    "                                           =num_workers)\n",
    "valid_loader_te_gru = torch.utils.data.DataLoader(train_data_te_gru, batch_size=batch_size,\n",
    "                                           sampler=valid_sampler, num_workers\n",
    "                                           =num_workers)\n",
    "test_loader_te_gru = torch.utils.data.DataLoader(test_data_te_gru, batch_size=batch_size,\n",
    "                                           num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dGqn0PMYnPDl"
   },
   "source": [
    "### b.2.4 Define the GRU architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "-40MW_h9nPDm"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YOEnLdxdnPDm",
    "outputId": "566f8130-a5b6-478f-8302-b0176ec0e43a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRUModel(\n",
      "  (gru): GRU(300, 50, batch_first=True)\n",
      "  (fc): Linear(in_features=50, out_features=2, bias=True)\n",
      ")\n",
      "GRUModel(\n",
      "  (gru): GRU(300, 50, batch_first=True)\n",
      "  (fc): Linear(in_features=50, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# define the GRU architecture\n",
    "class GRUModel(nn.Module):\n",
    "  def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
    "    super().__init__()\n",
    "    \n",
    "    # Number of hidden dimensions\n",
    "    self.hidden_dim = hidden_dim\n",
    "    \n",
    "    # Number of hidden layers\n",
    "    self.layer_dim = layer_dim\n",
    "    \n",
    "    # GRU\n",
    "    self.gru = nn.GRU(input_dim, hidden_dim, layer_dim, batch_first=True)\n",
    "    # Output layer\n",
    "    self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "  def forward(self, x):\n",
    "    \n",
    "    # Initialize hidden state with zeros\n",
    "    h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).to(device)\n",
    "    \n",
    "    # One time step\n",
    "    out, hn = self.gru(x, h0)\n",
    "    out = self.fc(out[:, -1, :])\n",
    "    return out\n",
    "\n",
    "# initialize GRU\n",
    "model_bi_gru = GRUModel(300, 50, 1, 2)\n",
    "model_te_gru = GRUModel(300, 50, 1, 3)\n",
    "model_bi_gru.cuda()\n",
    "model_te_gru.cuda()\n",
    "print(model_bi_gru)\n",
    "print(model_te_gru)\n",
    "\n",
    "# specify loss function (categorical cross-entropy)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# specify optimizer (stochastic gradient descent) and learning rate = 0.01\n",
    "optimizer_bi_gru = torch.optim.SGD(model_bi_gru.parameters(), lr=0.0075)\n",
    "optimizer_te_gru = torch.optim.SGD(model_te_gru.parameters(), lr=0.0075)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "perxDW86nPDm"
   },
   "source": [
    "### b.2.5 Train the Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fezVV4eDnPDm"
   },
   "source": [
    "#### Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8hwUrvYsnPDm",
    "outputId": "e8a4b522-6783-40f5-b3a1-359f64a53883"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.543465 \tValidation Loss: 0.133451\n",
      "Validation loss decreased (inf --> 0.133451).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 0.528786 \tValidation Loss: 0.131230\n",
      "Validation loss decreased (0.133451 --> 0.131230).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 0.520303 \tValidation Loss: 0.129382\n",
      "Validation loss decreased (0.131230 --> 0.129382).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 0.511893 \tValidation Loss: 0.127233\n",
      "Validation loss decreased (0.129382 --> 0.127233).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 0.500944 \tValidation Loss: 0.124069\n",
      "Validation loss decreased (0.127233 --> 0.124069).  Saving model ...\n",
      "Epoch: 6 \tTraining Loss: 0.479813 \tValidation Loss: 0.115475\n",
      "Validation loss decreased (0.124069 --> 0.115475).  Saving model ...\n",
      "Epoch: 7 \tTraining Loss: 0.405699 \tValidation Loss: 0.086526\n",
      "Validation loss decreased (0.115475 --> 0.086526).  Saving model ...\n",
      "Epoch: 8 \tTraining Loss: 0.298262 \tValidation Loss: 0.068004\n",
      "Validation loss decreased (0.086526 --> 0.068004).  Saving model ...\n",
      "Epoch: 9 \tTraining Loss: 0.247440 \tValidation Loss: 0.061789\n",
      "Validation loss decreased (0.068004 --> 0.061789).  Saving model ...\n",
      "Epoch: 10 \tTraining Loss: 0.229944 \tValidation Loss: 0.058322\n",
      "Validation loss decreased (0.061789 --> 0.058322).  Saving model ...\n",
      "Epoch: 11 \tTraining Loss: 0.220489 \tValidation Loss: 0.057351\n",
      "Validation loss decreased (0.058322 --> 0.057351).  Saving model ...\n",
      "Epoch: 12 \tTraining Loss: 0.214199 \tValidation Loss: 0.055843\n",
      "Validation loss decreased (0.057351 --> 0.055843).  Saving model ...\n",
      "Epoch: 13 \tTraining Loss: 0.209459 \tValidation Loss: 0.055543\n",
      "Validation loss decreased (0.055843 --> 0.055543).  Saving model ...\n",
      "Epoch: 14 \tTraining Loss: 0.205525 \tValidation Loss: 0.054248\n",
      "Validation loss decreased (0.055543 --> 0.054248).  Saving model ...\n",
      "Epoch: 15 \tTraining Loss: 0.201932 \tValidation Loss: 0.053697\n",
      "Validation loss decreased (0.054248 --> 0.053697).  Saving model ...\n",
      "Epoch: 16 \tTraining Loss: 0.198764 \tValidation Loss: 0.053693\n",
      "Validation loss decreased (0.053697 --> 0.053693).  Saving model ...\n",
      "Epoch: 17 \tTraining Loss: 0.196465 \tValidation Loss: 0.053320\n",
      "Validation loss decreased (0.053693 --> 0.053320).  Saving model ...\n",
      "Epoch: 18 \tTraining Loss: 0.193964 \tValidation Loss: 0.052851\n",
      "Validation loss decreased (0.053320 --> 0.052851).  Saving model ...\n",
      "Epoch: 19 \tTraining Loss: 0.191970 \tValidation Loss: 0.052048\n",
      "Validation loss decreased (0.052851 --> 0.052048).  Saving model ...\n",
      "Epoch: 20 \tTraining Loss: 0.189928 \tValidation Loss: 0.052111\n",
      "Epoch: 21 \tTraining Loss: 0.187955 \tValidation Loss: 0.052323\n",
      "Epoch: 22 \tTraining Loss: 0.186513 \tValidation Loss: 0.051949\n",
      "Validation loss decreased (0.052048 --> 0.051949).  Saving model ...\n",
      "Epoch: 23 \tTraining Loss: 0.184760 \tValidation Loss: 0.052479\n",
      "Epoch: 24 \tTraining Loss: 0.183286 \tValidation Loss: 0.051889\n",
      "Validation loss decreased (0.051949 --> 0.051889).  Saving model ...\n",
      "Epoch: 25 \tTraining Loss: 0.181964 \tValidation Loss: 0.050959\n",
      "Validation loss decreased (0.051889 --> 0.050959).  Saving model ...\n",
      "Epoch: 26 \tTraining Loss: 0.180434 \tValidation Loss: 0.050891\n",
      "Validation loss decreased (0.050959 --> 0.050891).  Saving model ...\n",
      "Epoch: 27 \tTraining Loss: 0.179064 \tValidation Loss: 0.050624\n",
      "Validation loss decreased (0.050891 --> 0.050624).  Saving model ...\n",
      "Epoch: 28 \tTraining Loss: 0.177696 \tValidation Loss: 0.050925\n",
      "Epoch: 29 \tTraining Loss: 0.176470 \tValidation Loss: 0.050752\n",
      "Epoch: 30 \tTraining Loss: 0.175145 \tValidation Loss: 0.050859\n",
      "Epoch: 31 \tTraining Loss: 0.174385 \tValidation Loss: 0.050505\n",
      "Validation loss decreased (0.050624 --> 0.050505).  Saving model ...\n",
      "Epoch: 32 \tTraining Loss: 0.173116 \tValidation Loss: 0.050630\n",
      "Epoch: 33 \tTraining Loss: 0.171957 \tValidation Loss: 0.051073\n",
      "Epoch: 34 \tTraining Loss: 0.170898 \tValidation Loss: 0.050649\n",
      "Epoch: 35 \tTraining Loss: 0.169705 \tValidation Loss: 0.050237\n",
      "Validation loss decreased (0.050505 --> 0.050237).  Saving model ...\n",
      "Epoch: 36 \tTraining Loss: 0.168737 \tValidation Loss: 0.050241\n",
      "Epoch: 37 \tTraining Loss: 0.167483 \tValidation Loss: 0.050530\n",
      "Epoch: 38 \tTraining Loss: 0.166716 \tValidation Loss: 0.050417\n",
      "Epoch: 39 \tTraining Loss: 0.165893 \tValidation Loss: 0.050378\n",
      "Epoch: 40 \tTraining Loss: 0.164900 \tValidation Loss: 0.050334\n",
      "Epoch: 41 \tTraining Loss: 0.163902 \tValidation Loss: 0.050249\n",
      "Epoch: 42 \tTraining Loss: 0.162837 \tValidation Loss: 0.049952\n",
      "Validation loss decreased (0.050237 --> 0.049952).  Saving model ...\n",
      "Epoch: 43 \tTraining Loss: 0.162028 \tValidation Loss: 0.050000\n",
      "Epoch: 44 \tTraining Loss: 0.161091 \tValidation Loss: 0.050476\n",
      "Epoch: 45 \tTraining Loss: 0.160563 \tValidation Loss: 0.049958\n",
      "Epoch: 46 \tTraining Loss: 0.159347 \tValidation Loss: 0.050925\n",
      "Epoch: 47 \tTraining Loss: 0.158352 \tValidation Loss: 0.050198\n",
      "Epoch: 48 \tTraining Loss: 0.157832 \tValidation Loss: 0.050061\n",
      "Epoch: 49 \tTraining Loss: 0.156686 \tValidation Loss: 0.050386\n",
      "Epoch: 50 \tTraining Loss: 0.155978 \tValidation Loss: 0.050424\n",
      "Epoch: 51 \tTraining Loss: 0.155122 \tValidation Loss: 0.050559\n",
      "Epoch: 52 \tTraining Loss: 0.154354 \tValidation Loss: 0.050601\n",
      "Epoch: 53 \tTraining Loss: 0.153668 \tValidation Loss: 0.050215\n",
      "Epoch: 54 \tTraining Loss: 0.152537 \tValidation Loss: 0.050671\n",
      "Epoch: 55 \tTraining Loss: 0.151776 \tValidation Loss: 0.050619\n",
      "Epoch: 56 \tTraining Loss: 0.151110 \tValidation Loss: 0.050134\n",
      "Epoch: 57 \tTraining Loss: 0.150182 \tValidation Loss: 0.050969\n",
      "Epoch: 58 \tTraining Loss: 0.149555 \tValidation Loss: 0.050301\n",
      "Epoch: 59 \tTraining Loss: 0.148545 \tValidation Loss: 0.051149\n",
      "Epoch: 60 \tTraining Loss: 0.147822 \tValidation Loss: 0.050432\n",
      "Epoch: 61 \tTraining Loss: 0.147300 \tValidation Loss: 0.050768\n",
      "Epoch: 62 \tTraining Loss: 0.146312 \tValidation Loss: 0.050338\n",
      "Epoch: 63 \tTraining Loss: 0.145849 \tValidation Loss: 0.050729\n",
      "Epoch: 64 \tTraining Loss: 0.144931 \tValidation Loss: 0.051113\n",
      "Epoch: 65 \tTraining Loss: 0.144121 \tValidation Loss: 0.050755\n",
      "Epoch: 66 \tTraining Loss: 0.143363 \tValidation Loss: 0.051394\n",
      "Epoch: 67 \tTraining Loss: 0.142547 \tValidation Loss: 0.051263\n",
      "Epoch: 68 \tTraining Loss: 0.141892 \tValidation Loss: 0.051464\n",
      "Epoch: 69 \tTraining Loss: 0.141275 \tValidation Loss: 0.050995\n",
      "Epoch: 70 \tTraining Loss: 0.140394 \tValidation Loss: 0.051058\n",
      "Epoch: 71 \tTraining Loss: 0.139737 \tValidation Loss: 0.051317\n",
      "Epoch: 72 \tTraining Loss: 0.138965 \tValidation Loss: 0.052670\n",
      "Epoch: 73 \tTraining Loss: 0.138319 \tValidation Loss: 0.051963\n",
      "Epoch: 74 \tTraining Loss: 0.137390 \tValidation Loss: 0.051442\n",
      "Epoch: 75 \tTraining Loss: 0.137007 \tValidation Loss: 0.053086\n",
      "Epoch: 76 \tTraining Loss: 0.136446 \tValidation Loss: 0.051664\n",
      "Epoch: 77 \tTraining Loss: 0.135466 \tValidation Loss: 0.052012\n",
      "Epoch: 78 \tTraining Loss: 0.134841 \tValidation Loss: 0.052127\n",
      "Epoch: 79 \tTraining Loss: 0.134031 \tValidation Loss: 0.052208\n",
      "Epoch: 80 \tTraining Loss: 0.133302 \tValidation Loss: 0.052294\n",
      "Epoch: 81 \tTraining Loss: 0.132515 \tValidation Loss: 0.053133\n",
      "Epoch: 82 \tTraining Loss: 0.132090 \tValidation Loss: 0.052516\n",
      "Epoch: 83 \tTraining Loss: 0.131343 \tValidation Loss: 0.052585\n",
      "Epoch: 84 \tTraining Loss: 0.130573 \tValidation Loss: 0.052653\n",
      "Epoch: 85 \tTraining Loss: 0.130143 \tValidation Loss: 0.053660\n",
      "Epoch: 86 \tTraining Loss: 0.129403 \tValidation Loss: 0.053198\n",
      "Epoch: 87 \tTraining Loss: 0.129061 \tValidation Loss: 0.053016\n",
      "Epoch: 88 \tTraining Loss: 0.128021 \tValidation Loss: 0.053548\n",
      "Epoch: 89 \tTraining Loss: 0.127443 \tValidation Loss: 0.053402\n",
      "Epoch: 90 \tTraining Loss: 0.126553 \tValidation Loss: 0.053317\n",
      "Epoch: 91 \tTraining Loss: 0.125954 \tValidation Loss: 0.053430\n",
      "Epoch: 92 \tTraining Loss: 0.125287 \tValidation Loss: 0.053883\n",
      "Epoch: 93 \tTraining Loss: 0.124968 \tValidation Loss: 0.054009\n",
      "Epoch: 94 \tTraining Loss: 0.124010 \tValidation Loss: 0.054941\n",
      "Epoch: 95 \tTraining Loss: 0.123600 \tValidation Loss: 0.054112\n",
      "Epoch: 96 \tTraining Loss: 0.122970 \tValidation Loss: 0.054777\n",
      "Epoch: 97 \tTraining Loss: 0.122368 \tValidation Loss: 0.054558\n",
      "Epoch: 98 \tTraining Loss: 0.121794 \tValidation Loss: 0.054935\n",
      "Epoch: 99 \tTraining Loss: 0.121255 \tValidation Loss: 0.054618\n",
      "Epoch: 100 \tTraining Loss: 0.120368 \tValidation Loss: 0.055588\n",
      "Time elapsed: 1067.62 s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "# number of epochs to train the model\n",
    "n_epochs = 100\n",
    "\n",
    "# initialize tracker for minimum validation loss\n",
    "valid_loss_min = np.Inf # set initial \"min\" to infinity\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "  # monitor training loss\n",
    "  train_loss = 0.0\n",
    "  valid_loss = 0.0\n",
    "\n",
    "  ###################\n",
    "  # train the model #\n",
    "  ###################\n",
    "  model_bi_gru.train() # prep model for training\n",
    "  for data, target in train_loader_bi_gru:\n",
    "    # transfer data and target to GPU\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    # clear the gradients of all optimized variables\n",
    "    optimizer_bi_gru.zero_grad()\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output = model_bi_gru(data.float())\n",
    "    # calculate the loss\n",
    "    loss = criterion(output, target)\n",
    "    # backward pass: compute gradient of the loss with respect to model parameters\n",
    "    loss.backward()\n",
    "    # perform a single optimization step (parameter update)\n",
    "    optimizer_bi_gru.step()\n",
    "    # update running training loss\n",
    "    train_loss += loss.item()*data.size(0)\n",
    "\n",
    "  ######################\n",
    "  # validate the model #\n",
    "  ######################\n",
    "  model_bi_gru.eval() # prep model for evaluation\n",
    "  for data, target in valid_loader_bi_gru:\n",
    "    # transfer data and target to GPU\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output = model_bi_gru(data.float())\n",
    "    # calculate the loss\n",
    "    loss = criterion(output, target)\n",
    "    # update running validation loss\n",
    "    valid_loss += loss.item()*data.size(0)\n",
    "    \n",
    "\n",
    "  # print training/validation statistics\n",
    "  # calculate average loss over an epoch\n",
    "  train_loss = train_loss/len(train_loader_bi_gru.dataset)\n",
    "  valid_loss = valid_loss/len(valid_loader_bi_gru.dataset)\n",
    "\n",
    "  print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch+1, train_loss, valid_loss))\n",
    "    \n",
    "  # save model if validation loss has decreased\n",
    "  if valid_loss <= valid_loss_min:\n",
    "    print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'\n",
    "        .format(valid_loss_min, valid_loss))\n",
    "    torch.save(model_bi_gru.state_dict(), 'model.pt')\n",
    "    valid_loss_min = valid_loss\n",
    "\n",
    "end = time.time()\n",
    "print('Time elapsed: %.2f s' % (end - start))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "tcHdp08enPDm"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model with the lowest validation loss\n",
    "model_bi_gru.load_state_dict(torch.load('model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "x3rLY_fGnPDn"
   },
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for out outputs\n",
    "with torch.no_grad():\n",
    "  for data in test_loader_bi_gru:\n",
    "    embeddings, labels = data\n",
    "    # transfer data and target to GPU\n",
    "    embeddings, labels = embeddings.to(device), labels.to(device)\n",
    "    # calculating outputs by running embeddings through the network\n",
    "    model_bi_gru.to(device)\n",
    "    outputs = model_bi_gru(embeddings.float())\n",
    "    # the class with the highest score is what we choose as prediction\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "FnqljAC6nPDn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy_GRU-model_my-own-word2vec_binary-test-set: 89 %\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy_GRU-model_my-own-word2vec_binary-test-set: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xip415VrnPDn"
   },
   "source": [
    "#### Ternary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lTD0PouMnPDn",
    "outputId": "5abff7fa-3e40-45f7-9fc3-54b49c665e9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.843879 \tValidation Loss: 0.206309\n",
      "Validation loss decreased (inf --> 0.206309).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 0.820839 \tValidation Loss: 0.203680\n",
      "Validation loss decreased (0.206309 --> 0.203680).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 0.812020 \tValidation Loss: 0.201723\n",
      "Validation loss decreased (0.203680 --> 0.201723).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 0.803926 \tValidation Loss: 0.199665\n",
      "Validation loss decreased (0.201723 --> 0.199665).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 0.793962 \tValidation Loss: 0.196630\n",
      "Validation loss decreased (0.199665 --> 0.196630).  Saving model ...\n",
      "Epoch: 6 \tTraining Loss: 0.772392 \tValidation Loss: 0.186267\n",
      "Validation loss decreased (0.196630 --> 0.186267).  Saving model ...\n",
      "Epoch: 7 \tTraining Loss: 0.696204 \tValidation Loss: 0.164028\n",
      "Validation loss decreased (0.186267 --> 0.164028).  Saving model ...\n",
      "Epoch: 8 \tTraining Loss: 0.622147 \tValidation Loss: 0.149678\n",
      "Validation loss decreased (0.164028 --> 0.149678).  Saving model ...\n",
      "Epoch: 9 \tTraining Loss: 0.583606 \tValidation Loss: 0.144007\n",
      "Validation loss decreased (0.149678 --> 0.144007).  Saving model ...\n",
      "Epoch: 10 \tTraining Loss: 0.564721 \tValidation Loss: 0.140073\n",
      "Validation loss decreased (0.144007 --> 0.140073).  Saving model ...\n",
      "Epoch: 11 \tTraining Loss: 0.548888 \tValidation Loss: 0.136885\n",
      "Validation loss decreased (0.140073 --> 0.136885).  Saving model ...\n",
      "Epoch: 12 \tTraining Loss: 0.532053 \tValidation Loss: 0.132302\n",
      "Validation loss decreased (0.136885 --> 0.132302).  Saving model ...\n",
      "Epoch: 13 \tTraining Loss: 0.514027 \tValidation Loss: 0.128113\n",
      "Validation loss decreased (0.132302 --> 0.128113).  Saving model ...\n",
      "Epoch: 14 \tTraining Loss: 0.498591 \tValidation Loss: 0.124855\n",
      "Validation loss decreased (0.128113 --> 0.124855).  Saving model ...\n",
      "Epoch: 15 \tTraining Loss: 0.486582 \tValidation Loss: 0.122386\n",
      "Validation loss decreased (0.124855 --> 0.122386).  Saving model ...\n",
      "Epoch: 16 \tTraining Loss: 0.476957 \tValidation Loss: 0.120411\n",
      "Validation loss decreased (0.122386 --> 0.120411).  Saving model ...\n",
      "Epoch: 17 \tTraining Loss: 0.469002 \tValidation Loss: 0.118714\n",
      "Validation loss decreased (0.120411 --> 0.118714).  Saving model ...\n",
      "Epoch: 18 \tTraining Loss: 0.462603 \tValidation Loss: 0.117573\n",
      "Validation loss decreased (0.118714 --> 0.117573).  Saving model ...\n",
      "Epoch: 19 \tTraining Loss: 0.457540 \tValidation Loss: 0.116183\n",
      "Validation loss decreased (0.117573 --> 0.116183).  Saving model ...\n",
      "Epoch: 20 \tTraining Loss: 0.453123 \tValidation Loss: 0.115454\n",
      "Validation loss decreased (0.116183 --> 0.115454).  Saving model ...\n",
      "Epoch: 21 \tTraining Loss: 0.449369 \tValidation Loss: 0.115188\n",
      "Validation loss decreased (0.115454 --> 0.115188).  Saving model ...\n",
      "Epoch: 22 \tTraining Loss: 0.446145 \tValidation Loss: 0.114364\n",
      "Validation loss decreased (0.115188 --> 0.114364).  Saving model ...\n",
      "Epoch: 23 \tTraining Loss: 0.443137 \tValidation Loss: 0.113676\n",
      "Validation loss decreased (0.114364 --> 0.113676).  Saving model ...\n",
      "Epoch: 24 \tTraining Loss: 0.440755 \tValidation Loss: 0.113493\n",
      "Validation loss decreased (0.113676 --> 0.113493).  Saving model ...\n",
      "Epoch: 25 \tTraining Loss: 0.438229 \tValidation Loss: 0.113138\n",
      "Validation loss decreased (0.113493 --> 0.113138).  Saving model ...\n",
      "Epoch: 26 \tTraining Loss: 0.436103 \tValidation Loss: 0.113005\n",
      "Validation loss decreased (0.113138 --> 0.113005).  Saving model ...\n",
      "Epoch: 27 \tTraining Loss: 0.434030 \tValidation Loss: 0.112440\n",
      "Validation loss decreased (0.113005 --> 0.112440).  Saving model ...\n",
      "Epoch: 28 \tTraining Loss: 0.432014 \tValidation Loss: 0.112524\n",
      "Epoch: 29 \tTraining Loss: 0.430341 \tValidation Loss: 0.112466\n",
      "Epoch: 30 \tTraining Loss: 0.428726 \tValidation Loss: 0.111640\n",
      "Validation loss decreased (0.112440 --> 0.111640).  Saving model ...\n",
      "Epoch: 31 \tTraining Loss: 0.427046 \tValidation Loss: 0.111427\n",
      "Validation loss decreased (0.111640 --> 0.111427).  Saving model ...\n",
      "Epoch: 32 \tTraining Loss: 0.425564 \tValidation Loss: 0.111622\n",
      "Epoch: 33 \tTraining Loss: 0.424265 \tValidation Loss: 0.111067\n",
      "Validation loss decreased (0.111427 --> 0.111067).  Saving model ...\n",
      "Epoch: 34 \tTraining Loss: 0.422934 \tValidation Loss: 0.111063\n",
      "Validation loss decreased (0.111067 --> 0.111063).  Saving model ...\n",
      "Epoch: 35 \tTraining Loss: 0.421506 \tValidation Loss: 0.111010\n",
      "Validation loss decreased (0.111063 --> 0.111010).  Saving model ...\n",
      "Epoch: 36 \tTraining Loss: 0.420403 \tValidation Loss: 0.110754\n",
      "Validation loss decreased (0.111010 --> 0.110754).  Saving model ...\n",
      "Epoch: 37 \tTraining Loss: 0.419034 \tValidation Loss: 0.111024\n",
      "Epoch: 38 \tTraining Loss: 0.417900 \tValidation Loss: 0.110867\n",
      "Epoch: 39 \tTraining Loss: 0.416785 \tValidation Loss: 0.110792\n",
      "Epoch: 40 \tTraining Loss: 0.415630 \tValidation Loss: 0.110555\n",
      "Validation loss decreased (0.110754 --> 0.110555).  Saving model ...\n",
      "Epoch: 41 \tTraining Loss: 0.414590 \tValidation Loss: 0.110328\n",
      "Validation loss decreased (0.110555 --> 0.110328).  Saving model ...\n",
      "Epoch: 42 \tTraining Loss: 0.413645 \tValidation Loss: 0.110453\n",
      "Epoch: 43 \tTraining Loss: 0.412527 \tValidation Loss: 0.110811\n",
      "Epoch: 44 \tTraining Loss: 0.411533 \tValidation Loss: 0.110085\n",
      "Validation loss decreased (0.110328 --> 0.110085).  Saving model ...\n",
      "Epoch: 45 \tTraining Loss: 0.410575 \tValidation Loss: 0.110437\n",
      "Epoch: 46 \tTraining Loss: 0.409730 \tValidation Loss: 0.110218\n",
      "Epoch: 47 \tTraining Loss: 0.408611 \tValidation Loss: 0.109953\n",
      "Validation loss decreased (0.110085 --> 0.109953).  Saving model ...\n",
      "Epoch: 48 \tTraining Loss: 0.407784 \tValidation Loss: 0.110046\n",
      "Epoch: 49 \tTraining Loss: 0.406953 \tValidation Loss: 0.109966\n",
      "Epoch: 50 \tTraining Loss: 0.406090 \tValidation Loss: 0.110097\n",
      "Epoch: 51 \tTraining Loss: 0.405081 \tValidation Loss: 0.110127\n",
      "Epoch: 52 \tTraining Loss: 0.404182 \tValidation Loss: 0.109877\n",
      "Validation loss decreased (0.109953 --> 0.109877).  Saving model ...\n",
      "Epoch: 53 \tTraining Loss: 0.403376 \tValidation Loss: 0.110417\n",
      "Epoch: 54 \tTraining Loss: 0.402661 \tValidation Loss: 0.110170\n",
      "Epoch: 55 \tTraining Loss: 0.401590 \tValidation Loss: 0.110139\n",
      "Epoch: 56 \tTraining Loss: 0.400865 \tValidation Loss: 0.110486\n",
      "Epoch: 57 \tTraining Loss: 0.400374 \tValidation Loss: 0.110386\n",
      "Epoch: 58 \tTraining Loss: 0.399453 \tValidation Loss: 0.110460\n",
      "Epoch: 59 \tTraining Loss: 0.398634 \tValidation Loss: 0.110150\n",
      "Epoch: 60 \tTraining Loss: 0.397801 \tValidation Loss: 0.110476\n",
      "Epoch: 61 \tTraining Loss: 0.397073 \tValidation Loss: 0.110175\n",
      "Epoch: 62 \tTraining Loss: 0.396391 \tValidation Loss: 0.110157\n",
      "Epoch: 63 \tTraining Loss: 0.395480 \tValidation Loss: 0.110324\n",
      "Epoch: 64 \tTraining Loss: 0.394814 \tValidation Loss: 0.110404\n",
      "Epoch: 65 \tTraining Loss: 0.394184 \tValidation Loss: 0.110286\n",
      "Epoch: 66 \tTraining Loss: 0.393494 \tValidation Loss: 0.110942\n",
      "Epoch: 67 \tTraining Loss: 0.392720 \tValidation Loss: 0.110711\n",
      "Epoch: 68 \tTraining Loss: 0.392083 \tValidation Loss: 0.110584\n",
      "Epoch: 69 \tTraining Loss: 0.391568 \tValidation Loss: 0.110434\n",
      "Epoch: 70 \tTraining Loss: 0.390590 \tValidation Loss: 0.110837\n",
      "Epoch: 71 \tTraining Loss: 0.389754 \tValidation Loss: 0.111288\n",
      "Epoch: 72 \tTraining Loss: 0.389307 \tValidation Loss: 0.110837\n",
      "Epoch: 73 \tTraining Loss: 0.388584 \tValidation Loss: 0.110989\n",
      "Epoch: 74 \tTraining Loss: 0.388156 \tValidation Loss: 0.111047\n",
      "Epoch: 75 \tTraining Loss: 0.387236 \tValidation Loss: 0.111153\n",
      "Epoch: 76 \tTraining Loss: 0.386716 \tValidation Loss: 0.111086\n",
      "Epoch: 77 \tTraining Loss: 0.386147 \tValidation Loss: 0.110856\n",
      "Epoch: 78 \tTraining Loss: 0.385497 \tValidation Loss: 0.111110\n",
      "Epoch: 79 \tTraining Loss: 0.384863 \tValidation Loss: 0.111494\n",
      "Epoch: 80 \tTraining Loss: 0.384145 \tValidation Loss: 0.110859\n",
      "Epoch: 81 \tTraining Loss: 0.383397 \tValidation Loss: 0.111763\n",
      "Epoch: 82 \tTraining Loss: 0.382867 \tValidation Loss: 0.111639\n",
      "Epoch: 83 \tTraining Loss: 0.382399 \tValidation Loss: 0.111151\n",
      "Epoch: 84 \tTraining Loss: 0.381748 \tValidation Loss: 0.111721\n",
      "Epoch: 85 \tTraining Loss: 0.380996 \tValidation Loss: 0.111928\n",
      "Epoch: 86 \tTraining Loss: 0.380492 \tValidation Loss: 0.111414\n",
      "Epoch: 87 \tTraining Loss: 0.379822 \tValidation Loss: 0.112189\n",
      "Epoch: 88 \tTraining Loss: 0.379195 \tValidation Loss: 0.111759\n",
      "Epoch: 89 \tTraining Loss: 0.378590 \tValidation Loss: 0.111990\n",
      "Epoch: 90 \tTraining Loss: 0.377946 \tValidation Loss: 0.111823\n",
      "Epoch: 91 \tTraining Loss: 0.377261 \tValidation Loss: 0.111979\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 92 \tTraining Loss: 0.376878 \tValidation Loss: 0.112289\n",
      "Epoch: 93 \tTraining Loss: 0.376471 \tValidation Loss: 0.112280\n",
      "Epoch: 94 \tTraining Loss: 0.375591 \tValidation Loss: 0.112989\n",
      "Epoch: 95 \tTraining Loss: 0.374890 \tValidation Loss: 0.112580\n",
      "Epoch: 96 \tTraining Loss: 0.374808 \tValidation Loss: 0.112880\n",
      "Epoch: 97 \tTraining Loss: 0.373899 \tValidation Loss: 0.112457\n",
      "Epoch: 98 \tTraining Loss: 0.373454 \tValidation Loss: 0.112694\n",
      "Epoch: 99 \tTraining Loss: 0.372878 \tValidation Loss: 0.112552\n",
      "Epoch: 100 \tTraining Loss: 0.372338 \tValidation Loss: 0.112876\n",
      "Time elapsed: 1457.47 s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "# number of epochs to train the model\n",
    "n_epochs = 100\n",
    "\n",
    "# initialize tracker for minimum validation loss\n",
    "valid_loss_min = np.Inf # set initial \"min\" to infinity\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "  # monitor training loss\n",
    "  train_loss = 0.0\n",
    "  valid_loss = 0.0\n",
    "\n",
    "  ###################\n",
    "  # train the model #\n",
    "  ###################\n",
    "  model_te_gru.train() # prep model for training\n",
    "  for data, target in train_loader_te_gru:\n",
    "    # transfer data and target to GPU\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    # clear the gradients of all optimized variables\n",
    "    optimizer_te_gru.zero_grad()\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output = model_te_gru(data.float())\n",
    "    # calculate the loss\n",
    "    loss = criterion(output, target)\n",
    "    # backward pass: compute gradient of the loss with respect to model parameters\n",
    "    loss.backward()\n",
    "    # perform a single optimization step (parameter update)\n",
    "    optimizer_te_gru.step()\n",
    "    # update running training loss\n",
    "    train_loss += loss.item()*data.size(0)\n",
    "\n",
    "  ######################\n",
    "  # validate the model #\n",
    "  ######################\n",
    "  model_te_gru.eval() # prep model for evaluation\n",
    "  for data, target in valid_loader_te_gru:\n",
    "    # transfer data and target to GPU\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output = model_te_gru(data.float())\n",
    "    # calculate the loss\n",
    "    loss = criterion(output, target)\n",
    "    # update running validation loss\n",
    "    valid_loss += loss.item()*data.size(0)\n",
    "    \n",
    "\n",
    "  # print training/validation statistics\n",
    "  # calculate average loss over an epoch\n",
    "  train_loss = train_loss/len(train_loader_te_gru.dataset)\n",
    "  valid_loss = valid_loss/len(valid_loader_te_gru.dataset)\n",
    "\n",
    "  print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch+1, train_loss, valid_loss))\n",
    "    \n",
    "  # save model if validation loss has decreased\n",
    "  if valid_loss <= valid_loss_min:\n",
    "    print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'\n",
    "        .format(valid_loss_min, valid_loss))\n",
    "    torch.save(model_te_gru.state_dict(), 'model.pt')\n",
    "    valid_loss_min = valid_loss\n",
    "\n",
    "end = time.time()\n",
    "print('Time elapsed: %.2f s' % (end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zTLrz7JOnPDo",
    "outputId": "35631f7a-f832-410c-8d74-5800eaea7d2f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model with the lowest validation loss\n",
    "model_te_gru.load_state_dict(torch.load('model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "kSXiVWcfnPDo"
   },
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for out outputs\n",
    "with torch.no_grad():\n",
    "  for data in test_loader_te_gru:\n",
    "    embeddings, labels = data\n",
    "    # transfer data and target to GPU\n",
    "    embeddings, labels = embeddings.to(device), labels.to(device)\n",
    "    # calculating outputs by running embeddings through the network\n",
    "    model_te_gru.to(device)\n",
    "    outputs = model_te_gru(embeddings.float())\n",
    "    # the class with the highest score is what we choose as prediction\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7o4X1AH2nPDo",
    "outputId": "7df998f4-a103-4478-8886-0bab97a3c08e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy_GRU-model_my-own-word2vec_ternary-test-set: 76 %\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy_GRU-model_my-own-word2vec_ternary-test-set: %d %%' % (100 * correct / total))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "NIynUivVrEG-",
    "GG7uTtUno9FP",
    "fOQb3I0foJ0u",
    "BDIK-j9mo9FQ",
    "gytGCZQoo9FR",
    "yuPZxv7uo9FQ",
    "YMB2yBIRo9FR",
    "KJ_OOzJ9o9FS",
    "iqFTNNXwo9FS",
    "Y3HPTa3to9FS",
    "vpmFTWcypdBz",
    "jF9eeQtxPoJH",
    "wDPvIh6eUYFV",
    "dzUbaCdyo9FT",
    "Cgnopk2XDgqJ",
    "RGiglGGXo9FT"
   ],
   "name": "HW2_CSCI544.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
